<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>OO (1)</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install --upgrade tensorflow
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting tensorflow
  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)
     || 86.3MB 34kB/s 
Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)
Requirement already satisfied, skipping upgrade: keras-preprocessing&gt;=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)
Requirement already satisfied, skipping upgrade: keras-applications&gt;=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)
Requirement already satisfied, skipping upgrade: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)
Requirement already satisfied, skipping upgrade: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)
Requirement already satisfied, skipping upgrade: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.4)
Requirement already satisfied, skipping upgrade: termcolor&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)
Requirement already satisfied, skipping upgrade: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)
Collecting tensorflow-estimator&lt;2.1.0,&gt;=2.0.0
  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)
     || 450kB 42.9MB/s 
Requirement already satisfied, skipping upgrade: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)
Requirement already satisfied, skipping upgrade: grpcio&gt;=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)
Requirement already satisfied, skipping upgrade: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)
Collecting tensorboard&lt;2.1.0,&gt;=2.0.0
  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)
     || 3.8MB 36.0MB/s 
Requirement already satisfied, skipping upgrade: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)
Requirement already satisfied, skipping upgrade: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)
Requirement already satisfied, skipping upgrade: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)
Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tensorflow) (2.8.0)
Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.6.1-&gt;tensorflow) (42.0.2)
Collecting google-auth&lt;2,&gt;=1.6.3
  Downloading https://files.pythonhosted.org/packages/54/31/f944cbd5bdbcc90d5b36f0615036308c8ec1e41b4788da5b55d4900f6803/google_auth-1.8.2-py2.py3-none-any.whl (75kB)
     || 81kB 10.6MB/s 
Requirement already satisfied, skipping upgrade: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (2.21.0)
Requirement already satisfied, skipping upgrade: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (3.1.1)
Requirement already satisfied, skipping upgrade: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (0.4.1)
Requirement already satisfied, skipping upgrade: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (0.16.0)
Requirement already satisfied, skipping upgrade: cachetools&lt;3.2,&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (3.1.1)
Requirement already satisfied, skipping upgrade: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (0.2.7)
Requirement already satisfied, skipping upgrade: rsa&lt;4.1,&gt;=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (4.0)
Requirement already satisfied, skipping upgrade: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (1.24.3)
Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (2.8)
Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (2019.11.28)
Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (3.0.4)
Requirement already satisfied, skipping upgrade: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (1.3.0)
Requirement already satisfied, skipping upgrade: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (0.4.8)
Requirement already satisfied, skipping upgrade: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (3.1.0)
<span class="ansi-red-fg">ERROR: tensorboard 2.0.2 has requirement grpcio&gt;=1.24.3, but you&#39;ll have grpcio 1.15.0 which is incompatible.</span>
<span class="ansi-red-fg">ERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you&#39;ll have google-auth 1.8.2 which is incompatible.</span>
Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow
  Found existing installation: tensorflow-estimator 1.15.1
    Uninstalling tensorflow-estimator-1.15.1:
      Successfully uninstalled tensorflow-estimator-1.15.1
  Found existing installation: google-auth 1.4.2
    Uninstalling google-auth-1.4.2:
      Successfully uninstalled google-auth-1.4.2
  Found existing installation: tensorboard 1.15.0
    Uninstalling tensorboard-1.15.0:
      Successfully uninstalled tensorboard-1.15.0
  Found existing installation: tensorflow 1.15.0
    Uninstalling tensorflow-1.15.0:
      Successfully uninstalled tensorflow-1.15.0
Successfully installed google-auth-1.8.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>rm -rf ./logs
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="ch">#!pip install -q tf-nightly-2.0-preview</span>
<span class="c1"># Load the TensorBoard notebook extension</span>
<span class="c1">#!pip3 install tfp-nightly</span>
<span class="o">!</span>pip install --upgrade grpcio 
<span class="c1">#!pip install tensorflow-gpu==2.0</span>
<span class="c1">#!pip freeze</span>

<span class="o">%</span><span class="k">load_ext</span> tensorboard
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Requirement already up-to-date: grpcio in /usr/local/lib/python3.6/dist-packages (1.25.0)
Requirement already satisfied, skipping upgrade: six&gt;=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio) (1.12.0)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#conda create -n tf2 python=3.6</span>
<span class="c1">#activate tf2</span>
<span class="c1">#!pip3 show tensorflow</span>
<span class="c1">#!echo $PYTHONPATH</span>

<span class="c1">#!export PYTHONPATH=/usr/local/lib/python3.6/dist-packages:$PYTHONPATH.</span>
<span class="c1">#!echo $PYTHONPATH</span>


<span class="c1">#!pip uninstall tf-nightly-2.0-preview</span>
<span class="c1">#!pip install tf-nightly-gpu-2.0-preview</span>
<span class="c1">#  %reload_ext tensorboard</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>


<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">PIL</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="k">import</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span><span class="o">,</span> <span class="nn">os</span>
<span class="c1">#from google.colab import files</span>
<span class="o">%</span><span class="k">load_ext</span> tensorboard

<span class="kn">from</span> <span class="nn">IPython</span> <span class="k">import</span> <span class="n">display</span>

<span class="n">logs_base_dir</span> <span class="o">=</span> <span class="s2">&quot;./logs&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">logs_base_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">%</span><span class="k">tensorboard</span> --logdir {logs_base_dir}


<span class="k">class</span> <span class="nc">Trainer</span><span class="p">():</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">60000</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">examples_to_generate</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_train</span><span class="o">=</span><span class="mi">60000</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">examples_to_generate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="s1">&#39;train_accuracy&#39;</span><span class="p">)</span>
    <span class="c1">#self.batch=self.num_train/self.BATCH_SIZE</span>
  

  <span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
    <span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
    <span class="n">train_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_images</span> <span class="o">-</span> <span class="mf">127.5</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span> <span class="c1"># Normalize the images to [-1, 1]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">train_images</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">BUFFER_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">gen_tape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">disc_tape</span><span class="p">:</span>
      <span class="n">generated_images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

      <span class="n">real_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">fake_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">generated_images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">loss_calc</span><span class="p">(</span><span class="n">fake_output</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">loss_calc</span><span class="p">(</span><span class="n">real_output</span><span class="p">,</span> <span class="n">fake_output</span><span class="p">)</span>

    <span class="n">gradients_of_generator</span> <span class="o">=</span> <span class="n">gen_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">gradients_of_discriminator</span> <span class="o">=</span> <span class="n">disc_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients_of_generator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients_of_discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">train_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">train_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">current_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y%m</span><span class="si">%d</span><span class="s2">-%H%M%S&quot;</span><span class="p">)</span>
    <span class="n">train_log_dir</span> <span class="o">=</span> <span class="s1">&#39;logs/gradient_tape/train&#39;</span>
    <span class="n">train_summary_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">train_log_dir</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">train_summary_writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;generator_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;discriminator_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">template</span> <span class="o">=</span> <span class="s1">&#39;Batch </span><span class="si">{}</span><span class="s1">, generator loss: </span><span class="si">{}</span><span class="s1">, discriminator loss: </span><span class="si">{}</span><span class="s1"> &#39;</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">batch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">train_loss</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">train_loss</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>



  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">EPOCHS</span><span class="p">):</span>
      <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;EPOCH:&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
      <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

      <span class="k">for</span> <span class="n">images</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="n">counter</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">epoch</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">BATCH_SIZE</span> <span class="o">+</span> <span class="n">counter</span><span class="p">)</span>
      <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generator loss:&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Discriminator loss:&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>



      <span class="c1"># Produce images for the GIF as we go</span>
      <span class="c1"># display.clear_output(wait=True)</span>
    <span class="n">generate_and_save_images</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                             <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

      <span class="c1"># Save the model every 15 epochs</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">15</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
         <span class="n">checkpoint</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">file_prefix</span> <span class="o">=</span> <span class="n">checkpoint_prefix</span><span class="p">)</span>

    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Time for epoch </span><span class="si">{}</span><span class="s1"> is </span><span class="si">{}</span><span class="s1"> sec&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>

    <span class="c1"># Generate after the final epoch</span>
    <span class="c1">#display.clear_output(wait=True)</span>
    <span class="c1">#generate_and_save_images(self.generator.model,</span>
    <span class="c1">#                       self.EPOCHS,</span>
    <span class="c1">#                       self.seed)</span>
       <span class="c1"># noise = tf.random.normal([self.BATCH_SIZE, self.latent_dim])</span>
       <span class="c1"># generator = Generator()</span>
       <span class="c1"># generated_images = generator.model(noise, training=True)</span>

       <span class="c1"># discriminator = Discriminator()</span>
       <span class="c1"># real_output = discriminator.model(images, training=True)</span>
       <span class="c1"># fake_output = discriminator.model(generated_images, training=True)</span>

       <span class="c1"># generator.loss(fake_output)</span>
       <span class="c1"># print(&quot;loss:&quot;, generator.loss)</span>
       <span class="c1"># discriminator.loss(real_output, fake_output)</span>

       <span class="c1"># generator.gradient()</span>
       <span class="c1"># print(generator.gradients)</span>
       <span class="c1"># generator.optimize()</span>

       <span class="c1"># discriminator.gradient()</span>
       <span class="c1"># discriminator.optimize()</span>



<span class="k">class</span> <span class="nc">Generator</span><span class="p">():</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">256</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">256</span><span class="p">)))</span>
    

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
    

  <span class="k">def</span> <span class="nf">loss_calc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fake_output</span><span class="p">):</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">fake_output</span><span class="p">),</span> <span class="n">fake_output</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">gradient_tape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">gradient_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>



<span class="k">class</span> <span class="nc">Discriminator</span><span class="p">():</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                                     <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">loss_calc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">real_prediction</span><span class="p">,</span> <span class="n">fake_prediction</span><span class="p">):</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">real_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">real_prediction</span><span class="p">),</span> <span class="n">real_prediction</span><span class="p">))</span>
    <span class="n">fake_loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">fake_prediction</span><span class="p">),</span> <span class="n">fake_prediction</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">real_loss</span> <span class="o">+</span> <span class="n">fake_loss</span>
 
    



  <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">gradient_tape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">gradient_tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">generate_and_save_images</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">test_input</span><span class="p">):</span>
  
  <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">127.5</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;image_at_epoch_</span><span class="si">{:04d}</span><span class="s1">.png&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
  <span class="c1">#files.download(&#39;image_at_epoch_{:04d}.png&#39;)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>2.0.0
The tensorboard extension is already loaded. To reload it, use:
  %reload_ext tensorboard
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">

    <div id="root"></div>
    <script>
      (function() {
        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};
        window.TENSORBOARD_ENV["IN_COLAB"] = true;
        document.querySelector("base").href = "https://localhost:6006";
        function fixUpTensorboard(root) {
          const tftb = root.querySelector("tf-tensorboard");
          // Disable the fragment manipulation behavior in Colab. Not
          // only is the behavior not useful (as the iframe's location
          // is not visible to the user), it causes TensorBoard's usage
          // of `window.replace` to navigate away from the page and to
          // the `localhost:<port>` URL specified by the base URI, which
          // in turn causes the frame to (likely) crash.
          tftb.removeAttribute("use-hash");
        }
        function executeAllScripts(root) {
          // When `script` elements are inserted into the DOM by
          // assigning to an element's `innerHTML`, the scripts are not
          // executed. Thus, we manually re-insert these scripts so that
          // TensorBoard can initialize itself.
          for (const script of root.querySelectorAll("script")) {
            const newScript = document.createElement("script");
            newScript.type = script.type;
            newScript.textContent = script.textContent;
            root.appendChild(newScript);
            script.remove();
          }
        }
        function setHeight(root, height) {
          // We set the height dynamically after the TensorBoard UI has
          // been initialized. This avoids an intermediate state in
          // which the container plus the UI become taller than the
          // final width and cause the Colab output frame to be
          // permanently resized, eventually leading to an empty
          // vertical gap below the TensorBoard UI. It's not clear
          // exactly what causes this problematic intermediate state,
          // but setting the height late seems to fix it.
          root.style.height = `${height}px`;
        }
        const root = document.getElementById("root");
        fetch(".")
          .then((x) => x.text())
          .then((html) => void (root.innerHTML = html))
          .then(() => fixUpTensorboard(root))
          .then(() => executeAllScripts(root))
          .then(() => setHeight(root, 800));
      })();
    </script>
  
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>EPOCH: 0
Batch 2, generator loss: 0.6835362911224365, discriminator loss: 1.4428815841674805 
Batch 3, generator loss: 0.6720609664916992, discriminator loss: 1.3937342166900635 
Batch 4, generator loss: 0.6603684425354004, discriminator loss: 1.3574188947677612 
Batch 5, generator loss: 0.655584454536438, discriminator loss: 1.3112208843231201 
Batch 6, generator loss: 0.6506916284561157, discriminator loss: 1.272505521774292 
Batch 7, generator loss: 0.6515117883682251, discriminator loss: 1.2283802032470703 
Batch 8, generator loss: 0.647024393081665, discriminator loss: 1.1950500011444092 
Batch 9, generator loss: 0.6488259434700012, discriminator loss: 1.153601050376892 
Batch 10, generator loss: 0.6501643657684326, discriminator loss: 1.1230186223983765 
Batch 11, generator loss: 0.6520845890045166, discriminator loss: 1.0878934860229492 
Batch 12, generator loss: 0.6541358232498169, discriminator loss: 1.0542272329330444 
Batch 13, generator loss: 0.6540826559066772, discriminator loss: 1.0290300846099854 
Batch 14, generator loss: 0.6557806730270386, discriminator loss: 0.9980542063713074 
Batch 15, generator loss: 0.6595396995544434, discriminator loss: 0.9731134176254272 
Batch 16, generator loss: 0.6634246110916138, discriminator loss: 0.9474889039993286 
Batch 17, generator loss: 0.6630289554595947, discriminator loss: 0.9321628212928772 
Batch 18, generator loss: 0.65846186876297, discriminator loss: 0.9185858964920044 
Batch 19, generator loss: 0.6590811014175415, discriminator loss: 0.9031216502189636 
Batch 20, generator loss: 0.6563114523887634, discriminator loss: 0.8893887996673584 
Batch 21, generator loss: 0.6592448949813843, discriminator loss: 0.8816542625427246 
Batch 22, generator loss: 0.658574104309082, discriminator loss: 0.8720005750656128 
Batch 23, generator loss: 0.6563760042190552, discriminator loss: 0.8616555333137512 
Batch 24, generator loss: 0.6599158048629761, discriminator loss: 0.8480508327484131 
Batch 25, generator loss: 0.6569350957870483, discriminator loss: 0.848349928855896 
Batch 26, generator loss: 0.6551952362060547, discriminator loss: 0.843701958656311 
Batch 27, generator loss: 0.6507680416107178, discriminator loss: 0.8424906134605408 
Batch 28, generator loss: 0.6520969271659851, discriminator loss: 0.8402822017669678 
Batch 29, generator loss: 0.6497187614440918, discriminator loss: 0.8417589664459229 
Batch 30, generator loss: 0.6421642303466797, discriminator loss: 0.8474274277687073 
Batch 31, generator loss: 0.639857292175293, discriminator loss: 0.845085620880127 
Batch 32, generator loss: 0.6329399943351746, discriminator loss: 0.8540448546409607 
Batch 33, generator loss: 0.6210808753967285, discriminator loss: 0.8694285154342651 
Batch 34, generator loss: 0.6125198006629944, discriminator loss: 0.8777056336402893 
Batch 35, generator loss: 0.5963130593299866, discriminator loss: 0.8960446715354919 
Batch 36, generator loss: 0.5887754559516907, discriminator loss: 0.9091705083847046 
Batch 37, generator loss: 0.5740452408790588, discriminator loss: 0.9333348274230957 
Batch 38, generator loss: 0.5586177110671997, discriminator loss: 0.9564850926399231 
Batch 39, generator loss: 0.5421342849731445, discriminator loss: 0.9808429479598999 
Batch 40, generator loss: 0.5337657928466797, discriminator loss: 0.9993383884429932 
Batch 41, generator loss: 0.5160183906555176, discriminator loss: 1.030632734298706 
Batch 42, generator loss: 0.5141772031784058, discriminator loss: 1.0413182973861694 
Batch 43, generator loss: 0.5090470910072327, discriminator loss: 1.0530675649642944 
Batch 44, generator loss: 0.5133594274520874, discriminator loss: 1.0557715892791748 
Batch 45, generator loss: 0.526779055595398, discriminator loss: 1.0455870628356934 
Batch 46, generator loss: 0.5403590202331543, discriminator loss: 1.0306187868118286 
Batch 47, generator loss: 0.561051607131958, discriminator loss: 1.0115844011306763 
Batch 48, generator loss: 0.5738718509674072, discriminator loss: 1.0008530616760254 
Batch 49, generator loss: 0.5819671154022217, discriminator loss: 0.995529055595398 
Batch 50, generator loss: 0.5859658122062683, discriminator loss: 1.0053504705429077 
Batch 51, generator loss: 0.5844055414199829, discriminator loss: 1.0105921030044556 
Batch 52, generator loss: 0.5764428377151489, discriminator loss: 1.0287076234817505 
Batch 53, generator loss: 0.5794504880905151, discriminator loss: 1.0330116748809814 
Batch 54, generator loss: 0.5866701602935791, discriminator loss: 1.0327715873718262 
Batch 55, generator loss: 0.609970211982727, discriminator loss: 1.0019954442977905 
Batch 56, generator loss: 0.6305487751960754, discriminator loss: 0.9844104647636414 
Batch 57, generator loss: 0.666273832321167, discriminator loss: 0.9502352476119995 
Batch 58, generator loss: 0.694818913936615, discriminator loss: 0.9223288893699646 
Batch 59, generator loss: 0.7273369431495667, discriminator loss: 0.8859502077102661 
Batch 60, generator loss: 0.7506858110427856, discriminator loss: 0.8703988790512085 
Batch 61, generator loss: 0.7695747017860413, discriminator loss: 0.8498152494430542 
Batch 62, generator loss: 0.789945125579834, discriminator loss: 0.8374502658843994 
Batch 63, generator loss: 0.8104724884033203, discriminator loss: 0.8089166283607483 
Batch 64, generator loss: 0.8260727524757385, discriminator loss: 0.792649507522583 
Batch 65, generator loss: 0.8368782997131348, discriminator loss: 0.7724112272262573 
Batch 66, generator loss: 0.8478376865386963, discriminator loss: 0.7701683044433594 
Batch 67, generator loss: 0.861811637878418, discriminator loss: 0.7579816579818726 
Batch 68, generator loss: 0.8481512069702148, discriminator loss: 0.7541348934173584 
Batch 69, generator loss: 0.8344152569770813, discriminator loss: 0.768464982509613 
Batch 70, generator loss: 0.8175274133682251, discriminator loss: 0.7553081512451172 
Batch 71, generator loss: 0.798938512802124, discriminator loss: 0.7761266827583313 
Batch 72, generator loss: 0.7807363271713257, discriminator loss: 0.7813680171966553 
Batch 73, generator loss: 0.7580329179763794, discriminator loss: 0.805941641330719 
Batch 74, generator loss: 0.7543232440948486, discriminator loss: 0.8185786008834839 
Batch 75, generator loss: 0.7478761672973633, discriminator loss: 0.8346925377845764 
Batch 76, generator loss: 0.7474960088729858, discriminator loss: 0.8386964797973633 
Batch 77, generator loss: 0.73314368724823, discriminator loss: 0.86469566822052 
Batch 78, generator loss: 0.7386794686317444, discriminator loss: 0.8547748923301697 
Batch 79, generator loss: 0.7291151285171509, discriminator loss: 0.8740862607955933 
Batch 80, generator loss: 0.7287300825119019, discriminator loss: 0.8938902020454407 
Batch 81, generator loss: 0.7253046035766602, discriminator loss: 0.9095156788825989 
Batch 82, generator loss: 0.7312871217727661, discriminator loss: 0.9031067490577698 
Batch 83, generator loss: 0.7340593934059143, discriminator loss: 0.927376389503479 
Batch 84, generator loss: 0.7447689771652222, discriminator loss: 0.9414060711860657 
Batch 85, generator loss: 0.7625765800476074, discriminator loss: 0.9251095056533813 
Batch 86, generator loss: 0.7812018394470215, discriminator loss: 0.9335546493530273 
Batch 87, generator loss: 0.8171297311782837, discriminator loss: 0.9343466758728027 
Batch 88, generator loss: 0.8351855278015137, discriminator loss: 0.9341573715209961 
Batch 89, generator loss: 0.844733715057373, discriminator loss: 0.9314019680023193 
Batch 90, generator loss: 0.8360859155654907, discriminator loss: 0.9538804292678833 
Batch 91, generator loss: 0.8319241404533386, discriminator loss: 0.9638959169387817 
Batch 92, generator loss: 0.7869776487350464, discriminator loss: 0.9945995807647705 
Batch 93, generator loss: 0.7351736426353455, discriminator loss: 1.047070026397705 
Batch 94, generator loss: 0.6796787977218628, discriminator loss: 1.0862292051315308 
Batch 95, generator loss: 0.634063720703125, discriminator loss: 1.1391245126724243 
Batch 96, generator loss: 0.6164777278900146, discriminator loss: 1.1558139324188232 
Batch 97, generator loss: 0.6047691106796265, discriminator loss: 1.2067489624023438 
Batch 98, generator loss: 0.6201798319816589, discriminator loss: 1.1870920658111572 
Batch 99, generator loss: 0.6618070602416992, discriminator loss: 1.163528323173523 
Batch 100, generator loss: 0.7016004323959351, discriminator loss: 1.129193663597107 
Batch 101, generator loss: 0.7435941696166992, discriminator loss: 1.1093106269836426 
Batch 102, generator loss: 0.7892391681671143, discriminator loss: 1.0548523664474487 
Batch 103, generator loss: 0.8265408873558044, discriminator loss: 1.0119848251342773 
Batch 104, generator loss: 0.8756364583969116, discriminator loss: 1.0021440982818604 
Batch 105, generator loss: 0.9179582595825195, discriminator loss: 0.9441519975662231 
Batch 106, generator loss: 0.9381651282310486, discriminator loss: 0.9076248407363892 
Batch 107, generator loss: 0.951637864112854, discriminator loss: 0.8868939876556396 
Batch 108, generator loss: 0.9771398305892944, discriminator loss: 0.8599178791046143 
Batch 109, generator loss: 0.9709193706512451, discriminator loss: 0.8419003486633301 
Batch 110, generator loss: 0.9535777568817139, discriminator loss: 0.8313016891479492 
Batch 111, generator loss: 0.9495719075202942, discriminator loss: 0.7972086071968079 
Batch 112, generator loss: 0.9245635271072388, discriminator loss: 0.799567461013794 
Batch 113, generator loss: 0.9067715406417847, discriminator loss: 0.7908046245574951 
Batch 114, generator loss: 0.8901550769805908, discriminator loss: 0.7835090160369873 
Batch 115, generator loss: 0.8852955102920532, discriminator loss: 0.8079997301101685 
Batch 116, generator loss: 0.8939502239227295, discriminator loss: 0.7685009837150574 
Batch 117, generator loss: 0.907921314239502, discriminator loss: 0.7743763327598572 
Batch 118, generator loss: 0.9044102430343628, discriminator loss: 0.7966419458389282 
Batch 119, generator loss: 0.9180400371551514, discriminator loss: 0.7961028814315796 
Batch 120, generator loss: 0.918271541595459, discriminator loss: 0.8002886176109314 
Batch 121, generator loss: 0.8988596200942993, discriminator loss: 0.8259719014167786 
Batch 122, generator loss: 0.9059034585952759, discriminator loss: 0.8273191452026367 
Batch 123, generator loss: 0.8678537607192993, discriminator loss: 0.8751162886619568 
Batch 124, generator loss: 0.8505114316940308, discriminator loss: 0.8910905122756958 
Batch 125, generator loss: 0.8405945897102356, discriminator loss: 0.9084771871566772 
Batch 126, generator loss: 0.8080448508262634, discriminator loss: 0.9339914321899414 
Batch 127, generator loss: 0.8045573234558105, discriminator loss: 0.9540996551513672 
Batch 128, generator loss: 0.7952975630760193, discriminator loss: 0.9855506420135498 
Batch 129, generator loss: 0.7882684469223022, discriminator loss: 0.990592896938324 
Batch 130, generator loss: 0.771184504032135, discriminator loss: 0.99156254529953 
Batch 131, generator loss: 0.7818036079406738, discriminator loss: 0.968657910823822 
Batch 132, generator loss: 0.7644511461257935, discriminator loss: 0.9998494386672974 
Batch 133, generator loss: 0.7814226150512695, discriminator loss: 0.9883294105529785 
Batch 134, generator loss: 0.8101619482040405, discriminator loss: 0.9437218308448792 
Batch 135, generator loss: 0.8433731198310852, discriminator loss: 0.9304271936416626 
Batch 136, generator loss: 0.8725669980049133, discriminator loss: 0.9339957237243652 
Batch 137, generator loss: 0.9090175032615662, discriminator loss: 0.8722798824310303 
Batch 138, generator loss: 0.9179011583328247, discriminator loss: 0.8761824369430542 
Batch 139, generator loss: 0.9263087511062622, discriminator loss: 0.8430878520011902 
Batch 140, generator loss: 0.9448885917663574, discriminator loss: 0.8316625356674194 
Batch 141, generator loss: 0.9248604774475098, discriminator loss: 0.8516296148300171 
Batch 142, generator loss: 0.9174805283546448, discriminator loss: 0.8491659164428711 
Batch 143, generator loss: 0.907600998878479, discriminator loss: 0.855426013469696 
Batch 144, generator loss: 0.9367964267730713, discriminator loss: 0.837833821773529 
Batch 145, generator loss: 0.9554692506790161, discriminator loss: 0.8274151086807251 
Batch 146, generator loss: 0.9807748794555664, discriminator loss: 0.8271494507789612 
Batch 147, generator loss: 0.992985725402832, discriminator loss: 0.8293659687042236 
Batch 148, generator loss: 0.9694877862930298, discriminator loss: 0.8370323181152344 
Batch 149, generator loss: 0.9511927366256714, discriminator loss: 0.8792070150375366 
Batch 150, generator loss: 0.8869403600692749, discriminator loss: 0.9318329691886902 
Batch 151, generator loss: 0.86238694190979, discriminator loss: 0.9477542638778687 
Batch 152, generator loss: 0.8122388124465942, discriminator loss: 1.0135152339935303 
Batch 153, generator loss: 0.7682465314865112, discriminator loss: 1.0512635707855225 
Batch 154, generator loss: 0.7445053458213806, discriminator loss: 1.1153584718704224 
Batch 155, generator loss: 0.729526698589325, discriminator loss: 1.1785633563995361 
Batch 156, generator loss: 0.7218327522277832, discriminator loss: 1.1684775352478027 
Batch 157, generator loss: 0.7323608994483948, discriminator loss: 1.2092089653015137 
Batch 158, generator loss: 0.7359941601753235, discriminator loss: 1.2506968975067139 
Batch 159, generator loss: 0.7477174997329712, discriminator loss: 1.2345561981201172 
Batch 160, generator loss: 0.7409435510635376, discriminator loss: 1.212827205657959 
Batch 161, generator loss: 0.7306491136550903, discriminator loss: 1.2212064266204834 
Batch 162, generator loss: 0.7213736772537231, discriminator loss: 1.2498217821121216 
Batch 163, generator loss: 0.7121812701225281, discriminator loss: 1.2817133665084839 
Batch 164, generator loss: 0.7040174007415771, discriminator loss: 1.2564198970794678 
Batch 165, generator loss: 0.6803549528121948, discriminator loss: 1.3075873851776123 
Batch 166, generator loss: 0.6647288799285889, discriminator loss: 1.3317463397979736 
Batch 167, generator loss: 0.6588728427886963, discriminator loss: 1.3891403675079346 
Batch 168, generator loss: 0.6626070737838745, discriminator loss: 1.3038876056671143 
Batch 169, generator loss: 0.6526502966880798, discriminator loss: 1.3841354846954346 
Batch 170, generator loss: 0.6495256423950195, discriminator loss: 1.3809736967086792 
Batch 171, generator loss: 0.6618605256080627, discriminator loss: 1.3451138734817505 
Batch 172, generator loss: 0.6599758863449097, discriminator loss: 1.3878262042999268 
Batch 173, generator loss: 0.6663800477981567, discriminator loss: 1.3314461708068848 
Batch 174, generator loss: 0.6954221725463867, discriminator loss: 1.362743854522705 
Batch 175, generator loss: 0.6972870826721191, discriminator loss: 1.33285653591156 
Batch 176, generator loss: 0.6902202367782593, discriminator loss: 1.3143630027770996 
Batch 177, generator loss: 0.7065768241882324, discriminator loss: 1.2789437770843506 
Batch 178, generator loss: 0.7058732509613037, discriminator loss: 1.2875088453292847 
Batch 179, generator loss: 0.6934618949890137, discriminator loss: 1.3321526050567627 
Batch 180, generator loss: 0.6771409511566162, discriminator loss: 1.3197176456451416 
Batch 181, generator loss: 0.6645077466964722, discriminator loss: 1.331005334854126 
Batch 182, generator loss: 0.6514129638671875, discriminator loss: 1.3467528820037842 
Batch 183, generator loss: 0.6458350419998169, discriminator loss: 1.3850141763687134 
Batch 184, generator loss: 0.6344592571258545, discriminator loss: 1.3874711990356445 
Batch 185, generator loss: 0.6347559690475464, discriminator loss: 1.416949987411499 
Batch 186, generator loss: 0.6408743858337402, discriminator loss: 1.41996169090271 
Batch 187, generator loss: 0.6366402506828308, discriminator loss: 1.4241758584976196 
Batch 188, generator loss: 0.6589622497558594, discriminator loss: 1.4267942905426025 
Batch 189, generator loss: 0.658270001411438, discriminator loss: 1.419439435005188 
Batch 190, generator loss: 0.6731255054473877, discriminator loss: 1.5215120315551758 
Batch 191, generator loss: 0.6885229349136353, discriminator loss: 1.4498775005340576 
Batch 192, generator loss: 0.6904083490371704, discriminator loss: 1.488092064857483 
Batch 193, generator loss: 0.6989916563034058, discriminator loss: 1.44977605342865 
Batch 194, generator loss: 0.7021461129188538, discriminator loss: 1.4726665019989014 
Batch 195, generator loss: 0.7030303478240967, discriminator loss: 1.4079089164733887 
Batch 196, generator loss: 0.7049447894096375, discriminator loss: 1.4100933074951172 
Batch 197, generator loss: 0.7160073518753052, discriminator loss: 1.39220130443573 
Batch 198, generator loss: 0.7204886674880981, discriminator loss: 1.4058412313461304 
Batch 199, generator loss: 0.7128587961196899, discriminator loss: 1.4131263494491577 
Batch 200, generator loss: 0.7148427963256836, discriminator loss: 1.3734569549560547 
Batch 201, generator loss: 0.703848123550415, discriminator loss: 1.3652539253234863 
Batch 202, generator loss: 0.7105013132095337, discriminator loss: 1.364762783050537 
Batch 203, generator loss: 0.7001361846923828, discriminator loss: 1.3677551746368408 
Batch 204, generator loss: 0.7207179069519043, discriminator loss: 1.392863392829895 
Batch 205, generator loss: 0.722991943359375, discriminator loss: 1.3734736442565918 
Batch 206, generator loss: 0.723838210105896, discriminator loss: 1.364241361618042 
Batch 207, generator loss: 0.7380948066711426, discriminator loss: 1.381242275238037 
Batch 208, generator loss: 0.7338572144508362, discriminator loss: 1.3876264095306396 
Batch 209, generator loss: 0.7463905811309814, discriminator loss: 1.3682456016540527 
Batch 210, generator loss: 0.7421824932098389, discriminator loss: 1.3917161226272583 
Batch 211, generator loss: 0.7542355060577393, discriminator loss: 1.3997561931610107 
Batch 212, generator loss: 0.7504892945289612, discriminator loss: 1.3890639543533325 
Batch 213, generator loss: 0.7636957168579102, discriminator loss: 1.3745031356811523 
Batch 214, generator loss: 0.7542872428894043, discriminator loss: 1.3664312362670898 
Batch 215, generator loss: 0.7570244073867798, discriminator loss: 1.381657361984253 
Batch 216, generator loss: 0.7724464535713196, discriminator loss: 1.341500997543335 
Batch 217, generator loss: 0.7756137847900391, discriminator loss: 1.3366018533706665 
Batch 218, generator loss: 0.7958952188491821, discriminator loss: 1.3050299882888794 
Batch 219, generator loss: 0.822745680809021, discriminator loss: 1.2939260005950928 
Batch 220, generator loss: 0.8098060488700867, discriminator loss: 1.2552510499954224 
Batch 221, generator loss: 0.822847843170166, discriminator loss: 1.2267186641693115 
Batch 222, generator loss: 0.8206262588500977, discriminator loss: 1.187643051147461 
Batch 223, generator loss: 0.8337602615356445, discriminator loss: 1.183427333831787 
Batch 224, generator loss: 0.8468711376190186, discriminator loss: 1.1640764474868774 
Batch 225, generator loss: 0.8612486124038696, discriminator loss: 1.1478264331817627 
Batch 226, generator loss: 0.8875383734703064, discriminator loss: 1.0749696493148804 
Batch 227, generator loss: 0.8917399048805237, discriminator loss: 1.0810538530349731 
Batch 228, generator loss: 0.8906624913215637, discriminator loss: 1.0697416067123413 
Batch 229, generator loss: 0.9102814793586731, discriminator loss: 1.0519611835479736 
Batch 230, generator loss: 0.9232953786849976, discriminator loss: 1.013317346572876 
Batch 231, generator loss: 0.9280562996864319, discriminator loss: 1.0092153549194336 
Batch 232, generator loss: 0.9376777410507202, discriminator loss: 0.9946489334106445 
Batch 233, generator loss: 0.9505308866500854, discriminator loss: 0.9861108064651489 
Batch 234, generator loss: 0.9528689384460449, discriminator loss: 0.9778193235397339 
Batch 235, generator loss: 0.9591952562332153, discriminator loss: 0.9811542630195618 
Batch 236, generator loss: 0.9682015776634216, discriminator loss: 0.9531605243682861 
Generator loss: tf.Tensor(0.9682016, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(0.9531605, shape=(), dtype=float32)
EPOCH: 1
Batch 258, generator loss: 0.9754214286804199, discriminator loss: 0.9400163888931274 
Batch 259, generator loss: 0.9502513408660889, discriminator loss: 0.9558801651000977 
Batch 260, generator loss: 0.9524343013763428, discriminator loss: 0.9419140815734863 
Batch 261, generator loss: 0.9504857063293457, discriminator loss: 0.9736549258232117 
Batch 262, generator loss: 0.9666638374328613, discriminator loss: 0.9600245356559753 
Batch 263, generator loss: 0.9568765163421631, discriminator loss: 1.0148717164993286 
Batch 264, generator loss: 0.9496458768844604, discriminator loss: 1.0037362575531006 
Batch 265, generator loss: 0.9152414798736572, discriminator loss: 0.9969149827957153 
Batch 266, generator loss: 0.8971821069717407, discriminator loss: 1.0027341842651367 
Batch 267, generator loss: 0.8792048096656799, discriminator loss: 1.0565590858459473 
Batch 268, generator loss: 0.8524138927459717, discriminator loss: 1.1011605262756348 
Batch 269, generator loss: 0.835712194442749, discriminator loss: 1.1180930137634277 
Batch 270, generator loss: 0.8255786895751953, discriminator loss: 1.11741304397583 
Batch 271, generator loss: 0.8284300565719604, discriminator loss: 1.1673901081085205 
Batch 272, generator loss: 0.7967692017555237, discriminator loss: 1.2006932497024536 
Batch 273, generator loss: 0.8082000017166138, discriminator loss: 1.2101340293884277 
Batch 274, generator loss: 0.8056668043136597, discriminator loss: 1.2266430854797363 
Batch 275, generator loss: 0.816270112991333, discriminator loss: 1.2248481512069702 
Batch 276, generator loss: 0.8241536617279053, discriminator loss: 1.2628413438796997 
Batch 277, generator loss: 0.8383093476295471, discriminator loss: 1.209376573562622 
Batch 278, generator loss: 0.819778323173523, discriminator loss: 1.2887059450149536 
Batch 279, generator loss: 0.8112009763717651, discriminator loss: 1.3039910793304443 
Batch 280, generator loss: 0.7931772470474243, discriminator loss: 1.2944695949554443 
Batch 281, generator loss: 0.7556411027908325, discriminator loss: 1.2901259660720825 
Batch 282, generator loss: 0.7154108285903931, discriminator loss: 1.3311381340026855 
Batch 283, generator loss: 0.6762062907218933, discriminator loss: 1.3732656240463257 
Batch 284, generator loss: 0.6563793420791626, discriminator loss: 1.3526225090026855 
Batch 285, generator loss: 0.6477816104888916, discriminator loss: 1.411171317100525 
Batch 286, generator loss: 0.6234860420227051, discriminator loss: 1.4025527238845825 
Batch 287, generator loss: 0.623365044593811, discriminator loss: 1.4124383926391602 
Batch 288, generator loss: 0.6206625699996948, discriminator loss: 1.460822582244873 
Batch 289, generator loss: 0.6277259588241577, discriminator loss: 1.4370213747024536 
Batch 290, generator loss: 0.6172966361045837, discriminator loss: 1.5306718349456787 
Batch 291, generator loss: 0.631366491317749, discriminator loss: 1.499271035194397 
Batch 292, generator loss: 0.6330230236053467, discriminator loss: 1.547886610031128 
Batch 293, generator loss: 0.6429023742675781, discriminator loss: 1.451963186264038 
Batch 294, generator loss: 0.6453472375869751, discriminator loss: 1.503349781036377 
Batch 295, generator loss: 0.6358159780502319, discriminator loss: 1.513225793838501 
Batch 296, generator loss: 0.6309400796890259, discriminator loss: 1.5117082595825195 
Batch 297, generator loss: 0.621015191078186, discriminator loss: 1.4967541694641113 
Batch 298, generator loss: 0.6089745759963989, discriminator loss: 1.5098676681518555 
Batch 299, generator loss: 0.5865451097488403, discriminator loss: 1.574817419052124 
Batch 300, generator loss: 0.5778562426567078, discriminator loss: 1.528425931930542 
Batch 301, generator loss: 0.5536774396896362, discriminator loss: 1.546787977218628 
Batch 302, generator loss: 0.5570698976516724, discriminator loss: 1.508689045906067 
Batch 303, generator loss: 0.5507030487060547, discriminator loss: 1.5026583671569824 
Batch 304, generator loss: 0.5736672282218933, discriminator loss: 1.4829167127609253 
Batch 305, generator loss: 0.5979810953140259, discriminator loss: 1.416677713394165 
Batch 306, generator loss: 0.6102561950683594, discriminator loss: 1.4533164501190186 
Batch 307, generator loss: 0.6434586048126221, discriminator loss: 1.3532161712646484 
Batch 308, generator loss: 0.6770449280738831, discriminator loss: 1.3155945539474487 
Batch 309, generator loss: 0.692699134349823, discriminator loss: 1.2660247087478638 
Batch 310, generator loss: 0.7204185724258423, discriminator loss: 1.2230944633483887 
Batch 311, generator loss: 0.7747049331665039, discriminator loss: 1.1630113124847412 
Batch 312, generator loss: 0.7806892991065979, discriminator loss: 1.1544837951660156 
Batch 313, generator loss: 0.8120148777961731, discriminator loss: 1.0793565511703491 
Batch 314, generator loss: 0.823137640953064, discriminator loss: 1.0683517456054688 
Batch 315, generator loss: 0.8544024229049683, discriminator loss: 1.0164183378219604 
Batch 316, generator loss: 0.8648671507835388, discriminator loss: 0.9854496717453003 
Batch 317, generator loss: 0.8829399943351746, discriminator loss: 0.9533476233482361 
Batch 318, generator loss: 0.8880835771560669, discriminator loss: 0.9384339451789856 
Batch 319, generator loss: 0.9057359099388123, discriminator loss: 0.9089561700820923 
Batch 320, generator loss: 0.9288803339004517, discriminator loss: 0.8834441304206848 
Batch 321, generator loss: 0.9542146921157837, discriminator loss: 0.8731715679168701 
Batch 322, generator loss: 0.9784629344940186, discriminator loss: 0.8396002054214478 
Batch 323, generator loss: 1.002180814743042, discriminator loss: 0.8494658470153809 
Batch 324, generator loss: 1.0299253463745117, discriminator loss: 0.8399309515953064 
Batch 325, generator loss: 1.065670371055603, discriminator loss: 0.8055365085601807 
Batch 326, generator loss: 1.0619549751281738, discriminator loss: 0.8008745908737183 
Batch 327, generator loss: 1.0773545503616333, discriminator loss: 0.8085694909095764 
Batch 328, generator loss: 1.099190354347229, discriminator loss: 0.798706591129303 
Batch 329, generator loss: 1.1113324165344238, discriminator loss: 0.7611317038536072 
Batch 330, generator loss: 1.1204512119293213, discriminator loss: 0.820799708366394 
Batch 331, generator loss: 1.1223819255828857, discriminator loss: 0.8234549760818481 
Batch 332, generator loss: 1.125281810760498, discriminator loss: 0.7975807189941406 
Batch 333, generator loss: 1.105362892150879, discriminator loss: 0.8165491819381714 
Batch 334, generator loss: 1.0972102880477905, discriminator loss: 0.8549388647079468 
Batch 335, generator loss: 1.0794662237167358, discriminator loss: 0.8428406715393066 
Batch 336, generator loss: 1.0782418251037598, discriminator loss: 0.868170440196991 
Batch 337, generator loss: 1.066042184829712, discriminator loss: 0.9132965207099915 
Batch 338, generator loss: 1.0424561500549316, discriminator loss: 0.8990698456764221 
Batch 339, generator loss: 1.0179593563079834, discriminator loss: 0.9189811944961548 
Batch 340, generator loss: 0.9979468584060669, discriminator loss: 0.9608626365661621 
Batch 341, generator loss: 0.9892393350601196, discriminator loss: 0.9663692712783813 
Batch 342, generator loss: 0.9592316150665283, discriminator loss: 1.0261445045471191 
Batch 343, generator loss: 0.9452676773071289, discriminator loss: 1.0295937061309814 
Batch 344, generator loss: 0.8966192603111267, discriminator loss: 1.1027393341064453 
Batch 345, generator loss: 0.844629168510437, discriminator loss: 1.1755280494689941 
Batch 346, generator loss: 0.8051882386207581, discriminator loss: 1.2123496532440186 
Batch 347, generator loss: 0.7690550088882446, discriminator loss: 1.2745267152786255 
Batch 348, generator loss: 0.7455379962921143, discriminator loss: 1.4211993217468262 
Batch 349, generator loss: 0.7326539158821106, discriminator loss: 1.356268048286438 
Batch 350, generator loss: 0.7659692168235779, discriminator loss: 1.413773775100708 
Batch 351, generator loss: 0.7577821016311646, discriminator loss: 1.461381435394287 
Batch 352, generator loss: 0.7664424777030945, discriminator loss: 1.453843355178833 
Batch 353, generator loss: 0.7747258543968201, discriminator loss: 1.5057475566864014 
Batch 354, generator loss: 0.746763288974762, discriminator loss: 1.5305287837982178 
Batch 355, generator loss: 0.7481441497802734, discriminator loss: 1.4994432926177979 
Batch 356, generator loss: 0.7463160157203674, discriminator loss: 1.4846322536468506 
Batch 357, generator loss: 0.7133495807647705, discriminator loss: 1.5512112379074097 
Batch 358, generator loss: 0.7394987344741821, discriminator loss: 1.4535423517227173 
Batch 359, generator loss: 0.7039065361022949, discriminator loss: 1.5166358947753906 
Batch 360, generator loss: 0.66504967212677, discriminator loss: 1.556034803390503 
Batch 361, generator loss: 0.6579575538635254, discriminator loss: 1.6127768754959106 
Batch 362, generator loss: 0.6797840595245361, discriminator loss: 1.6191824674606323 
Batch 363, generator loss: 0.6745790839195251, discriminator loss: 1.625356674194336 
Batch 364, generator loss: 0.6747305989265442, discriminator loss: 1.643296241760254 
Batch 365, generator loss: 0.6812612414360046, discriminator loss: 1.6245977878570557 
Batch 366, generator loss: 0.6580787897109985, discriminator loss: 1.63125741481781 
Batch 367, generator loss: 0.6457499265670776, discriminator loss: 1.6700310707092285 
Batch 368, generator loss: 0.6544491648674011, discriminator loss: 1.7265806198120117 
Batch 369, generator loss: 0.604933500289917, discriminator loss: 1.8602867126464844 
Batch 370, generator loss: 0.5786839723587036, discriminator loss: 1.7546789646148682 
Batch 371, generator loss: 0.5743107199668884, discriminator loss: 1.8043832778930664 
Batch 372, generator loss: 0.5708876848220825, discriminator loss: 1.8040504455566406 
Batch 373, generator loss: 0.5959101915359497, discriminator loss: 1.737384557723999 
Batch 374, generator loss: 0.5848672389984131, discriminator loss: 1.7661898136138916 
Batch 375, generator loss: 0.6286454200744629, discriminator loss: 1.7866559028625488 
Batch 376, generator loss: 0.6312073469161987, discriminator loss: 1.710810661315918 
Batch 377, generator loss: 0.6436870098114014, discriminator loss: 1.6783719062805176 
Batch 378, generator loss: 0.6651026010513306, discriminator loss: 1.6327142715454102 
Batch 379, generator loss: 0.6790341138839722, discriminator loss: 1.5805370807647705 
Batch 380, generator loss: 0.6714144945144653, discriminator loss: 1.5364917516708374 
Batch 381, generator loss: 0.7143697142601013, discriminator loss: 1.4656643867492676 
Batch 382, generator loss: 0.6978602409362793, discriminator loss: 1.508335828781128 
Batch 383, generator loss: 0.676545262336731, discriminator loss: 1.479101538658142 
Batch 384, generator loss: 0.7071565985679626, discriminator loss: 1.434937834739685 
Batch 385, generator loss: 0.7153465747833252, discriminator loss: 1.402550458908081 
Batch 386, generator loss: 0.7505137324333191, discriminator loss: 1.3293566703796387 
Batch 387, generator loss: 0.780535876750946, discriminator loss: 1.328721046447754 
Batch 388, generator loss: 0.7812080383300781, discriminator loss: 1.3467577695846558 
Batch 389, generator loss: 0.7975822687149048, discriminator loss: 1.3137094974517822 
Batch 390, generator loss: 0.7915458679199219, discriminator loss: 1.295566439628601 
Batch 391, generator loss: 0.7999580502510071, discriminator loss: 1.2605963945388794 
Batch 392, generator loss: 0.7930787801742554, discriminator loss: 1.2486544847488403 
Batch 393, generator loss: 0.8281434774398804, discriminator loss: 1.2210609912872314 
Batch 394, generator loss: 0.8447411060333252, discriminator loss: 1.2064037322998047 
Batch 395, generator loss: 0.8642784953117371, discriminator loss: 1.157658576965332 
Batch 396, generator loss: 0.8540830612182617, discriminator loss: 1.1419734954833984 
Batch 397, generator loss: 0.8463731408119202, discriminator loss: 1.1435297727584839 
Batch 398, generator loss: 0.8693414926528931, discriminator loss: 1.1312860250473022 
Batch 399, generator loss: 0.8785010576248169, discriminator loss: 1.1068775653839111 
Batch 400, generator loss: 0.8951610326766968, discriminator loss: 1.0638253688812256 
Batch 401, generator loss: 0.8962168097496033, discriminator loss: 1.0725748538970947 
Batch 402, generator loss: 0.9201887845993042, discriminator loss: 1.040393352508545 
Batch 403, generator loss: 0.930169939994812, discriminator loss: 1.0232570171356201 
Batch 404, generator loss: 0.9370465278625488, discriminator loss: 1.017003059387207 
Batch 405, generator loss: 0.9503638744354248, discriminator loss: 1.0083584785461426 
Batch 406, generator loss: 0.9762994647026062, discriminator loss: 0.9910770654678345 
Batch 407, generator loss: 0.9579871296882629, discriminator loss: 0.9967771768569946 
Batch 408, generator loss: 0.9753799438476562, discriminator loss: 0.9761002659797668 
Batch 409, generator loss: 0.962315559387207, discriminator loss: 0.9553848505020142 
Batch 410, generator loss: 0.9609555006027222, discriminator loss: 0.9797103404998779 
Batch 411, generator loss: 0.9588254690170288, discriminator loss: 0.9508301019668579 
Batch 412, generator loss: 0.9732257127761841, discriminator loss: 0.9426164627075195 
Batch 413, generator loss: 0.9736331701278687, discriminator loss: 0.9420884251594543 
Batch 414, generator loss: 0.9911133050918579, discriminator loss: 0.9442363381385803 
Batch 415, generator loss: 0.9780031442642212, discriminator loss: 0.9770549535751343 
Batch 416, generator loss: 0.9808555841445923, discriminator loss: 0.9524366855621338 
Batch 417, generator loss: 0.9943224787712097, discriminator loss: 0.9601938128471375 
Batch 418, generator loss: 1.0046840906143188, discriminator loss: 0.9594743251800537 
Batch 419, generator loss: 0.9845427870750427, discriminator loss: 0.9893661737442017 
Batch 420, generator loss: 0.9712125658988953, discriminator loss: 0.9985892176628113 
Batch 421, generator loss: 0.9497478008270264, discriminator loss: 0.997685968875885 
Batch 422, generator loss: 0.9260702133178711, discriminator loss: 1.0148743391036987 
Batch 423, generator loss: 0.9124764800071716, discriminator loss: 1.0389244556427002 
Batch 424, generator loss: 0.9008578062057495, discriminator loss: 1.0771803855895996 
Batch 425, generator loss: 0.9161330461502075, discriminator loss: 1.06383216381073 
Batch 426, generator loss: 0.8951377868652344, discriminator loss: 1.1298730373382568 
Batch 427, generator loss: 0.8898333311080933, discriminator loss: 1.101091980934143 
Batch 428, generator loss: 0.8967580795288086, discriminator loss: 1.1439056396484375 
Batch 429, generator loss: 0.8777784705162048, discriminator loss: 1.1825834512710571 
Batch 430, generator loss: 0.8736730813980103, discriminator loss: 1.1890819072723389 
Batch 431, generator loss: 0.8714256286621094, discriminator loss: 1.2337863445281982 
Batch 432, generator loss: 0.8353320360183716, discriminator loss: 1.2403719425201416 
Batch 433, generator loss: 0.7930371761322021, discriminator loss: 1.2588835954666138 
Batch 434, generator loss: 0.8004788756370544, discriminator loss: 1.278423547744751 
Batch 435, generator loss: 0.7753238081932068, discriminator loss: 1.2859580516815186 
Batch 436, generator loss: 0.764342188835144, discriminator loss: 1.3020758628845215 
Batch 437, generator loss: 0.7803043723106384, discriminator loss: 1.3172316551208496 
Batch 438, generator loss: 0.7747801542282104, discriminator loss: 1.3434667587280273 
Batch 439, generator loss: 0.7666488289833069, discriminator loss: 1.3774595260620117 
Batch 440, generator loss: 0.7631790637969971, discriminator loss: 1.3678245544433594 
Batch 441, generator loss: 0.7453683614730835, discriminator loss: 1.365706205368042 
Batch 442, generator loss: 0.747434139251709, discriminator loss: 1.387679100036621 
Batch 443, generator loss: 0.7432756423950195, discriminator loss: 1.3630146980285645 
Batch 444, generator loss: 0.7331264019012451, discriminator loss: 1.4242677688598633 
Batch 445, generator loss: 0.7414031028747559, discriminator loss: 1.382861852645874 
Batch 446, generator loss: 0.7388464212417603, discriminator loss: 1.3860888481140137 
Batch 447, generator loss: 0.7410871386528015, discriminator loss: 1.4248783588409424 
Batch 448, generator loss: 0.7362797260284424, discriminator loss: 1.4375585317611694 
Batch 449, generator loss: 0.7357422113418579, discriminator loss: 1.465485692024231 
Batch 450, generator loss: 0.7404706478118896, discriminator loss: 1.474660038948059 
Batch 451, generator loss: 0.7253906726837158, discriminator loss: 1.49253249168396 
Batch 452, generator loss: 0.6985692977905273, discriminator loss: 1.479525089263916 
Batch 453, generator loss: 0.6912769079208374, discriminator loss: 1.516632318496704 
Batch 454, generator loss: 0.679770290851593, discriminator loss: 1.5165766477584839 
Batch 455, generator loss: 0.6662049293518066, discriminator loss: 1.5487048625946045 
Batch 456, generator loss: 0.6474971771240234, discriminator loss: 1.5264784097671509 
Batch 457, generator loss: 0.6669826507568359, discriminator loss: 1.5298218727111816 
Batch 458, generator loss: 0.6651124954223633, discriminator loss: 1.547285795211792 
Batch 459, generator loss: 0.6504340767860413, discriminator loss: 1.572973608970642 
Batch 460, generator loss: 0.6455050706863403, discriminator loss: 1.5230542421340942 
Batch 461, generator loss: 0.6351561546325684, discriminator loss: 1.5513622760772705 
Batch 462, generator loss: 0.65553218126297, discriminator loss: 1.5074032545089722 
Batch 463, generator loss: 0.6659534573554993, discriminator loss: 1.5223678350448608 
Batch 464, generator loss: 0.6677708029747009, discriminator loss: 1.519755244255066 
Batch 465, generator loss: 0.6967718005180359, discriminator loss: 1.4461376667022705 
Batch 466, generator loss: 0.7136764526367188, discriminator loss: 1.4288774728775024 
Batch 467, generator loss: 0.7274308800697327, discriminator loss: 1.40549898147583 
Batch 468, generator loss: 0.7239032983779907, discriminator loss: 1.3812081813812256 
Batch 469, generator loss: 0.7325422167778015, discriminator loss: 1.3358649015426636 
Batch 470, generator loss: 0.7843093276023865, discriminator loss: 1.2574801445007324 
Batch 471, generator loss: 0.8110944032669067, discriminator loss: 1.2418832778930664 
Batch 472, generator loss: 0.7935709953308105, discriminator loss: 1.2711329460144043 
Batch 473, generator loss: 0.8320279121398926, discriminator loss: 1.178983211517334 
Batch 474, generator loss: 0.8489201068878174, discriminator loss: 1.144153356552124 
Batch 475, generator loss: 0.8462375402450562, discriminator loss: 1.1279139518737793 
Batch 476, generator loss: 0.8479262590408325, discriminator loss: 1.0846881866455078 
Batch 477, generator loss: 0.8753504753112793, discriminator loss: 1.0440903902053833 
Batch 478, generator loss: 0.8662765622138977, discriminator loss: 1.076791763305664 
Batch 479, generator loss: 0.9061906933784485, discriminator loss: 1.0476208925247192 
Batch 480, generator loss: 0.8812847137451172, discriminator loss: 1.0687658786773682 
Batch 481, generator loss: 0.8910441994667053, discriminator loss: 1.0545079708099365 
Batch 482, generator loss: 0.8812129497528076, discriminator loss: 1.0452368259429932 
Batch 483, generator loss: 0.8978490829467773, discriminator loss: 1.0639886856079102 
Batch 484, generator loss: 0.9086500406265259, discriminator loss: 1.048882007598877 
Batch 485, generator loss: 0.9132387638092041, discriminator loss: 1.074535608291626 
Batch 486, generator loss: 0.9193837642669678, discriminator loss: 1.068880558013916 
Batch 487, generator loss: 0.8479386568069458, discriminator loss: 1.0942665338516235 
Batch 488, generator loss: 0.8418222665786743, discriminator loss: 1.1260442733764648 
Batch 489, generator loss: 0.8520573377609253, discriminator loss: 1.1480627059936523 
Batch 490, generator loss: 0.8373165130615234, discriminator loss: 1.1812543869018555 
Batch 491, generator loss: 0.8417612314224243, discriminator loss: 1.1791801452636719 
Batch 492, generator loss: 0.82718425989151, discriminator loss: 1.250619888305664 
Generator loss: tf.Tensor(0.82718426, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.2506199, shape=(), dtype=float32)
EPOCH: 2
Batch 514, generator loss: 0.7989754676818848, discriminator loss: 1.2300530672073364 
Batch 515, generator loss: 0.8076128959655762, discriminator loss: 1.2589061260223389 
Batch 516, generator loss: 0.7841346263885498, discriminator loss: 1.2736856937408447 
Batch 517, generator loss: 0.7982190847396851, discriminator loss: 1.247236728668213 
Batch 518, generator loss: 0.7803440093994141, discriminator loss: 1.3497734069824219 
Batch 519, generator loss: 0.7857784032821655, discriminator loss: 1.3126779794692993 
Batch 520, generator loss: 0.7773149013519287, discriminator loss: 1.3828980922698975 
Batch 521, generator loss: 0.76183021068573, discriminator loss: 1.3757915496826172 
Batch 522, generator loss: 0.7290550470352173, discriminator loss: 1.3970434665679932 
Batch 523, generator loss: 0.7526099681854248, discriminator loss: 1.400786280632019 
Batch 524, generator loss: 0.7583756446838379, discriminator loss: 1.403533935546875 
Batch 525, generator loss: 0.7333956956863403, discriminator loss: 1.381385087966919 
Batch 526, generator loss: 0.7250156402587891, discriminator loss: 1.4225810766220093 
Batch 527, generator loss: 0.7239861488342285, discriminator loss: 1.460975170135498 
Batch 528, generator loss: 0.7260966300964355, discriminator loss: 1.4338195323944092 
Batch 529, generator loss: 0.7345913648605347, discriminator loss: 1.4884216785430908 
Batch 530, generator loss: 0.7353497743606567, discriminator loss: 1.4851471185684204 
Batch 531, generator loss: 0.7458910942077637, discriminator loss: 1.4527528285980225 
Batch 532, generator loss: 0.7080600261688232, discriminator loss: 1.514328956604004 
Batch 533, generator loss: 0.7094641923904419, discriminator loss: 1.4413515329360962 
Batch 534, generator loss: 0.7344757318496704, discriminator loss: 1.4511404037475586 
Batch 535, generator loss: 0.7300382852554321, discriminator loss: 1.480832576751709 
Batch 536, generator loss: 0.7334083914756775, discriminator loss: 1.4993388652801514 
Batch 537, generator loss: 0.7609528303146362, discriminator loss: 1.4156122207641602 
Batch 538, generator loss: 0.7454267144203186, discriminator loss: 1.4713214635849 
Batch 539, generator loss: 0.7376879453659058, discriminator loss: 1.4353199005126953 
Batch 540, generator loss: 0.7476050853729248, discriminator loss: 1.426945447921753 
Batch 541, generator loss: 0.7641503810882568, discriminator loss: 1.399186611175537 
Batch 542, generator loss: 0.8097779154777527, discriminator loss: 1.3918683528900146 
Batch 543, generator loss: 0.7959229946136475, discriminator loss: 1.3950483798980713 
Batch 544, generator loss: 0.8263683319091797, discriminator loss: 1.3383195400238037 
Batch 545, generator loss: 0.7992720603942871, discriminator loss: 1.3046767711639404 
Batch 546, generator loss: 0.8043221235275269, discriminator loss: 1.3118666410446167 
Batch 547, generator loss: 0.8085924983024597, discriminator loss: 1.2885568141937256 
Batch 548, generator loss: 0.8341136574745178, discriminator loss: 1.2940293550491333 
Batch 549, generator loss: 0.8481824398040771, discriminator loss: 1.222179651260376 
Batch 550, generator loss: 0.8655588626861572, discriminator loss: 1.1876132488250732 
Batch 551, generator loss: 0.8869184255599976, discriminator loss: 1.1881749629974365 
Batch 552, generator loss: 0.9134424924850464, discriminator loss: 1.1645166873931885 
Batch 553, generator loss: 0.9335119128227234, discriminator loss: 1.1384868621826172 
Batch 554, generator loss: 0.9378422498703003, discriminator loss: 1.1252933740615845 
Batch 555, generator loss: 0.9282491207122803, discriminator loss: 1.0655229091644287 
Batch 556, generator loss: 0.9380824565887451, discriminator loss: 1.0558748245239258 
Batch 557, generator loss: 0.909233808517456, discriminator loss: 1.0377252101898193 
Batch 558, generator loss: 0.9666263461112976, discriminator loss: 1.0418872833251953 
Batch 559, generator loss: 0.9784992933273315, discriminator loss: 1.0217928886413574 
Batch 560, generator loss: 0.9730804562568665, discriminator loss: 1.0245739221572876 
Batch 561, generator loss: 1.0035492181777954, discriminator loss: 0.9833322763442993 
Batch 562, generator loss: 1.0381731986999512, discriminator loss: 0.9222010970115662 
Batch 563, generator loss: 1.0296059846878052, discriminator loss: 0.9852021336555481 
Batch 564, generator loss: 1.039750337600708, discriminator loss: 0.9712827801704407 
Batch 565, generator loss: 1.0162677764892578, discriminator loss: 0.9393925070762634 
Batch 566, generator loss: 1.0294692516326904, discriminator loss: 0.920626699924469 
Batch 567, generator loss: 1.0369799137115479, discriminator loss: 0.9377354383468628 
Batch 568, generator loss: 1.072336196899414, discriminator loss: 0.9404899477958679 
Batch 569, generator loss: 1.0448299646377563, discriminator loss: 0.9548266530036926 
Batch 570, generator loss: 1.046101689338684, discriminator loss: 0.9125416278839111 
Batch 571, generator loss: 1.0354156494140625, discriminator loss: 0.9140139818191528 
Batch 572, generator loss: 1.029693841934204, discriminator loss: 0.9313638210296631 
Batch 573, generator loss: 1.0493443012237549, discriminator loss: 0.9210764765739441 
Batch 574, generator loss: 1.0531718730926514, discriminator loss: 0.9360948801040649 
Batch 575, generator loss: 1.0601539611816406, discriminator loss: 0.936142086982727 
Batch 576, generator loss: 1.0666195154190063, discriminator loss: 0.9327308535575867 
Batch 577, generator loss: 1.0490822792053223, discriminator loss: 0.9709514379501343 
Batch 578, generator loss: 1.0319315195083618, discriminator loss: 0.9568771123886108 
Batch 579, generator loss: 1.0360126495361328, discriminator loss: 0.945460319519043 
Batch 580, generator loss: 1.0334553718566895, discriminator loss: 0.965518593788147 
Batch 581, generator loss: 1.0434726476669312, discriminator loss: 0.967155396938324 
Batch 582, generator loss: 1.0361363887786865, discriminator loss: 0.9715620279312134 
Batch 583, generator loss: 1.0481458902359009, discriminator loss: 0.9985065460205078 
Batch 584, generator loss: 1.0286756753921509, discriminator loss: 1.0481704473495483 
Batch 585, generator loss: 1.0378611087799072, discriminator loss: 1.0278074741363525 
Batch 586, generator loss: 1.0324933528900146, discriminator loss: 1.027710199356079 
Batch 587, generator loss: 0.9847152233123779, discriminator loss: 1.0579538345336914 
Batch 588, generator loss: 0.9739707708358765, discriminator loss: 1.005913257598877 
Batch 589, generator loss: 0.9728906154632568, discriminator loss: 1.0977509021759033 
Batch 590, generator loss: 0.9728748798370361, discriminator loss: 1.0852261781692505 
Batch 591, generator loss: 0.9694713354110718, discriminator loss: 1.11152184009552 
Batch 592, generator loss: 0.9639557600021362, discriminator loss: 1.1255394220352173 
Batch 593, generator loss: 0.9568395614624023, discriminator loss: 1.1298539638519287 
Batch 594, generator loss: 0.9199293851852417, discriminator loss: 1.150130271911621 
Batch 595, generator loss: 0.9130197167396545, discriminator loss: 1.1822404861450195 
Batch 596, generator loss: 0.8784456849098206, discriminator loss: 1.252500057220459 
Batch 597, generator loss: 0.8818110227584839, discriminator loss: 1.3146717548370361 
Batch 598, generator loss: 0.8449987769126892, discriminator loss: 1.2647608518600464 
Batch 599, generator loss: 0.8156281113624573, discriminator loss: 1.316537857055664 
Batch 600, generator loss: 0.7925000190734863, discriminator loss: 1.3078155517578125 
Batch 601, generator loss: 0.7761418223381042, discriminator loss: 1.3098268508911133 
Batch 602, generator loss: 0.7954435348510742, discriminator loss: 1.3413591384887695 
Batch 603, generator loss: 0.8086988925933838, discriminator loss: 1.4131301641464233 
Batch 604, generator loss: 0.7770413756370544, discriminator loss: 1.371660590171814 
Batch 605, generator loss: 0.7695884704589844, discriminator loss: 1.4500244855880737 
Batch 606, generator loss: 0.7754039764404297, discriminator loss: 1.37136709690094 
Batch 607, generator loss: 0.7692865133285522, discriminator loss: 1.4173736572265625 
Batch 608, generator loss: 0.7497663497924805, discriminator loss: 1.512225866317749 
Batch 609, generator loss: 0.7344808578491211, discriminator loss: 1.4179182052612305 
Batch 610, generator loss: 0.7213665843009949, discriminator loss: 1.4624632596969604 
Batch 611, generator loss: 0.7300111055374146, discriminator loss: 1.4494707584381104 
Batch 612, generator loss: 0.7318230867385864, discriminator loss: 1.461047649383545 
Batch 613, generator loss: 0.6993504762649536, discriminator loss: 1.4506657123565674 
Batch 614, generator loss: 0.7661499977111816, discriminator loss: 1.4173364639282227 
Batch 615, generator loss: 0.7690290212631226, discriminator loss: 1.4120323657989502 
Batch 616, generator loss: 0.7234125137329102, discriminator loss: 1.4152129888534546 
Batch 617, generator loss: 0.7303343415260315, discriminator loss: 1.4393501281738281 
Batch 618, generator loss: 0.7520873546600342, discriminator loss: 1.3840327262878418 
Batch 619, generator loss: 0.7620477676391602, discriminator loss: 1.335146427154541 
Batch 620, generator loss: 0.7966018319129944, discriminator loss: 1.3164725303649902 
Batch 621, generator loss: 0.7717746496200562, discriminator loss: 1.372322678565979 
Batch 622, generator loss: 0.7614707350730896, discriminator loss: 1.314174771308899 
Batch 623, generator loss: 0.7712010145187378, discriminator loss: 1.3034372329711914 
Batch 624, generator loss: 0.770889401435852, discriminator loss: 1.2702016830444336 
Batch 625, generator loss: 0.7724404335021973, discriminator loss: 1.262273907661438 
Batch 626, generator loss: 0.7989072203636169, discriminator loss: 1.217459797859192 
Batch 627, generator loss: 0.8393470048904419, discriminator loss: 1.192335605621338 
Batch 628, generator loss: 0.8495603799819946, discriminator loss: 1.188820719718933 
Batch 629, generator loss: 0.8730289340019226, discriminator loss: 1.1737287044525146 
Batch 630, generator loss: 0.8697768449783325, discriminator loss: 1.1567927598953247 
Batch 631, generator loss: 0.90699702501297, discriminator loss: 1.1412066221237183 
Batch 632, generator loss: 0.8650522232055664, discriminator loss: 1.1849069595336914 
Batch 633, generator loss: 0.9014347791671753, discriminator loss: 1.1234694719314575 
Batch 634, generator loss: 0.8622104525566101, discriminator loss: 1.1444971561431885 
Batch 635, generator loss: 0.8605227470397949, discriminator loss: 1.109168291091919 
Batch 636, generator loss: 0.8504706621170044, discriminator loss: 1.115335464477539 
Batch 637, generator loss: 0.8365523219108582, discriminator loss: 1.1402747631072998 
Batch 638, generator loss: 0.903522253036499, discriminator loss: 1.0800665616989136 
Batch 639, generator loss: 0.9388105869293213, discriminator loss: 1.0743129253387451 
Batch 640, generator loss: 0.9431099891662598, discriminator loss: 1.079864263534546 
Batch 641, generator loss: 0.9018206596374512, discriminator loss: 1.1058458089828491 
Batch 642, generator loss: 0.8908308744430542, discriminator loss: 1.1656992435455322 
Batch 643, generator loss: 0.8889422416687012, discriminator loss: 1.1296133995056152 
Batch 644, generator loss: 0.8594236969947815, discriminator loss: 1.0862135887145996 
Batch 645, generator loss: 0.8289400935173035, discriminator loss: 1.1664707660675049 
Batch 646, generator loss: 0.8457974195480347, discriminator loss: 1.1297298669815063 
Batch 647, generator loss: 0.8754872679710388, discriminator loss: 1.1444392204284668 
Batch 648, generator loss: 0.8666409254074097, discriminator loss: 1.1593955755233765 
Batch 649, generator loss: 0.892247200012207, discriminator loss: 1.1541157960891724 
Batch 650, generator loss: 0.8819168210029602, discriminator loss: 1.2186862230300903 
Batch 651, generator loss: 0.8214417695999146, discriminator loss: 1.1593317985534668 
Batch 652, generator loss: 0.8117069602012634, discriminator loss: 1.2610958814620972 
Batch 653, generator loss: 0.8221280574798584, discriminator loss: 1.237417221069336 
Batch 654, generator loss: 0.8119022250175476, discriminator loss: 1.2708816528320312 
Batch 655, generator loss: 0.8143405318260193, discriminator loss: 1.3163061141967773 
Batch 656, generator loss: 0.7938694357872009, discriminator loss: 1.343201994895935 
Batch 657, generator loss: 0.7775405645370483, discriminator loss: 1.2912476062774658 
Batch 658, generator loss: 0.7613519430160522, discriminator loss: 1.3628840446472168 
Batch 659, generator loss: 0.7497751712799072, discriminator loss: 1.3883943557739258 
Batch 660, generator loss: 0.7105666399002075, discriminator loss: 1.4016327857971191 
Batch 661, generator loss: 0.7220346331596375, discriminator loss: 1.430249810218811 
Batch 662, generator loss: 0.7180461883544922, discriminator loss: 1.4004716873168945 
Batch 663, generator loss: 0.7561192512512207, discriminator loss: 1.4945915937423706 
Batch 664, generator loss: 0.7680214643478394, discriminator loss: 1.4765939712524414 
Batch 665, generator loss: 0.752647876739502, discriminator loss: 1.4810419082641602 
Batch 666, generator loss: 0.7193480730056763, discriminator loss: 1.4683798551559448 
Batch 667, generator loss: 0.7055646181106567, discriminator loss: 1.4793784618377686 
Batch 668, generator loss: 0.6987016201019287, discriminator loss: 1.4965622425079346 
Batch 669, generator loss: 0.6938086748123169, discriminator loss: 1.5546534061431885 
Batch 670, generator loss: 0.7140923738479614, discriminator loss: 1.4632377624511719 
Batch 671, generator loss: 0.7308386564254761, discriminator loss: 1.5416581630706787 
Batch 672, generator loss: 0.7424296140670776, discriminator loss: 1.5221095085144043 
Batch 673, generator loss: 0.7537802457809448, discriminator loss: 1.4824340343475342 
Batch 674, generator loss: 0.7446467876434326, discriminator loss: 1.4837734699249268 
Batch 675, generator loss: 0.7599306106567383, discriminator loss: 1.5030889511108398 
Batch 676, generator loss: 0.7335141897201538, discriminator loss: 1.4908558130264282 
Batch 677, generator loss: 0.7307660579681396, discriminator loss: 1.4575326442718506 
Batch 678, generator loss: 0.7145951986312866, discriminator loss: 1.4844262599945068 
Batch 679, generator loss: 0.744947075843811, discriminator loss: 1.4237356185913086 
Batch 680, generator loss: 0.770355761051178, discriminator loss: 1.4261713027954102 
Batch 681, generator loss: 0.7847299575805664, discriminator loss: 1.43488347530365 
Batch 682, generator loss: 0.7955809831619263, discriminator loss: 1.3835346698760986 
Batch 683, generator loss: 0.8045583367347717, discriminator loss: 1.4429646730422974 
Batch 684, generator loss: 0.7944204807281494, discriminator loss: 1.3455188274383545 
Batch 685, generator loss: 0.7928603887557983, discriminator loss: 1.3495512008666992 
Batch 686, generator loss: 0.7953460216522217, discriminator loss: 1.3304953575134277 
Batch 687, generator loss: 0.7936347723007202, discriminator loss: 1.3111882209777832 
Batch 688, generator loss: 0.8185670375823975, discriminator loss: 1.2675994634628296 
Batch 689, generator loss: 0.8214372396469116, discriminator loss: 1.2862403392791748 
Batch 690, generator loss: 0.8488134145736694, discriminator loss: 1.2573366165161133 
Batch 691, generator loss: 0.8502357006072998, discriminator loss: 1.253450632095337 
Batch 692, generator loss: 0.8691266775131226, discriminator loss: 1.279076099395752 
Batch 693, generator loss: 0.8675112128257751, discriminator loss: 1.260339379310608 
Batch 694, generator loss: 0.8302911520004272, discriminator loss: 1.2615408897399902 
Batch 695, generator loss: 0.8513654470443726, discriminator loss: 1.1903023719787598 
Batch 696, generator loss: 0.821444034576416, discriminator loss: 1.2394136190414429 
Batch 697, generator loss: 0.8113192319869995, discriminator loss: 1.211271047592163 
Batch 698, generator loss: 0.8265575170516968, discriminator loss: 1.18965744972229 
Batch 699, generator loss: 0.868690013885498, discriminator loss: 1.1994684934616089 
Batch 700, generator loss: 0.8726969361305237, discriminator loss: 1.1923019886016846 
Batch 701, generator loss: 0.8901399970054626, discriminator loss: 1.152491569519043 
Batch 702, generator loss: 0.8746870756149292, discriminator loss: 1.1892169713974 
Batch 703, generator loss: 0.8671627044677734, discriminator loss: 1.1809494495391846 
Batch 704, generator loss: 0.8836158514022827, discriminator loss: 1.1921954154968262 
Batch 705, generator loss: 0.8730562925338745, discriminator loss: 1.1435835361480713 
Batch 706, generator loss: 0.8493973016738892, discriminator loss: 1.1782705783843994 
Batch 707, generator loss: 0.8479504585266113, discriminator loss: 1.1690404415130615 
Batch 708, generator loss: 0.831045389175415, discriminator loss: 1.1811020374298096 
Batch 709, generator loss: 0.8671790957450867, discriminator loss: 1.203089714050293 
Batch 710, generator loss: 0.8538999557495117, discriminator loss: 1.185241937637329 
Batch 711, generator loss: 0.8821872472763062, discriminator loss: 1.1624929904937744 
Batch 712, generator loss: 0.8490462303161621, discriminator loss: 1.173073172569275 
Batch 713, generator loss: 0.8455320596694946, discriminator loss: 1.21565580368042 
Batch 714, generator loss: 0.8403019905090332, discriminator loss: 1.2205606698989868 
Batch 715, generator loss: 0.8307896852493286, discriminator loss: 1.2221248149871826 
Batch 716, generator loss: 0.8285921812057495, discriminator loss: 1.2034285068511963 
Batch 717, generator loss: 0.816322922706604, discriminator loss: 1.2282321453094482 
Batch 718, generator loss: 0.8026047945022583, discriminator loss: 1.2350448369979858 
Batch 719, generator loss: 0.7870403528213501, discriminator loss: 1.2508444786071777 
Batch 720, generator loss: 0.7826122045516968, discriminator loss: 1.2557775974273682 
Batch 721, generator loss: 0.7973864078521729, discriminator loss: 1.3403980731964111 
Batch 722, generator loss: 0.8247624635696411, discriminator loss: 1.3247978687286377 
Batch 723, generator loss: 0.756757378578186, discriminator loss: 1.301025152206421 
Batch 724, generator loss: 0.7841744422912598, discriminator loss: 1.2833435535430908 
Batch 725, generator loss: 0.7558624744415283, discriminator loss: 1.333329677581787 
Batch 726, generator loss: 0.7355395555496216, discriminator loss: 1.3585929870605469 
Batch 727, generator loss: 0.7507982850074768, discriminator loss: 1.357384443283081 
Batch 728, generator loss: 0.7258511185646057, discriminator loss: 1.4468655586242676 
Batch 729, generator loss: 0.7309862971305847, discriminator loss: 1.4166014194488525 
Batch 730, generator loss: 0.7076153755187988, discriminator loss: 1.4194433689117432 
Batch 731, generator loss: 0.6900452375411987, discriminator loss: 1.4311121702194214 
Batch 732, generator loss: 0.6711280345916748, discriminator loss: 1.4421651363372803 
Batch 733, generator loss: 0.6706650257110596, discriminator loss: 1.4829989671707153 
Batch 734, generator loss: 0.7067499160766602, discriminator loss: 1.4295248985290527 
Batch 735, generator loss: 0.6897348165512085, discriminator loss: 1.480209231376648 
Batch 736, generator loss: 0.6915117502212524, discriminator loss: 1.4680283069610596 
Batch 737, generator loss: 0.6752490997314453, discriminator loss: 1.4974020719528198 
Batch 738, generator loss: 0.6638568639755249, discriminator loss: 1.4855287075042725 
Batch 739, generator loss: 0.6650525331497192, discriminator loss: 1.5032260417938232 
Batch 740, generator loss: 0.6528440117835999, discriminator loss: 1.444471836090088 
Batch 741, generator loss: 0.6589957475662231, discriminator loss: 1.485434651374817 
Batch 742, generator loss: 0.6333816051483154, discriminator loss: 1.4877190589904785 
Batch 743, generator loss: 0.6772729754447937, discriminator loss: 1.4879281520843506 
Batch 744, generator loss: 0.6750154495239258, discriminator loss: 1.483332872390747 
Batch 745, generator loss: 0.6822154521942139, discriminator loss: 1.4575374126434326 
Batch 746, generator loss: 0.6900055408477783, discriminator loss: 1.4865186214447021 
Batch 747, generator loss: 0.6815318465232849, discriminator loss: 1.4680395126342773 
Batch 748, generator loss: 0.6644854545593262, discriminator loss: 1.450697898864746 
Generator loss: tf.Tensor(0.66448545, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.4506979, shape=(), dtype=float32)
EPOCH: 3
Batch 770, generator loss: 0.661017894744873, discriminator loss: 1.4519643783569336 
Batch 771, generator loss: 0.6597184538841248, discriminator loss: 1.4077571630477905 
Batch 772, generator loss: 0.6587071418762207, discriminator loss: 1.4469842910766602 
Batch 773, generator loss: 0.6651638746261597, discriminator loss: 1.3892958164215088 
Batch 774, generator loss: 0.6679654717445374, discriminator loss: 1.4199892282485962 
Batch 775, generator loss: 0.7114351391792297, discriminator loss: 1.3590083122253418 
Batch 776, generator loss: 0.7014487385749817, discriminator loss: 1.395865797996521 
Batch 777, generator loss: 0.7150627970695496, discriminator loss: 1.3686704635620117 
Batch 778, generator loss: 0.7507641315460205, discriminator loss: 1.343421220779419 
Batch 779, generator loss: 0.743617057800293, discriminator loss: 1.3285247087478638 
Batch 780, generator loss: 0.7379870414733887, discriminator loss: 1.3026652336120605 
Batch 781, generator loss: 0.7301464080810547, discriminator loss: 1.2956444025039673 
Batch 782, generator loss: 0.7551295757293701, discriminator loss: 1.2579841613769531 
Batch 783, generator loss: 0.7461980581283569, discriminator loss: 1.2759571075439453 
Batch 784, generator loss: 0.7775789499282837, discriminator loss: 1.200535774230957 
Batch 785, generator loss: 0.7866538166999817, discriminator loss: 1.2343268394470215 
Batch 786, generator loss: 0.7860070466995239, discriminator loss: 1.212878942489624 
Batch 787, generator loss: 0.7794603109359741, discriminator loss: 1.2119274139404297 
Batch 788, generator loss: 0.8178795576095581, discriminator loss: 1.183669090270996 
Batch 789, generator loss: 0.8395842909812927, discriminator loss: 1.1942932605743408 
Batch 790, generator loss: 0.8545047044754028, discriminator loss: 1.1553730964660645 
Batch 791, generator loss: 0.8526273965835571, discriminator loss: 1.1308581829071045 
Batch 792, generator loss: 0.8742207288742065, discriminator loss: 1.1479696035385132 
Batch 793, generator loss: 0.8792948722839355, discriminator loss: 1.105331540107727 
Batch 794, generator loss: 0.8523044586181641, discriminator loss: 1.1378071308135986 
Batch 795, generator loss: 0.855893075466156, discriminator loss: 1.0992954969406128 
Batch 796, generator loss: 0.9005030393600464, discriminator loss: 1.1174664497375488 
Batch 797, generator loss: 0.9358802437782288, discriminator loss: 1.062063455581665 
Batch 798, generator loss: 0.9303421378135681, discriminator loss: 1.0572513341903687 
Batch 799, generator loss: 0.9331725239753723, discriminator loss: 1.0644111633300781 
Batch 800, generator loss: 0.953417956829071, discriminator loss: 1.060896873474121 
Batch 801, generator loss: 0.9622174501419067, discriminator loss: 1.0373977422714233 
Batch 802, generator loss: 0.9525251388549805, discriminator loss: 1.0376241207122803 
Batch 803, generator loss: 0.9484635591506958, discriminator loss: 1.0462782382965088 
Batch 804, generator loss: 0.9339517951011658, discriminator loss: 1.056666374206543 
Batch 805, generator loss: 0.9826757907867432, discriminator loss: 1.0240308046340942 
Batch 806, generator loss: 0.9639777541160583, discriminator loss: 1.065064787864685 
Batch 807, generator loss: 0.9863811135292053, discriminator loss: 1.0472064018249512 
Batch 808, generator loss: 0.9652004241943359, discriminator loss: 1.04400634765625 
Batch 809, generator loss: 0.9420222640037537, discriminator loss: 1.075278401374817 
Batch 810, generator loss: 0.956288754940033, discriminator loss: 1.0298187732696533 
Batch 811, generator loss: 0.9550300240516663, discriminator loss: 1.0655642747879028 
Batch 812, generator loss: 0.9817900061607361, discriminator loss: 1.0678683519363403 
Batch 813, generator loss: 0.9686653017997742, discriminator loss: 1.0592806339263916 
Batch 814, generator loss: 0.9509629011154175, discriminator loss: 1.068211317062378 
Batch 815, generator loss: 0.971479058265686, discriminator loss: 1.0733691453933716 
Batch 816, generator loss: 0.9353862404823303, discriminator loss: 1.060671329498291 
Batch 817, generator loss: 0.9444730877876282, discriminator loss: 1.0716896057128906 
Batch 818, generator loss: 0.9469760060310364, discriminator loss: 1.0648552179336548 
Batch 819, generator loss: 0.9370613098144531, discriminator loss: 1.1145261526107788 
Batch 820, generator loss: 0.9436506032943726, discriminator loss: 1.1228992938995361 
Batch 821, generator loss: 0.9573550224304199, discriminator loss: 1.081895351409912 
Batch 822, generator loss: 0.9116671085357666, discriminator loss: 1.127949833869934 
Batch 823, generator loss: 0.8957959413528442, discriminator loss: 1.1024260520935059 
Batch 824, generator loss: 0.898322582244873, discriminator loss: 1.122177243232727 
Batch 825, generator loss: 0.929421067237854, discriminator loss: 1.1310725212097168 
Batch 826, generator loss: 0.924917459487915, discriminator loss: 1.1368111371994019 
Batch 827, generator loss: 0.9610967040061951, discriminator loss: 1.1307878494262695 
Batch 828, generator loss: 0.9238693118095398, discriminator loss: 1.1408483982086182 
Batch 829, generator loss: 0.8932639360427856, discriminator loss: 1.153283953666687 
Batch 830, generator loss: 0.8642275929450989, discriminator loss: 1.180527925491333 
Batch 831, generator loss: 0.8792328834533691, discriminator loss: 1.151062250137329 
Batch 832, generator loss: 0.8643754720687866, discriminator loss: 1.1566030979156494 
Batch 833, generator loss: 0.8803192377090454, discriminator loss: 1.1990947723388672 
Batch 834, generator loss: 0.8971538543701172, discriminator loss: 1.1920597553253174 
Batch 835, generator loss: 0.8894175887107849, discriminator loss: 1.2259610891342163 
Batch 836, generator loss: 0.8820496797561646, discriminator loss: 1.2562263011932373 
Batch 837, generator loss: 0.7925016283988953, discriminator loss: 1.2600083351135254 
Batch 838, generator loss: 0.7858060598373413, discriminator loss: 1.275373935699463 
Batch 839, generator loss: 0.822290301322937, discriminator loss: 1.2672046422958374 
Batch 840, generator loss: 0.805622398853302, discriminator loss: 1.3436601161956787 
Batch 841, generator loss: 0.7945088744163513, discriminator loss: 1.3860774040222168 
Batch 842, generator loss: 0.7884978652000427, discriminator loss: 1.3704546689987183 
Batch 843, generator loss: 0.7449612021446228, discriminator loss: 1.4063925743103027 
Batch 844, generator loss: 0.7468420267105103, discriminator loss: 1.4008073806762695 
Batch 845, generator loss: 0.7389065027236938, discriminator loss: 1.4001983404159546 
Batch 846, generator loss: 0.7124779224395752, discriminator loss: 1.477126955986023 
Batch 847, generator loss: 0.6812921166419983, discriminator loss: 1.4983494281768799 
Batch 848, generator loss: 0.7233264446258545, discriminator loss: 1.4906747341156006 
Batch 849, generator loss: 0.720299243927002, discriminator loss: 1.5169832706451416 
Batch 850, generator loss: 0.7021971940994263, discriminator loss: 1.5328904390335083 
Batch 851, generator loss: 0.6821799874305725, discriminator loss: 1.5640922784805298 
Batch 852, generator loss: 0.6395236253738403, discriminator loss: 1.5511720180511475 
Batch 853, generator loss: 0.632304847240448, discriminator loss: 1.6317164897918701 
Batch 854, generator loss: 0.6087030172348022, discriminator loss: 1.6080594062805176 
Batch 855, generator loss: 0.6509279012680054, discriminator loss: 1.6207828521728516 
Batch 856, generator loss: 0.6680660247802734, discriminator loss: 1.566827416419983 
Batch 857, generator loss: 0.6666218042373657, discriminator loss: 1.6091766357421875 
Batch 858, generator loss: 0.6645273566246033, discriminator loss: 1.6056461334228516 
Batch 859, generator loss: 0.6467182040214539, discriminator loss: 1.578527808189392 
Batch 860, generator loss: 0.6050583124160767, discriminator loss: 1.6100776195526123 
Batch 861, generator loss: 0.6012921333312988, discriminator loss: 1.6361727714538574 
Batch 862, generator loss: 0.6121534705162048, discriminator loss: 1.573232650756836 
Batch 863, generator loss: 0.633664608001709, discriminator loss: 1.5577600002288818 
Batch 864, generator loss: 0.679850697517395, discriminator loss: 1.5197449922561646 
Batch 865, generator loss: 0.6740303635597229, discriminator loss: 1.5622715950012207 
Batch 866, generator loss: 0.6643662452697754, discriminator loss: 1.5062952041625977 
Batch 867, generator loss: 0.6311640739440918, discriminator loss: 1.5005837678909302 
Batch 868, generator loss: 0.6532241106033325, discriminator loss: 1.4824308156967163 
Batch 869, generator loss: 0.6434159278869629, discriminator loss: 1.461525559425354 
Batch 870, generator loss: 0.671438455581665, discriminator loss: 1.4184433221817017 
Batch 871, generator loss: 0.7077211737632751, discriminator loss: 1.4089856147766113 
Batch 872, generator loss: 0.7304685115814209, discriminator loss: 1.3614728450775146 
Batch 873, generator loss: 0.7204613089561462, discriminator loss: 1.3391902446746826 
Batch 874, generator loss: 0.7289081811904907, discriminator loss: 1.3092889785766602 
Batch 875, generator loss: 0.7497609257698059, discriminator loss: 1.2715907096862793 
Batch 876, generator loss: 0.7621598839759827, discriminator loss: 1.2696398496627808 
Batch 877, generator loss: 0.7492406964302063, discriminator loss: 1.2329912185668945 
Batch 878, generator loss: 0.7710071802139282, discriminator loss: 1.2019089460372925 
Batch 879, generator loss: 0.795560359954834, discriminator loss: 1.196272850036621 
Batch 880, generator loss: 0.8312833309173584, discriminator loss: 1.1452306509017944 
Batch 881, generator loss: 0.8446804285049438, discriminator loss: 1.1332449913024902 
Batch 882, generator loss: 0.8974469304084778, discriminator loss: 1.1082226037979126 
Batch 883, generator loss: 0.8898851871490479, discriminator loss: 1.1034355163574219 
Batch 884, generator loss: 0.8737940788269043, discriminator loss: 1.0634348392486572 
Batch 885, generator loss: 0.8698968887329102, discriminator loss: 1.0540202856063843 
Batch 886, generator loss: 0.9108220338821411, discriminator loss: 1.0313539505004883 
Batch 887, generator loss: 0.9379457235336304, discriminator loss: 1.010083794593811 
Batch 888, generator loss: 0.9649026393890381, discriminator loss: 1.0180702209472656 
Batch 889, generator loss: 0.980652928352356, discriminator loss: 1.022268295288086 
Batch 890, generator loss: 0.9971218109130859, discriminator loss: 1.0117435455322266 
Batch 891, generator loss: 0.9265773296356201, discriminator loss: 1.0110281705856323 
Batch 892, generator loss: 0.9409953355789185, discriminator loss: 1.0041693449020386 
Batch 893, generator loss: 0.9452463388442993, discriminator loss: 0.9812588095664978 
Batch 894, generator loss: 0.9780707955360413, discriminator loss: 0.9983720779418945 
Batch 895, generator loss: 1.0070075988769531, discriminator loss: 0.9909505844116211 
Batch 896, generator loss: 0.9543909430503845, discriminator loss: 1.0666208267211914 
Batch 897, generator loss: 0.9700705409049988, discriminator loss: 1.025813341140747 
Batch 898, generator loss: 0.9430675506591797, discriminator loss: 1.0253510475158691 
Batch 899, generator loss: 0.8899520635604858, discriminator loss: 1.0872433185577393 
Batch 900, generator loss: 0.9219590425491333, discriminator loss: 1.101205825805664 
Batch 901, generator loss: 0.8988475799560547, discriminator loss: 1.1252188682556152 
Batch 902, generator loss: 0.8874002695083618, discriminator loss: 1.1253364086151123 
Batch 903, generator loss: 0.8927994966506958, discriminator loss: 1.131757378578186 
Batch 904, generator loss: 0.9217696189880371, discriminator loss: 1.1731314659118652 
Batch 905, generator loss: 0.8814868927001953, discriminator loss: 1.2322680950164795 
Batch 906, generator loss: 0.8537623286247253, discriminator loss: 1.200899600982666 
Batch 907, generator loss: 0.8278684616088867, discriminator loss: 1.2323493957519531 
Batch 908, generator loss: 0.8332045078277588, discriminator loss: 1.2450339794158936 
Batch 909, generator loss: 0.8327625393867493, discriminator loss: 1.267378330230713 
Batch 910, generator loss: 0.8423166275024414, discriminator loss: 1.2707114219665527 
Batch 911, generator loss: 0.8378763198852539, discriminator loss: 1.2668626308441162 
Batch 912, generator loss: 0.8294042348861694, discriminator loss: 1.2711275815963745 
Batch 913, generator loss: 0.8398424386978149, discriminator loss: 1.310577392578125 
Batch 914, generator loss: 0.810104489326477, discriminator loss: 1.2926548719406128 
Batch 915, generator loss: 0.8254366517066956, discriminator loss: 1.2546926736831665 
Batch 916, generator loss: 0.8481662273406982, discriminator loss: 1.2350660562515259 
Batch 917, generator loss: 0.8540099859237671, discriminator loss: 1.2738101482391357 
Batch 918, generator loss: 0.8410775661468506, discriminator loss: 1.2747523784637451 
Batch 919, generator loss: 0.8717168569564819, discriminator loss: 1.234104871749878 
Batch 920, generator loss: 0.8668409585952759, discriminator loss: 1.2305772304534912 
Batch 921, generator loss: 0.9018236398696899, discriminator loss: 1.1952290534973145 
Batch 922, generator loss: 0.8730623722076416, discriminator loss: 1.2027430534362793 
Batch 923, generator loss: 0.8592014908790588, discriminator loss: 1.1630876064300537 
Batch 924, generator loss: 0.8935353755950928, discriminator loss: 1.1563615798950195 
Batch 925, generator loss: 0.9184374809265137, discriminator loss: 1.1066293716430664 
Batch 926, generator loss: 0.9413019418716431, discriminator loss: 1.1194047927856445 
Batch 927, generator loss: 0.913192868232727, discriminator loss: 1.125120759010315 
Batch 928, generator loss: 0.9559124708175659, discriminator loss: 1.0874230861663818 
Batch 929, generator loss: 0.9608421325683594, discriminator loss: 1.0914795398712158 
Batch 930, generator loss: 0.9575867652893066, discriminator loss: 1.0533205270767212 
Batch 931, generator loss: 0.9255078434944153, discriminator loss: 1.0960434675216675 
Batch 932, generator loss: 0.9431753158569336, discriminator loss: 1.0897679328918457 
Batch 933, generator loss: 0.9546456933021545, discriminator loss: 1.072460651397705 
Batch 934, generator loss: 0.9804458022117615, discriminator loss: 1.0666075944900513 
Batch 935, generator loss: 0.9567198157310486, discriminator loss: 1.0813417434692383 
Batch 936, generator loss: 0.949796736240387, discriminator loss: 1.0687978267669678 
Batch 937, generator loss: 0.9371316432952881, discriminator loss: 1.0701004266738892 
Batch 938, generator loss: 0.9239820241928101, discriminator loss: 1.0681077241897583 
Batch 939, generator loss: 0.9497413635253906, discriminator loss: 1.0940898656845093 
Batch 940, generator loss: 0.9788285493850708, discriminator loss: 1.084107518196106 
Batch 941, generator loss: 0.9526498317718506, discriminator loss: 1.1039202213287354 
Batch 942, generator loss: 0.9208225607872009, discriminator loss: 1.1090288162231445 
Batch 943, generator loss: 0.9362383484840393, discriminator loss: 1.1035020351409912 
Batch 944, generator loss: 0.9346904754638672, discriminator loss: 1.0876801013946533 
Batch 945, generator loss: 0.9188501834869385, discriminator loss: 1.1352219581604004 
Batch 946, generator loss: 0.9130128622055054, discriminator loss: 1.0978755950927734 
Batch 947, generator loss: 0.9221878051757812, discriminator loss: 1.1462790966033936 
Batch 948, generator loss: 0.8927861452102661, discriminator loss: 1.1303420066833496 
Batch 949, generator loss: 0.9123895168304443, discriminator loss: 1.140836238861084 
Batch 950, generator loss: 0.9041935205459595, discriminator loss: 1.2227983474731445 
Batch 951, generator loss: 0.9050009250640869, discriminator loss: 1.1234397888183594 
Batch 952, generator loss: 0.8571064472198486, discriminator loss: 1.1937520503997803 
Batch 953, generator loss: 0.8891019821166992, discriminator loss: 1.183466911315918 
Batch 954, generator loss: 0.8831576108932495, discriminator loss: 1.168928623199463 
Batch 955, generator loss: 0.8621664643287659, discriminator loss: 1.2073655128479004 
Batch 956, generator loss: 0.8529890179634094, discriminator loss: 1.171144723892212 
Batch 957, generator loss: 0.8587861061096191, discriminator loss: 1.2182902097702026 
Batch 958, generator loss: 0.8736118078231812, discriminator loss: 1.2271956205368042 
Batch 959, generator loss: 0.8846585750579834, discriminator loss: 1.2030308246612549 
Batch 960, generator loss: 0.8349048495292664, discriminator loss: 1.2711255550384521 
Batch 961, generator loss: 0.7953498363494873, discriminator loss: 1.2728536128997803 
Batch 962, generator loss: 0.7941218614578247, discriminator loss: 1.2340755462646484 
Batch 963, generator loss: 0.8033865094184875, discriminator loss: 1.2271578311920166 
Batch 964, generator loss: 0.8551961183547974, discriminator loss: 1.287956953048706 
Batch 965, generator loss: 0.8310834765434265, discriminator loss: 1.2471253871917725 
Batch 966, generator loss: 0.8459628224372864, discriminator loss: 1.2574641704559326 
Batch 967, generator loss: 0.8170024156570435, discriminator loss: 1.2518147230148315 
Batch 968, generator loss: 0.780024528503418, discriminator loss: 1.2898457050323486 
Batch 969, generator loss: 0.7788506746292114, discriminator loss: 1.214198112487793 
Batch 970, generator loss: 0.7737112641334534, discriminator loss: 1.213287591934204 
Batch 971, generator loss: 0.8145313262939453, discriminator loss: 1.2425657510757446 
Batch 972, generator loss: 0.8706523776054382, discriminator loss: 1.2309080362319946 
Batch 973, generator loss: 0.8795582056045532, discriminator loss: 1.2239903211593628 
Batch 974, generator loss: 0.8447866439819336, discriminator loss: 1.235837459564209 
Batch 975, generator loss: 0.7978531122207642, discriminator loss: 1.2542524337768555 
Batch 976, generator loss: 0.7503839731216431, discriminator loss: 1.2546000480651855 
Batch 977, generator loss: 0.7194796800613403, discriminator loss: 1.2307103872299194 
Batch 978, generator loss: 0.8027119040489197, discriminator loss: 1.1791326999664307 
Batch 979, generator loss: 0.9120205640792847, discriminator loss: 1.1445589065551758 
Batch 980, generator loss: 0.9040002226829529, discriminator loss: 1.2005269527435303 
Batch 981, generator loss: 0.9406304359436035, discriminator loss: 1.147193193435669 
Batch 982, generator loss: 0.8800018429756165, discriminator loss: 1.1478625535964966 
Batch 983, generator loss: 0.8279412984848022, discriminator loss: 1.1719188690185547 
Batch 984, generator loss: 0.8104326725006104, discriminator loss: 1.188974380493164 
Batch 985, generator loss: 0.8176388740539551, discriminator loss: 1.1487237215042114 
Batch 986, generator loss: 0.8641682863235474, discriminator loss: 1.1498689651489258 
Batch 987, generator loss: 0.9216437339782715, discriminator loss: 1.142026424407959 
Batch 988, generator loss: 0.9471147656440735, discriminator loss: 1.0986831188201904 
Batch 989, generator loss: 0.9476460218429565, discriminator loss: 1.117456316947937 
Batch 990, generator loss: 0.9166586399078369, discriminator loss: 1.1274127960205078 
Batch 991, generator loss: 0.8835374116897583, discriminator loss: 1.1307135820388794 
Batch 992, generator loss: 0.8616683483123779, discriminator loss: 1.1930912733078003 
Batch 993, generator loss: 0.8343282341957092, discriminator loss: 1.1772987842559814 
Batch 994, generator loss: 0.8766427040100098, discriminator loss: 1.2246601581573486 
Batch 995, generator loss: 0.9325228929519653, discriminator loss: 1.1954604387283325 
Batch 996, generator loss: 0.9025763869285583, discriminator loss: 1.246605634689331 
Batch 997, generator loss: 0.8853694200515747, discriminator loss: 1.2681803703308105 
Batch 998, generator loss: 0.8185688257217407, discriminator loss: 1.2370095252990723 
Batch 999, generator loss: 0.7803754210472107, discriminator loss: 1.320943832397461 
Batch 1000, generator loss: 0.8092482089996338, discriminator loss: 1.2956984043121338 
Batch 1001, generator loss: 0.8433205485343933, discriminator loss: 1.3882255554199219 
Batch 1002, generator loss: 0.8585963249206543, discriminator loss: 1.3096463680267334 
Batch 1003, generator loss: 0.8141495585441589, discriminator loss: 1.3819940090179443 
Batch 1004, generator loss: 0.7534857392311096, discriminator loss: 1.4610259532928467 
Generator loss: tf.Tensor(0.75348574, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.461026, shape=(), dtype=float32)
EPOCH: 4
Batch 1026, generator loss: 0.7601828575134277, discriminator loss: 1.3745043277740479 
Batch 1027, generator loss: 0.7812985181808472, discriminator loss: 1.4311399459838867 
Batch 1028, generator loss: 0.793524980545044, discriminator loss: 1.4485039710998535 
Batch 1029, generator loss: 0.821385383605957, discriminator loss: 1.4672555923461914 
Batch 1030, generator loss: 0.7892159819602966, discriminator loss: 1.4622316360473633 
Batch 1031, generator loss: 0.7447359561920166, discriminator loss: 1.502243161201477 
Batch 1032, generator loss: 0.6911355257034302, discriminator loss: 1.5535335540771484 
Batch 1033, generator loss: 0.746945858001709, discriminator loss: 1.4675171375274658 
Batch 1034, generator loss: 0.7452442646026611, discriminator loss: 1.465715765953064 
Batch 1035, generator loss: 0.7845855355262756, discriminator loss: 1.4390537738800049 
Batch 1036, generator loss: 0.7972795367240906, discriminator loss: 1.4758763313293457 
Batch 1037, generator loss: 0.8008323907852173, discriminator loss: 1.5271072387695312 
Batch 1038, generator loss: 0.7489741444587708, discriminator loss: 1.4375267028808594 
Batch 1039, generator loss: 0.718647301197052, discriminator loss: 1.4738264083862305 
Batch 1040, generator loss: 0.7383981347084045, discriminator loss: 1.460530400276184 
Batch 1041, generator loss: 0.8126331567764282, discriminator loss: 1.4791655540466309 
Batch 1042, generator loss: 0.8163530826568604, discriminator loss: 1.3818912506103516 
Batch 1043, generator loss: 0.8182870149612427, discriminator loss: 1.4211106300354004 
Batch 1044, generator loss: 0.770042359828949, discriminator loss: 1.3834936618804932 
Batch 1045, generator loss: 0.7939399480819702, discriminator loss: 1.4424809217453003 
Batch 1046, generator loss: 0.77210932970047, discriminator loss: 1.4639394283294678 
Batch 1047, generator loss: 0.7989804744720459, discriminator loss: 1.4492930173873901 
Batch 1048, generator loss: 0.7961857318878174, discriminator loss: 1.3815598487854004 
Batch 1049, generator loss: 0.8071781396865845, discriminator loss: 1.4368267059326172 
Batch 1050, generator loss: 0.782802164554596, discriminator loss: 1.4248508214950562 
Batch 1051, generator loss: 0.7401614189147949, discriminator loss: 1.4518537521362305 
Batch 1052, generator loss: 0.7572941780090332, discriminator loss: 1.4165091514587402 
Batch 1053, generator loss: 0.7796049118041992, discriminator loss: 1.4600205421447754 
Batch 1054, generator loss: 0.822419285774231, discriminator loss: 1.4016940593719482 
Batch 1055, generator loss: 0.8293622136116028, discriminator loss: 1.4208853244781494 
Batch 1056, generator loss: 0.7751245498657227, discriminator loss: 1.4048542976379395 
Batch 1057, generator loss: 0.7341029644012451, discriminator loss: 1.4092776775360107 
Batch 1058, generator loss: 0.7779591679573059, discriminator loss: 1.4061720371246338 
Batch 1059, generator loss: 0.7718409299850464, discriminator loss: 1.426795244216919 
Batch 1060, generator loss: 0.7948462963104248, discriminator loss: 1.405086874961853 
Batch 1061, generator loss: 0.7707206010818481, discriminator loss: 1.4000152349472046 
Batch 1062, generator loss: 0.7423893213272095, discriminator loss: 1.4341027736663818 
Batch 1063, generator loss: 0.7613687515258789, discriminator loss: 1.4116225242614746 
Batch 1064, generator loss: 0.7994238138198853, discriminator loss: 1.3759000301361084 
Batch 1065, generator loss: 0.7942596673965454, discriminator loss: 1.3959498405456543 
Batch 1066, generator loss: 0.7598550319671631, discriminator loss: 1.4230116605758667 
Batch 1067, generator loss: 0.7488633990287781, discriminator loss: 1.4338291883468628 
Batch 1068, generator loss: 0.7558032870292664, discriminator loss: 1.4333117008209229 
Batch 1069, generator loss: 0.7355196475982666, discriminator loss: 1.4131120443344116 
Batch 1070, generator loss: 0.7212069034576416, discriminator loss: 1.4318795204162598 
Batch 1071, generator loss: 0.7817338109016418, discriminator loss: 1.430683970451355 
Batch 1072, generator loss: 0.806976318359375, discriminator loss: 1.433603286743164 
Batch 1073, generator loss: 0.772494375705719, discriminator loss: 1.4197447299957275 
Batch 1074, generator loss: 0.7167236804962158, discriminator loss: 1.4097411632537842 
Batch 1075, generator loss: 0.6940089464187622, discriminator loss: 1.434044599533081 
Batch 1076, generator loss: 0.7156195640563965, discriminator loss: 1.528014898300171 
Batch 1077, generator loss: 0.7700443267822266, discriminator loss: 1.4137698411941528 
Batch 1078, generator loss: 0.7737738490104675, discriminator loss: 1.4441578388214111 
Batch 1079, generator loss: 0.7542057037353516, discriminator loss: 1.4425690174102783 
Batch 1080, generator loss: 0.7657867670059204, discriminator loss: 1.4222843647003174 
Batch 1081, generator loss: 0.7397986650466919, discriminator loss: 1.4015860557556152 
Batch 1082, generator loss: 0.71371990442276, discriminator loss: 1.4445323944091797 
Batch 1083, generator loss: 0.7328983545303345, discriminator loss: 1.427238941192627 
Batch 1084, generator loss: 0.755570650100708, discriminator loss: 1.401734471321106 
Batch 1085, generator loss: 0.754797101020813, discriminator loss: 1.4105879068374634 
Batch 1086, generator loss: 0.7891795039176941, discriminator loss: 1.3267323970794678 
Batch 1087, generator loss: 0.8059850335121155, discriminator loss: 1.3326566219329834 
Batch 1088, generator loss: 0.8304845094680786, discriminator loss: 1.2646842002868652 
Batch 1089, generator loss: 0.8388745188713074, discriminator loss: 1.2596192359924316 
Batch 1090, generator loss: 0.7905787229537964, discriminator loss: 1.2581167221069336 
Batch 1091, generator loss: 0.8559050559997559, discriminator loss: 1.2231769561767578 
Batch 1092, generator loss: 0.9024227857589722, discriminator loss: 1.1566250324249268 
Batch 1093, generator loss: 0.8862769603729248, discriminator loss: 1.1555016040802002 
Batch 1094, generator loss: 0.887019157409668, discriminator loss: 1.1418919563293457 
Batch 1095, generator loss: 0.9008300304412842, discriminator loss: 1.0797932147979736 
Batch 1096, generator loss: 0.941440224647522, discriminator loss: 1.0530242919921875 
Batch 1097, generator loss: 0.9662892818450928, discriminator loss: 1.0735654830932617 
Batch 1098, generator loss: 0.9620108604431152, discriminator loss: 1.0159472227096558 
Batch 1099, generator loss: 0.9755865335464478, discriminator loss: 0.9857227802276611 
Batch 1100, generator loss: 1.010780692100525, discriminator loss: 0.9677457809448242 
Batch 1101, generator loss: 1.0083513259887695, discriminator loss: 0.9468765258789062 
Batch 1102, generator loss: 1.0121361017227173, discriminator loss: 0.9598617553710938 
Batch 1103, generator loss: 1.0845000743865967, discriminator loss: 0.9195843935012817 
Batch 1104, generator loss: 1.08480703830719, discriminator loss: 0.8946536183357239 
Batch 1105, generator loss: 1.0201603174209595, discriminator loss: 0.9171445965766907 
Batch 1106, generator loss: 1.0283055305480957, discriminator loss: 0.9087687730789185 
Batch 1107, generator loss: 1.022991418838501, discriminator loss: 0.9358152151107788 
Batch 1108, generator loss: 1.0821397304534912, discriminator loss: 0.878247082233429 
Batch 1109, generator loss: 1.1147657632827759, discriminator loss: 0.8938280344009399 
Batch 1110, generator loss: 1.0777608156204224, discriminator loss: 0.910153865814209 
Batch 1111, generator loss: 1.0939335823059082, discriminator loss: 0.9251037836074829 
Batch 1112, generator loss: 1.0449504852294922, discriminator loss: 0.920749306678772 
Batch 1113, generator loss: 1.0354809761047363, discriminator loss: 0.912552535533905 
Batch 1114, generator loss: 1.0443885326385498, discriminator loss: 0.9089802503585815 
Batch 1115, generator loss: 1.086774468421936, discriminator loss: 0.9188960790634155 
Batch 1116, generator loss: 1.0724824666976929, discriminator loss: 0.9454317092895508 
Batch 1117, generator loss: 1.0865858793258667, discriminator loss: 0.9185594320297241 
Batch 1118, generator loss: 1.0595779418945312, discriminator loss: 0.9891135096549988 
Batch 1119, generator loss: 1.0725030899047852, discriminator loss: 1.0010716915130615 
Batch 1120, generator loss: 1.0158919095993042, discriminator loss: 1.0186586380004883 
Batch 1121, generator loss: 0.9474586248397827, discriminator loss: 1.0459208488464355 
Batch 1122, generator loss: 0.9582678079605103, discriminator loss: 1.0719218254089355 
Batch 1123, generator loss: 0.9264227151870728, discriminator loss: 1.150336742401123 
Batch 1124, generator loss: 0.9747523069381714, discriminator loss: 1.1574230194091797 
Batch 1125, generator loss: 0.9285935163497925, discriminator loss: 1.1456995010375977 
Batch 1126, generator loss: 0.8735886216163635, discriminator loss: 1.208730697631836 
Batch 1127, generator loss: 0.8705786466598511, discriminator loss: 1.244437575340271 
Batch 1128, generator loss: 0.8461909890174866, discriminator loss: 1.3177297115325928 
Batch 1129, generator loss: 0.8074573278427124, discriminator loss: 1.3166871070861816 
Batch 1130, generator loss: 0.7839394807815552, discriminator loss: 1.3525056838989258 
Batch 1131, generator loss: 0.8004469275474548, discriminator loss: 1.3470187187194824 
Batch 1132, generator loss: 0.7800034880638123, discriminator loss: 1.4529752731323242 
Batch 1133, generator loss: 0.7660555839538574, discriminator loss: 1.4278947114944458 
Batch 1134, generator loss: 0.756840705871582, discriminator loss: 1.4476385116577148 
Batch 1135, generator loss: 0.732853353023529, discriminator loss: 1.452319622039795 
Batch 1136, generator loss: 0.7145702838897705, discriminator loss: 1.5073169469833374 
Batch 1137, generator loss: 0.7128134369850159, discriminator loss: 1.5182251930236816 
Batch 1138, generator loss: 0.6850672960281372, discriminator loss: 1.5873517990112305 
Batch 1139, generator loss: 0.6977140307426453, discriminator loss: 1.5409457683563232 
Batch 1140, generator loss: 0.7326779365539551, discriminator loss: 1.536887526512146 
Batch 1141, generator loss: 0.6758462190628052, discriminator loss: 1.5919278860092163 
Batch 1142, generator loss: 0.6307053565979004, discriminator loss: 1.57291841506958 
Batch 1143, generator loss: 0.6628686785697937, discriminator loss: 1.5585099458694458 
Batch 1144, generator loss: 0.6856825947761536, discriminator loss: 1.6117693185806274 
Batch 1145, generator loss: 0.689030647277832, discriminator loss: 1.6318602561950684 
Batch 1146, generator loss: 0.6663209795951843, discriminator loss: 1.5853309631347656 
Batch 1147, generator loss: 0.624173641204834, discriminator loss: 1.6333163976669312 
Batch 1148, generator loss: 0.6115200519561768, discriminator loss: 1.5884943008422852 
Batch 1149, generator loss: 0.6375715136528015, discriminator loss: 1.5415245294570923 
Batch 1150, generator loss: 0.6415207386016846, discriminator loss: 1.5713083744049072 
Batch 1151, generator loss: 0.7080417275428772, discriminator loss: 1.5424036979675293 
Batch 1152, generator loss: 0.6793957352638245, discriminator loss: 1.5956236124038696 
Batch 1153, generator loss: 0.6587222814559937, discriminator loss: 1.579940676689148 
Batch 1154, generator loss: 0.6240159273147583, discriminator loss: 1.590017557144165 
Batch 1155, generator loss: 0.6231516599655151, discriminator loss: 1.5158677101135254 
Batch 1156, generator loss: 0.6306077241897583, discriminator loss: 1.5343236923217773 
Batch 1157, generator loss: 0.6399756669998169, discriminator loss: 1.5291582345962524 
Batch 1158, generator loss: 0.6647466421127319, discriminator loss: 1.544469952583313 
Batch 1159, generator loss: 0.6848604083061218, discriminator loss: 1.5045576095581055 
Batch 1160, generator loss: 0.6770340204238892, discriminator loss: 1.5125324726104736 
Batch 1161, generator loss: 0.6375651955604553, discriminator loss: 1.5338728427886963 
Batch 1162, generator loss: 0.6055347919464111, discriminator loss: 1.5020780563354492 
Batch 1163, generator loss: 0.6617168188095093, discriminator loss: 1.4412591457366943 
Batch 1164, generator loss: 0.6574945449829102, discriminator loss: 1.4874712228775024 
Batch 1165, generator loss: 0.6922231912612915, discriminator loss: 1.4339988231658936 
Batch 1166, generator loss: 0.6994351744651794, discriminator loss: 1.4748122692108154 
Batch 1167, generator loss: 0.6927476525306702, discriminator loss: 1.4477226734161377 
Batch 1168, generator loss: 0.640000581741333, discriminator loss: 1.4505643844604492 
Batch 1169, generator loss: 0.6502106785774231, discriminator loss: 1.4132072925567627 
Batch 1170, generator loss: 0.6625893115997314, discriminator loss: 1.3999803066253662 
Batch 1171, generator loss: 0.6783156394958496, discriminator loss: 1.415250539779663 
Batch 1172, generator loss: 0.7446253299713135, discriminator loss: 1.4192509651184082 
Batch 1173, generator loss: 0.745021402835846, discriminator loss: 1.405940055847168 
Batch 1174, generator loss: 0.7427674531936646, discriminator loss: 1.3908121585845947 
Batch 1175, generator loss: 0.6812920570373535, discriminator loss: 1.4228018522262573 
Batch 1176, generator loss: 0.6938410401344299, discriminator loss: 1.3729543685913086 
Batch 1177, generator loss: 0.671606183052063, discriminator loss: 1.3758702278137207 
Batch 1178, generator loss: 0.7014449834823608, discriminator loss: 1.3966058492660522 
Batch 1179, generator loss: 0.7581072449684143, discriminator loss: 1.2892248630523682 
Batch 1180, generator loss: 0.7832232713699341, discriminator loss: 1.3851386308670044 
Batch 1181, generator loss: 0.7924550175666809, discriminator loss: 1.4128919839859009 
Batch 1182, generator loss: 0.7858121395111084, discriminator loss: 1.2785351276397705 
Batch 1183, generator loss: 0.7206002473831177, discriminator loss: 1.3885984420776367 
Batch 1184, generator loss: 0.7044461965560913, discriminator loss: 1.3431121110916138 
Batch 1185, generator loss: 0.7228329181671143, discriminator loss: 1.3351722955703735 
Batch 1186, generator loss: 0.7906621694564819, discriminator loss: 1.3569459915161133 
Batch 1187, generator loss: 0.820784330368042, discriminator loss: 1.3394502401351929 
Batch 1188, generator loss: 0.8213942050933838, discriminator loss: 1.3789291381835938 
Batch 1189, generator loss: 0.7974547147750854, discriminator loss: 1.293931007385254 
Batch 1190, generator loss: 0.7780369520187378, discriminator loss: 1.3011077642440796 
Batch 1191, generator loss: 0.7638491988182068, discriminator loss: 1.2962590456008911 
Batch 1192, generator loss: 0.771729588508606, discriminator loss: 1.345369815826416 
Batch 1193, generator loss: 0.8120756149291992, discriminator loss: 1.3628604412078857 
Batch 1194, generator loss: 0.8453212976455688, discriminator loss: 1.291154384613037 
Batch 1195, generator loss: 0.8830691576004028, discriminator loss: 1.3189847469329834 
Batch 1196, generator loss: 0.8177454471588135, discriminator loss: 1.2776601314544678 
Batch 1197, generator loss: 0.8030643463134766, discriminator loss: 1.3183045387268066 
Batch 1198, generator loss: 0.777056872844696, discriminator loss: 1.3241811990737915 
Batch 1199, generator loss: 0.8274626731872559, discriminator loss: 1.3050425052642822 
Batch 1200, generator loss: 0.862528920173645, discriminator loss: 1.357067346572876 
Batch 1201, generator loss: 0.8915871381759644, discriminator loss: 1.2511261701583862 
Batch 1202, generator loss: 0.8712116479873657, discriminator loss: 1.351263165473938 
Batch 1203, generator loss: 0.813774049282074, discriminator loss: 1.3226349353790283 
Batch 1204, generator loss: 0.7953068017959595, discriminator loss: 1.2467272281646729 
Batch 1205, generator loss: 0.8398354053497314, discriminator loss: 1.3065478801727295 
Batch 1206, generator loss: 0.8480548858642578, discriminator loss: 1.2467964887619019 
Batch 1207, generator loss: 0.9282252192497253, discriminator loss: 1.2763532400131226 
Batch 1208, generator loss: 0.9190322160720825, discriminator loss: 1.2583277225494385 
Batch 1209, generator loss: 0.9013799428939819, discriminator loss: 1.223427653312683 
Batch 1210, generator loss: 0.8320980668067932, discriminator loss: 1.2500089406967163 
Batch 1211, generator loss: 0.8496766686439514, discriminator loss: 1.2758488655090332 
Batch 1212, generator loss: 0.8583337068557739, discriminator loss: 1.2662442922592163 
Batch 1213, generator loss: 0.8988523483276367, discriminator loss: 1.2515536546707153 
Batch 1214, generator loss: 0.8848497271537781, discriminator loss: 1.274972915649414 
Batch 1215, generator loss: 0.8888576030731201, discriminator loss: 1.2342250347137451 
Batch 1216, generator loss: 0.8591707944869995, discriminator loss: 1.2503514289855957 
Batch 1217, generator loss: 0.847037672996521, discriminator loss: 1.2261321544647217 
Batch 1218, generator loss: 0.9122410416603088, discriminator loss: 1.2678160667419434 
Batch 1219, generator loss: 0.877826452255249, discriminator loss: 1.2915294170379639 
Batch 1220, generator loss: 0.889531672000885, discriminator loss: 1.2439409494400024 
Batch 1221, generator loss: 0.9010693430900574, discriminator loss: 1.2554681301116943 
Batch 1222, generator loss: 0.839521586894989, discriminator loss: 1.291671633720398 
Batch 1223, generator loss: 0.859163761138916, discriminator loss: 1.2823443412780762 
Batch 1224, generator loss: 0.831454873085022, discriminator loss: 1.303877592086792 
Batch 1225, generator loss: 0.8722786903381348, discriminator loss: 1.2750953435897827 
Batch 1226, generator loss: 0.9089754223823547, discriminator loss: 1.2915432453155518 
Batch 1227, generator loss: 0.8755277395248413, discriminator loss: 1.2911385297775269 
Batch 1228, generator loss: 0.847531795501709, discriminator loss: 1.3028724193572998 
Batch 1229, generator loss: 0.8550314903259277, discriminator loss: 1.2716728448867798 
Batch 1230, generator loss: 0.8456172943115234, discriminator loss: 1.3557264804840088 
Batch 1231, generator loss: 0.8596596717834473, discriminator loss: 1.3344502449035645 
Batch 1232, generator loss: 0.8288095593452454, discriminator loss: 1.3909475803375244 
Batch 1233, generator loss: 0.8271464109420776, discriminator loss: 1.3650743961334229 
Batch 1234, generator loss: 0.7870490550994873, discriminator loss: 1.4051415920257568 
Batch 1235, generator loss: 0.8039029836654663, discriminator loss: 1.3672409057617188 
Batch 1236, generator loss: 0.8115469217300415, discriminator loss: 1.4115709066390991 
Batch 1237, generator loss: 0.8059117794036865, discriminator loss: 1.4051927328109741 
Batch 1238, generator loss: 0.8164116144180298, discriminator loss: 1.4572012424468994 
Batch 1239, generator loss: 0.7974592447280884, discriminator loss: 1.5095996856689453 
Batch 1240, generator loss: 0.7592852711677551, discriminator loss: 1.5254472494125366 
Batch 1241, generator loss: 0.7813125848770142, discriminator loss: 1.487398386001587 
Batch 1242, generator loss: 0.7380921840667725, discriminator loss: 1.580845594406128 
Batch 1243, generator loss: 0.7470909357070923, discriminator loss: 1.5502628087997437 
Batch 1244, generator loss: 0.7167894840240479, discriminator loss: 1.6383628845214844 
Batch 1245, generator loss: 0.7121562361717224, discriminator loss: 1.6411023139953613 
Batch 1246, generator loss: 0.73557049036026, discriminator loss: 1.6301661729812622 
Batch 1247, generator loss: 0.7021596431732178, discriminator loss: 1.6560167074203491 
Batch 1248, generator loss: 0.7171639204025269, discriminator loss: 1.6914339065551758 
Batch 1249, generator loss: 0.7087123394012451, discriminator loss: 1.6803778409957886 
Batch 1250, generator loss: 0.6655318140983582, discriminator loss: 1.685422658920288 
Batch 1251, generator loss: 0.6664268970489502, discriminator loss: 1.754130244255066 
Batch 1252, generator loss: 0.663141131401062, discriminator loss: 1.7064639329910278 
Batch 1253, generator loss: 0.6899282932281494, discriminator loss: 1.783745527267456 
Batch 1254, generator loss: 0.7065590620040894, discriminator loss: 1.8382458686828613 
Batch 1255, generator loss: 0.6408883333206177, discriminator loss: 1.7799251079559326 
Batch 1256, generator loss: 0.6114529371261597, discriminator loss: 1.7707630395889282 
Batch 1257, generator loss: 0.6153777837753296, discriminator loss: 1.8546092510223389 
Batch 1258, generator loss: 0.5629996657371521, discriminator loss: 1.844599723815918 
Batch 1259, generator loss: 0.5943920612335205, discriminator loss: 1.8406405448913574 
Batch 1260, generator loss: 0.5952991843223572, discriminator loss: 1.8430366516113281 
Generator loss: tf.Tensor(0.5952992, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.8430367, shape=(), dtype=float32)
EPOCH: 5
Batch 1282, generator loss: 0.6545125246047974, discriminator loss: 1.8226079940795898 
Batch 1283, generator loss: 0.6230423450469971, discriminator loss: 1.769742488861084 
Batch 1284, generator loss: 0.62833571434021, discriminator loss: 1.793996810913086 
Batch 1285, generator loss: 0.635927140712738, discriminator loss: 1.7585325241088867 
Batch 1286, generator loss: 0.6160494685173035, discriminator loss: 1.6784679889678955 
Batch 1287, generator loss: 0.6509793996810913, discriminator loss: 1.6971855163574219 
Batch 1288, generator loss: 0.6397455930709839, discriminator loss: 1.7075493335723877 
Batch 1289, generator loss: 0.6327153444290161, discriminator loss: 1.7272655963897705 
Batch 1290, generator loss: 0.6599900722503662, discriminator loss: 1.7015655040740967 
Batch 1291, generator loss: 0.6306953430175781, discriminator loss: 1.7401539087295532 
Batch 1292, generator loss: 0.6338170766830444, discriminator loss: 1.6511043310165405 
Batch 1293, generator loss: 0.6003649830818176, discriminator loss: 1.628659963607788 
Batch 1294, generator loss: 0.6120480298995972, discriminator loss: 1.6136925220489502 
Batch 1295, generator loss: 0.6609619855880737, discriminator loss: 1.5863637924194336 
Batch 1296, generator loss: 0.684445858001709, discriminator loss: 1.5601205825805664 
Batch 1297, generator loss: 0.7066707611083984, discriminator loss: 1.5647729635238647 
Batch 1298, generator loss: 0.7087268829345703, discriminator loss: 1.5379674434661865 
Batch 1299, generator loss: 0.7042162418365479, discriminator loss: 1.5236561298370361 
Batch 1300, generator loss: 0.692668080329895, discriminator loss: 1.4768872261047363 
Batch 1301, generator loss: 0.6940605640411377, discriminator loss: 1.447197675704956 
Batch 1302, generator loss: 0.69674152135849, discriminator loss: 1.4429779052734375 
Batch 1303, generator loss: 0.6949482560157776, discriminator loss: 1.397650957107544 
Batch 1304, generator loss: 0.7059869766235352, discriminator loss: 1.422989010810852 
Batch 1305, generator loss: 0.7327392101287842, discriminator loss: 1.406132698059082 
Batch 1306, generator loss: 0.7766519784927368, discriminator loss: 1.3644170761108398 
Batch 1307, generator loss: 0.7973436117172241, discriminator loss: 1.3271915912628174 
Batch 1308, generator loss: 0.7771977186203003, discriminator loss: 1.3322347402572632 
Batch 1309, generator loss: 0.8038842082023621, discriminator loss: 1.3383128643035889 
Batch 1310, generator loss: 0.7448239326477051, discriminator loss: 1.3093805313110352 
Batch 1311, generator loss: 0.7554850578308105, discriminator loss: 1.2470269203186035 
Batch 1312, generator loss: 0.7492504119873047, discriminator loss: 1.2572002410888672 
Batch 1313, generator loss: 0.813500165939331, discriminator loss: 1.2474606037139893 
Batch 1314, generator loss: 0.8109625577926636, discriminator loss: 1.2412407398223877 
Batch 1315, generator loss: 0.8353452086448669, discriminator loss: 1.2202621698379517 
Batch 1316, generator loss: 0.8235719799995422, discriminator loss: 1.2301087379455566 
Batch 1317, generator loss: 0.8385292887687683, discriminator loss: 1.2356430292129517 
Batch 1318, generator loss: 0.8105875849723816, discriminator loss: 1.2307225465774536 
Batch 1319, generator loss: 0.8217172622680664, discriminator loss: 1.2198679447174072 
Batch 1320, generator loss: 0.8140455484390259, discriminator loss: 1.2184116840362549 
Batch 1321, generator loss: 0.8156869411468506, discriminator loss: 1.2113678455352783 
Batch 1322, generator loss: 0.8153436183929443, discriminator loss: 1.2091448307037354 
Batch 1323, generator loss: 0.84112548828125, discriminator loss: 1.206712245941162 
Batch 1324, generator loss: 0.8314464092254639, discriminator loss: 1.2189704179763794 
Batch 1325, generator loss: 0.8472903370857239, discriminator loss: 1.196897029876709 
Batch 1326, generator loss: 0.8255407214164734, discriminator loss: 1.2065519094467163 
Batch 1327, generator loss: 0.808139443397522, discriminator loss: 1.2338757514953613 
Batch 1328, generator loss: 0.8247867822647095, discriminator loss: 1.1921461820602417 
Batch 1329, generator loss: 0.8266744017601013, discriminator loss: 1.2204673290252686 
Batch 1330, generator loss: 0.8217624425888062, discriminator loss: 1.2215757369995117 
Batch 1331, generator loss: 0.8370285034179688, discriminator loss: 1.2044917345046997 
Batch 1332, generator loss: 0.845789909362793, discriminator loss: 1.2001454830169678 
Batch 1333, generator loss: 0.847281813621521, discriminator loss: 1.2237269878387451 
Batch 1334, generator loss: 0.8609434366226196, discriminator loss: 1.199846625328064 
Batch 1335, generator loss: 0.8122998476028442, discriminator loss: 1.2254230976104736 
Batch 1336, generator loss: 0.7841086983680725, discriminator loss: 1.2152626514434814 
Batch 1337, generator loss: 0.7974046468734741, discriminator loss: 1.235400676727295 
Batch 1338, generator loss: 0.8398853540420532, discriminator loss: 1.2187598943710327 
Batch 1339, generator loss: 0.8431211709976196, discriminator loss: 1.2210981845855713 
Batch 1340, generator loss: 0.8600122928619385, discriminator loss: 1.228153944015503 
Batch 1341, generator loss: 0.8276098370552063, discriminator loss: 1.2490265369415283 
Batch 1342, generator loss: 0.8167026042938232, discriminator loss: 1.2708948850631714 
Batch 1343, generator loss: 0.7881983518600464, discriminator loss: 1.235533356666565 
Batch 1344, generator loss: 0.792104184627533, discriminator loss: 1.293853759765625 
Batch 1345, generator loss: 0.773363471031189, discriminator loss: 1.2783633470535278 
Batch 1346, generator loss: 0.7869545221328735, discriminator loss: 1.285187005996704 
Batch 1347, generator loss: 0.7869226932525635, discriminator loss: 1.3067505359649658 
Batch 1348, generator loss: 0.7904333472251892, discriminator loss: 1.298483967781067 
Batch 1349, generator loss: 0.7969577312469482, discriminator loss: 1.3321022987365723 
Batch 1350, generator loss: 0.7914518117904663, discriminator loss: 1.3147622346878052 
Batch 1351, generator loss: 0.7830721139907837, discriminator loss: 1.3104805946350098 
Batch 1352, generator loss: 0.765874981880188, discriminator loss: 1.2855942249298096 
Batch 1353, generator loss: 0.7524882555007935, discriminator loss: 1.3657739162445068 
Batch 1354, generator loss: 0.7346954345703125, discriminator loss: 1.3503837585449219 
Batch 1355, generator loss: 0.7580979466438293, discriminator loss: 1.3658524751663208 
Batch 1356, generator loss: 0.7778335213661194, discriminator loss: 1.3661549091339111 
Batch 1357, generator loss: 0.7690812349319458, discriminator loss: 1.3689627647399902 
Batch 1358, generator loss: 0.7498494982719421, discriminator loss: 1.3766725063323975 
Batch 1359, generator loss: 0.75693678855896, discriminator loss: 1.3629908561706543 
Batch 1360, generator loss: 0.7196695804595947, discriminator loss: 1.4393866062164307 
Batch 1361, generator loss: 0.7355133891105652, discriminator loss: 1.416719675064087 
Batch 1362, generator loss: 0.716185450553894, discriminator loss: 1.4325764179229736 
Batch 1363, generator loss: 0.715973973274231, discriminator loss: 1.3826487064361572 
Batch 1364, generator loss: 0.7352374792098999, discriminator loss: 1.4161951541900635 
Batch 1365, generator loss: 0.7517434358596802, discriminator loss: 1.3917127847671509 
Batch 1366, generator loss: 0.7277543544769287, discriminator loss: 1.4757534265518188 
Batch 1367, generator loss: 0.7231825590133667, discriminator loss: 1.4488614797592163 
Batch 1368, generator loss: 0.6986226439476013, discriminator loss: 1.4476397037506104 
Batch 1369, generator loss: 0.6896716356277466, discriminator loss: 1.4177285432815552 
Batch 1370, generator loss: 0.7394084930419922, discriminator loss: 1.3913838863372803 
Batch 1371, generator loss: 0.7338749170303345, discriminator loss: 1.4044981002807617 
Batch 1372, generator loss: 0.7433078289031982, discriminator loss: 1.422239065170288 
Batch 1373, generator loss: 0.7541981935501099, discriminator loss: 1.4552198648452759 
Batch 1374, generator loss: 0.7214539051055908, discriminator loss: 1.4318907260894775 
Batch 1375, generator loss: 0.6969044208526611, discriminator loss: 1.4245147705078125 
Batch 1376, generator loss: 0.6773382425308228, discriminator loss: 1.4779748916625977 
Batch 1377, generator loss: 0.690706729888916, discriminator loss: 1.4451496601104736 
Batch 1378, generator loss: 0.7409808039665222, discriminator loss: 1.4604276418685913 
Batch 1379, generator loss: 0.7420349717140198, discriminator loss: 1.432041049003601 
Batch 1380, generator loss: 0.7360721826553345, discriminator loss: 1.4363174438476562 
Batch 1381, generator loss: 0.6908935308456421, discriminator loss: 1.4619755744934082 
Batch 1382, generator loss: 0.7161556482315063, discriminator loss: 1.4403815269470215 
Batch 1383, generator loss: 0.7148038744926453, discriminator loss: 1.4487648010253906 
Batch 1384, generator loss: 0.7210142612457275, discriminator loss: 1.4243131875991821 
Batch 1385, generator loss: 0.7417441010475159, discriminator loss: 1.414302110671997 
Batch 1386, generator loss: 0.7194147109985352, discriminator loss: 1.420620322227478 
Batch 1387, generator loss: 0.714937150478363, discriminator loss: 1.4435667991638184 
Batch 1388, generator loss: 0.7319228649139404, discriminator loss: 1.436070442199707 
Batch 1389, generator loss: 0.7236247062683105, discriminator loss: 1.4471523761749268 
Batch 1390, generator loss: 0.7379757165908813, discriminator loss: 1.396216630935669 
Batch 1391, generator loss: 0.731448769569397, discriminator loss: 1.3854405879974365 
Batch 1392, generator loss: 0.736738920211792, discriminator loss: 1.3865227699279785 
Batch 1393, generator loss: 0.7795177102088928, discriminator loss: 1.4034106731414795 
Batch 1394, generator loss: 0.7605655789375305, discriminator loss: 1.3632276058197021 
Batch 1395, generator loss: 0.7759456038475037, discriminator loss: 1.3770064115524292 
Batch 1396, generator loss: 0.7514181137084961, discriminator loss: 1.3648918867111206 
Batch 1397, generator loss: 0.7492913007736206, discriminator loss: 1.337599515914917 
Batch 1398, generator loss: 0.7794878482818604, discriminator loss: 1.3334810733795166 
Batch 1399, generator loss: 0.7580310106277466, discriminator loss: 1.3353614807128906 
Batch 1400, generator loss: 0.7782362699508667, discriminator loss: 1.3048880100250244 
Batch 1401, generator loss: 0.7543232440948486, discriminator loss: 1.3164044618606567 
Batch 1402, generator loss: 0.7876330614089966, discriminator loss: 1.3250048160552979 
Batch 1403, generator loss: 0.8189659118652344, discriminator loss: 1.2798566818237305 
Batch 1404, generator loss: 0.8101963400840759, discriminator loss: 1.2994287014007568 
Batch 1405, generator loss: 0.7871717214584351, discriminator loss: 1.283188819885254 
Batch 1406, generator loss: 0.7847239375114441, discriminator loss: 1.279076337814331 
Batch 1407, generator loss: 0.7680968046188354, discriminator loss: 1.2968767881393433 
Batch 1408, generator loss: 0.7646412253379822, discriminator loss: 1.282670259475708 
Batch 1409, generator loss: 0.8021688461303711, discriminator loss: 1.2573699951171875 
Batch 1410, generator loss: 0.8246038556098938, discriminator loss: 1.218717336654663 
Batch 1411, generator loss: 0.8315122127532959, discriminator loss: 1.2488034963607788 
Batch 1412, generator loss: 0.8349799513816833, discriminator loss: 1.276819109916687 
Batch 1413, generator loss: 0.8472728729248047, discriminator loss: 1.209761381149292 
Batch 1414, generator loss: 0.8369918465614319, discriminator loss: 1.250702142715454 
Batch 1415, generator loss: 0.7994239330291748, discriminator loss: 1.2259886264801025 
Batch 1416, generator loss: 0.8026784658432007, discriminator loss: 1.2245309352874756 
Batch 1417, generator loss: 0.7888265252113342, discriminator loss: 1.217555284500122 
Batch 1418, generator loss: 0.8322529196739197, discriminator loss: 1.1968737840652466 
Batch 1419, generator loss: 0.8429583311080933, discriminator loss: 1.2010023593902588 
Batch 1420, generator loss: 0.869601845741272, discriminator loss: 1.2069491147994995 
Batch 1421, generator loss: 0.8515415787696838, discriminator loss: 1.1895488500595093 
Batch 1422, generator loss: 0.8531480431556702, discriminator loss: 1.1572191715240479 
Batch 1423, generator loss: 0.8711585998535156, discriminator loss: 1.1664562225341797 
Batch 1424, generator loss: 0.853384256362915, discriminator loss: 1.177781105041504 
Batch 1425, generator loss: 0.8472118377685547, discriminator loss: 1.1518181562423706 
Batch 1426, generator loss: 0.8634227514266968, discriminator loss: 1.1602098941802979 
Batch 1427, generator loss: 0.8759207725524902, discriminator loss: 1.1565145254135132 
Batch 1428, generator loss: 0.8716907501220703, discriminator loss: 1.1422932147979736 
Batch 1429, generator loss: 0.8889589905738831, discriminator loss: 1.129023790359497 
Batch 1430, generator loss: 0.8892580270767212, discriminator loss: 1.133425235748291 
Batch 1431, generator loss: 0.8660978078842163, discriminator loss: 1.1516671180725098 
Batch 1432, generator loss: 0.8857216835021973, discriminator loss: 1.1432782411575317 
Batch 1433, generator loss: 0.9030299782752991, discriminator loss: 1.132094144821167 
Batch 1434, generator loss: 0.887444794178009, discriminator loss: 1.1493858098983765 
Batch 1435, generator loss: 0.8806630373001099, discriminator loss: 1.1602048873901367 
Batch 1436, generator loss: 0.8600731492042542, discriminator loss: 1.137904167175293 
Batch 1437, generator loss: 0.8765621185302734, discriminator loss: 1.122087001800537 
Batch 1438, generator loss: 0.8587908744812012, discriminator loss: 1.1600737571716309 
Batch 1439, generator loss: 0.8885765671730042, discriminator loss: 1.1562168598175049 
Batch 1440, generator loss: 0.87418133020401, discriminator loss: 1.1544400453567505 
Batch 1441, generator loss: 0.8746182918548584, discriminator loss: 1.1573448181152344 
Batch 1442, generator loss: 0.9123727083206177, discriminator loss: 1.1303037405014038 
Batch 1443, generator loss: 0.8591334819793701, discriminator loss: 1.1741361618041992 
Batch 1444, generator loss: 0.887948215007782, discriminator loss: 1.1568671464920044 
Batch 1445, generator loss: 0.8555541038513184, discriminator loss: 1.1822712421417236 
Batch 1446, generator loss: 0.891810417175293, discriminator loss: 1.142195224761963 
Batch 1447, generator loss: 0.8705846071243286, discriminator loss: 1.1843430995941162 
Batch 1448, generator loss: 0.8788624405860901, discriminator loss: 1.1231458187103271 
Batch 1449, generator loss: 0.8740308284759521, discriminator loss: 1.160383701324463 
Batch 1450, generator loss: 0.8794970512390137, discriminator loss: 1.2407788038253784 
Batch 1451, generator loss: 0.8845678567886353, discriminator loss: 1.1646554470062256 
Batch 1452, generator loss: 0.8647232055664062, discriminator loss: 1.1593519449234009 
Batch 1453, generator loss: 0.791399359703064, discriminator loss: 1.2309943437576294 
Batch 1454, generator loss: 0.8592441082000732, discriminator loss: 1.1745330095291138 
Batch 1455, generator loss: 0.8471086025238037, discriminator loss: 1.2210379838943481 
Batch 1456, generator loss: 0.874953031539917, discriminator loss: 1.19598388671875 
Batch 1457, generator loss: 0.8579936623573303, discriminator loss: 1.2208223342895508 
Batch 1458, generator loss: 0.850387454032898, discriminator loss: 1.2495228052139282 
Batch 1459, generator loss: 0.8420756459236145, discriminator loss: 1.1809576749801636 
Batch 1460, generator loss: 0.8239728212356567, discriminator loss: 1.2507717609405518 
Batch 1461, generator loss: 0.8239667415618896, discriminator loss: 1.2119827270507812 
Batch 1462, generator loss: 0.8226838111877441, discriminator loss: 1.21541428565979 
Batch 1463, generator loss: 0.8161182403564453, discriminator loss: 1.2374815940856934 
Batch 1464, generator loss: 0.8705148100852966, discriminator loss: 1.1965336799621582 
Batch 1465, generator loss: 0.862926185131073, discriminator loss: 1.2451229095458984 
Batch 1466, generator loss: 0.8722051382064819, discriminator loss: 1.2456936836242676 
Batch 1467, generator loss: 0.8570374250411987, discriminator loss: 1.2090628147125244 
Batch 1468, generator loss: 0.7974220514297485, discriminator loss: 1.26248037815094 
Batch 1469, generator loss: 0.8021270036697388, discriminator loss: 1.2481904029846191 
Batch 1470, generator loss: 0.8133455514907837, discriminator loss: 1.219611644744873 
Batch 1471, generator loss: 0.7999575138092041, discriminator loss: 1.3037101030349731 
Batch 1472, generator loss: 0.8419463634490967, discriminator loss: 1.2457821369171143 
Batch 1473, generator loss: 0.8671525716781616, discriminator loss: 1.2276978492736816 
Batch 1474, generator loss: 0.8999605178833008, discriminator loss: 1.2288986444473267 
Batch 1475, generator loss: 0.8720279932022095, discriminator loss: 1.2479627132415771 
Batch 1476, generator loss: 0.858462393283844, discriminator loss: 1.2288403511047363 
Batch 1477, generator loss: 0.8362582921981812, discriminator loss: 1.2345062494277954 
Batch 1478, generator loss: 0.8385935425758362, discriminator loss: 1.2624566555023193 
Batch 1479, generator loss: 0.8344202041625977, discriminator loss: 1.2332711219787598 
Batch 1480, generator loss: 0.8288825750350952, discriminator loss: 1.2899394035339355 
Batch 1481, generator loss: 0.8376757502555847, discriminator loss: 1.261425495147705 
Batch 1482, generator loss: 0.8738865256309509, discriminator loss: 1.263573169708252 
Batch 1483, generator loss: 0.8771332502365112, discriminator loss: 1.262392520904541 
Batch 1484, generator loss: 0.859438419342041, discriminator loss: 1.2880840301513672 
Batch 1485, generator loss: 0.8138265609741211, discriminator loss: 1.2780157327651978 
Batch 1486, generator loss: 0.790677011013031, discriminator loss: 1.3033785820007324 
Batch 1487, generator loss: 0.7569555044174194, discriminator loss: 1.35200035572052 
Batch 1488, generator loss: 0.7752259969711304, discriminator loss: 1.3588552474975586 
Batch 1489, generator loss: 0.7761194705963135, discriminator loss: 1.3769776821136475 
Batch 1490, generator loss: 0.7570290565490723, discriminator loss: 1.3912622928619385 
Batch 1491, generator loss: 0.7832455635070801, discriminator loss: 1.3752903938293457 
Batch 1492, generator loss: 0.7536519169807434, discriminator loss: 1.3936402797698975 
Batch 1493, generator loss: 0.7114055156707764, discriminator loss: 1.4237943887710571 
Batch 1494, generator loss: 0.6934424638748169, discriminator loss: 1.4330915212631226 
Batch 1495, generator loss: 0.7209645509719849, discriminator loss: 1.4937689304351807 
Batch 1496, generator loss: 0.7062520384788513, discriminator loss: 1.4945597648620605 
Batch 1497, generator loss: 0.700269877910614, discriminator loss: 1.5139861106872559 
Batch 1498, generator loss: 0.6808888912200928, discriminator loss: 1.552354335784912 
Batch 1499, generator loss: 0.639247715473175, discriminator loss: 1.5844881534576416 
Batch 1500, generator loss: 0.6484959125518799, discriminator loss: 1.6006826162338257 
Batch 1501, generator loss: 0.6235128045082092, discriminator loss: 1.6004695892333984 
Batch 1502, generator loss: 0.6561847925186157, discriminator loss: 1.6074674129486084 
Batch 1503, generator loss: 0.648605227470398, discriminator loss: 1.6756293773651123 
Batch 1504, generator loss: 0.6800727844238281, discriminator loss: 1.6166560649871826 
Batch 1505, generator loss: 0.6199061870574951, discriminator loss: 1.651705265045166 
Batch 1506, generator loss: 0.6262655854225159, discriminator loss: 1.636887550354004 
Batch 1507, generator loss: 0.6209506988525391, discriminator loss: 1.6788452863693237 
Batch 1508, generator loss: 0.6229497790336609, discriminator loss: 1.6285816431045532 
Batch 1509, generator loss: 0.6586977243423462, discriminator loss: 1.591048002243042 
Batch 1510, generator loss: 0.6638475656509399, discriminator loss: 1.684171199798584 
Batch 1511, generator loss: 0.6540947556495667, discriminator loss: 1.6148699522018433 
Batch 1512, generator loss: 0.673157811164856, discriminator loss: 1.5938725471496582 
Batch 1513, generator loss: 0.6950943470001221, discriminator loss: 1.587228775024414 
Batch 1514, generator loss: 0.6785590648651123, discriminator loss: 1.5420104265213013 
Batch 1515, generator loss: 0.6866673231124878, discriminator loss: 1.4912759065628052 
Batch 1516, generator loss: 0.6879460215568542, discriminator loss: 1.4950311183929443 
Generator loss: tf.Tensor(0.687946, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.4950311, shape=(), dtype=float32)
EPOCH: 6
Batch 1538, generator loss: 0.6960725784301758, discriminator loss: 1.4672962427139282 
Batch 1539, generator loss: 0.7387445569038391, discriminator loss: 1.4605660438537598 
Batch 1540, generator loss: 0.7550102472305298, discriminator loss: 1.4592267274856567 
Batch 1541, generator loss: 0.7510085701942444, discriminator loss: 1.4887070655822754 
Batch 1542, generator loss: 0.7690751552581787, discriminator loss: 1.4165356159210205 
Batch 1543, generator loss: 0.7486974000930786, discriminator loss: 1.4156103134155273 
Batch 1544, generator loss: 0.7469687461853027, discriminator loss: 1.3400249481201172 
Batch 1545, generator loss: 0.7567981481552124, discriminator loss: 1.3528056144714355 
Batch 1546, generator loss: 0.7983720302581787, discriminator loss: 1.325073480606079 
Batch 1547, generator loss: 0.8619289994239807, discriminator loss: 1.2660666704177856 
Batch 1548, generator loss: 0.8615407347679138, discriminator loss: 1.2898356914520264 
Batch 1549, generator loss: 0.8360372185707092, discriminator loss: 1.2321738004684448 
Batch 1550, generator loss: 0.8645914793014526, discriminator loss: 1.2300467491149902 
Batch 1551, generator loss: 0.8451563119888306, discriminator loss: 1.2375953197479248 
Batch 1552, generator loss: 0.8557425737380981, discriminator loss: 1.191603660583496 
Batch 1553, generator loss: 0.8566374778747559, discriminator loss: 1.1840004920959473 
Batch 1554, generator loss: 0.8620085716247559, discriminator loss: 1.2104145288467407 
Batch 1555, generator loss: 0.8836516737937927, discriminator loss: 1.2009036540985107 
Batch 1556, generator loss: 0.9005141258239746, discriminator loss: 1.1859495639801025 
Batch 1557, generator loss: 0.8911740779876709, discriminator loss: 1.179823875427246 
Batch 1558, generator loss: 0.911164402961731, discriminator loss: 1.2107203006744385 
Batch 1559, generator loss: 0.9132042527198792, discriminator loss: 1.1457929611206055 
Batch 1560, generator loss: 0.906179666519165, discriminator loss: 1.2019363641738892 
Batch 1561, generator loss: 0.8734322786331177, discriminator loss: 1.1893846988677979 
Batch 1562, generator loss: 0.8773401975631714, discriminator loss: 1.182344913482666 
Batch 1563, generator loss: 0.8722332715988159, discriminator loss: 1.1628570556640625 
Batch 1564, generator loss: 0.8862435817718506, discriminator loss: 1.1564409732818604 
Batch 1565, generator loss: 0.8843410015106201, discriminator loss: 1.1757099628448486 
Batch 1566, generator loss: 0.9136835336685181, discriminator loss: 1.2143397331237793 
Batch 1567, generator loss: 0.9257022142410278, discriminator loss: 1.186165690422058 
Batch 1568, generator loss: 0.908400297164917, discriminator loss: 1.246742606163025 
Batch 1569, generator loss: 0.8878337740898132, discriminator loss: 1.212228775024414 
Batch 1570, generator loss: 0.8478348255157471, discriminator loss: 1.2274774312973022 
Batch 1571, generator loss: 0.8279587030410767, discriminator loss: 1.1980819702148438 
Batch 1572, generator loss: 0.8428586721420288, discriminator loss: 1.2535488605499268 
Batch 1573, generator loss: 0.8486573696136475, discriminator loss: 1.26499342918396 
Batch 1574, generator loss: 0.8488778471946716, discriminator loss: 1.253472089767456 
Batch 1575, generator loss: 0.882256269454956, discriminator loss: 1.224463939666748 
Batch 1576, generator loss: 0.8676272630691528, discriminator loss: 1.3009884357452393 
Batch 1577, generator loss: 0.8737940192222595, discriminator loss: 1.2485291957855225 
Batch 1578, generator loss: 0.8528969287872314, discriminator loss: 1.2428712844848633 
Batch 1579, generator loss: 0.8529001474380493, discriminator loss: 1.2502121925354004 
Batch 1580, generator loss: 0.8734122514724731, discriminator loss: 1.2222387790679932 
Batch 1581, generator loss: 0.8389819860458374, discriminator loss: 1.205802083015442 
Batch 1582, generator loss: 0.8668392300605774, discriminator loss: 1.2227753400802612 
Batch 1583, generator loss: 0.9009017944335938, discriminator loss: 1.1773110628128052 
Batch 1584, generator loss: 0.9025306701660156, discriminator loss: 1.2097020149230957 
Batch 1585, generator loss: 0.8753024339675903, discriminator loss: 1.2097110748291016 
Batch 1586, generator loss: 0.90202796459198, discriminator loss: 1.1432743072509766 
Batch 1587, generator loss: 0.8975564241409302, discriminator loss: 1.145902156829834 
Batch 1588, generator loss: 0.8881345987319946, discriminator loss: 1.126320719718933 
Batch 1589, generator loss: 0.9067308902740479, discriminator loss: 1.1169050931930542 
Batch 1590, generator loss: 0.9294564127922058, discriminator loss: 1.1290136575698853 
Batch 1591, generator loss: 0.9401010870933533, discriminator loss: 1.0961177349090576 
Batch 1592, generator loss: 0.9665926098823547, discriminator loss: 1.0666583776474 
Batch 1593, generator loss: 0.9720715284347534, discriminator loss: 1.0729459524154663 
Batch 1594, generator loss: 0.9788951873779297, discriminator loss: 1.028666377067566 
Batch 1595, generator loss: 0.9902799129486084, discriminator loss: 1.0212184190750122 
Batch 1596, generator loss: 0.9898383021354675, discriminator loss: 1.0045485496520996 
Batch 1597, generator loss: 1.001162052154541, discriminator loss: 0.9726688861846924 
Batch 1598, generator loss: 1.0209404230117798, discriminator loss: 0.973185658454895 
Batch 1599, generator loss: 1.057976484298706, discriminator loss: 0.9420279264450073 
Batch 1600, generator loss: 1.0780837535858154, discriminator loss: 0.9256524443626404 
Batch 1601, generator loss: 1.0883727073669434, discriminator loss: 0.9027560949325562 
Batch 1602, generator loss: 1.1145204305648804, discriminator loss: 0.9216912388801575 
Batch 1603, generator loss: 1.106032133102417, discriminator loss: 0.8647359609603882 
Batch 1604, generator loss: 1.0947880744934082, discriminator loss: 0.8764545917510986 
Batch 1605, generator loss: 1.0832797288894653, discriminator loss: 0.8847905993461609 
Batch 1606, generator loss: 1.1161224842071533, discriminator loss: 0.8292757272720337 
Batch 1607, generator loss: 1.1522834300994873, discriminator loss: 0.8499369025230408 
Batch 1608, generator loss: 1.165435552597046, discriminator loss: 0.838010311126709 
Batch 1609, generator loss: 1.198767900466919, discriminator loss: 0.8054114580154419 
Batch 1610, generator loss: 1.2287718057632446, discriminator loss: 0.7983653545379639 
Batch 1611, generator loss: 1.1908633708953857, discriminator loss: 0.8123992085456848 
Batch 1612, generator loss: 1.2227120399475098, discriminator loss: 0.8186410069465637 
Batch 1613, generator loss: 1.196465015411377, discriminator loss: 0.7887874841690063 
Batch 1614, generator loss: 1.1901273727416992, discriminator loss: 0.8429197072982788 
Batch 1615, generator loss: 1.1943902969360352, discriminator loss: 0.8154727220535278 
Batch 1616, generator loss: 1.1932545900344849, discriminator loss: 0.830664336681366 
Batch 1617, generator loss: 1.1813151836395264, discriminator loss: 0.8479167819023132 
Batch 1618, generator loss: 1.14121675491333, discriminator loss: 0.8505462408065796 
Batch 1619, generator loss: 1.181795597076416, discriminator loss: 0.8703902959823608 
Batch 1620, generator loss: 1.2030367851257324, discriminator loss: 0.8629807233810425 
Batch 1621, generator loss: 1.181875228881836, discriminator loss: 0.9285631775856018 
Batch 1622, generator loss: 1.1914434432983398, discriminator loss: 0.8903216123580933 
Batch 1623, generator loss: 1.1926827430725098, discriminator loss: 0.9212534427642822 
Batch 1624, generator loss: 1.146213173866272, discriminator loss: 0.9608626365661621 
Batch 1625, generator loss: 1.1065665483474731, discriminator loss: 0.9285914301872253 
Batch 1626, generator loss: 1.0490460395812988, discriminator loss: 0.9873639345169067 
Batch 1627, generator loss: 1.0821006298065186, discriminator loss: 0.9574288129806519 
Batch 1628, generator loss: 1.1439523696899414, discriminator loss: 1.0617625713348389 
Batch 1629, generator loss: 1.1255583763122559, discriminator loss: 1.0294743776321411 
Batch 1630, generator loss: 1.1207444667816162, discriminator loss: 1.0691782236099243 
Batch 1631, generator loss: 1.100019931793213, discriminator loss: 1.1039981842041016 
Batch 1632, generator loss: 1.0458890199661255, discriminator loss: 1.1205286979675293 
Batch 1633, generator loss: 1.0129799842834473, discriminator loss: 1.188825011253357 
Batch 1634, generator loss: 0.9382182955741882, discriminator loss: 1.1790382862091064 
Batch 1635, generator loss: 0.9192833304405212, discriminator loss: 1.199570894241333 
Batch 1636, generator loss: 0.9481766819953918, discriminator loss: 1.2751376628875732 
Batch 1637, generator loss: 0.921910285949707, discriminator loss: 1.3066926002502441 
Batch 1638, generator loss: 0.8870439529418945, discriminator loss: 1.34092378616333 
Batch 1639, generator loss: 0.880937933921814, discriminator loss: 1.2967802286148071 
Batch 1640, generator loss: 0.8675079345703125, discriminator loss: 1.3674819469451904 
Batch 1641, generator loss: 0.8628363609313965, discriminator loss: 1.4671456813812256 
Batch 1642, generator loss: 0.7989298105239868, discriminator loss: 1.40606689453125 
Batch 1643, generator loss: 0.817755937576294, discriminator loss: 1.4167065620422363 
Batch 1644, generator loss: 0.7770118713378906, discriminator loss: 1.3950507640838623 
Batch 1645, generator loss: 0.7133352756500244, discriminator loss: 1.5048954486846924 
Batch 1646, generator loss: 0.7256647348403931, discriminator loss: 1.4789438247680664 
Batch 1647, generator loss: 0.7816370725631714, discriminator loss: 1.4725985527038574 
Batch 1648, generator loss: 0.7741991877555847, discriminator loss: 1.5621964931488037 
Batch 1649, generator loss: 0.703595757484436, discriminator loss: 1.5717862844467163 
Batch 1650, generator loss: 0.6888260841369629, discriminator loss: 1.535334825515747 
Batch 1651, generator loss: 0.7429519295692444, discriminator loss: 1.4310171604156494 
Batch 1652, generator loss: 0.7186344861984253, discriminator loss: 1.5101674795150757 
Batch 1653, generator loss: 0.7010225057601929, discriminator loss: 1.5674716234207153 
Batch 1654, generator loss: 0.7034924626350403, discriminator loss: 1.5664832592010498 
Batch 1655, generator loss: 0.6848693490028381, discriminator loss: 1.5442458391189575 
Batch 1656, generator loss: 0.6927539110183716, discriminator loss: 1.5982840061187744 
Batch 1657, generator loss: 0.6969383955001831, discriminator loss: 1.5565290451049805 
Batch 1658, generator loss: 0.7244833707809448, discriminator loss: 1.549278974533081 
Batch 1659, generator loss: 0.7041445970535278, discriminator loss: 1.52803373336792 
Batch 1660, generator loss: 0.6671421527862549, discriminator loss: 1.5577950477600098 
Batch 1661, generator loss: 0.676673412322998, discriminator loss: 1.5212441682815552 
Batch 1662, generator loss: 0.681280791759491, discriminator loss: 1.5280545949935913 
Batch 1663, generator loss: 0.7315809726715088, discriminator loss: 1.4109348058700562 
Batch 1664, generator loss: 0.7453142404556274, discriminator loss: 1.4578880071640015 
Batch 1665, generator loss: 0.7474961876869202, discriminator loss: 1.3760149478912354 
Batch 1666, generator loss: 0.7656188011169434, discriminator loss: 1.4218024015426636 
Batch 1667, generator loss: 0.7688195109367371, discriminator loss: 1.3820393085479736 
Batch 1668, generator loss: 0.7580000162124634, discriminator loss: 1.3746873140335083 
Batch 1669, generator loss: 0.7779662609100342, discriminator loss: 1.3317997455596924 
Batch 1670, generator loss: 0.7998768091201782, discriminator loss: 1.2861969470977783 
Batch 1671, generator loss: 0.8054476380348206, discriminator loss: 1.3138798475265503 
Batch 1672, generator loss: 0.8464613556861877, discriminator loss: 1.2715283632278442 
Batch 1673, generator loss: 0.8479869365692139, discriminator loss: 1.2904237508773804 
Batch 1674, generator loss: 0.842586874961853, discriminator loss: 1.2338464260101318 
Batch 1675, generator loss: 0.840015172958374, discriminator loss: 1.194692611694336 
Batch 1676, generator loss: 0.8501172065734863, discriminator loss: 1.1547596454620361 
Batch 1677, generator loss: 0.8134797215461731, discriminator loss: 1.178152084350586 
Batch 1678, generator loss: 0.8693553805351257, discriminator loss: 1.1646809577941895 
Batch 1679, generator loss: 0.8971314430236816, discriminator loss: 1.1588711738586426 
Batch 1680, generator loss: 0.921842098236084, discriminator loss: 1.181457281112671 
Batch 1681, generator loss: 0.9036567807197571, discriminator loss: 1.1799439191818237 
Batch 1682, generator loss: 0.9294474720954895, discriminator loss: 1.2122690677642822 
Batch 1683, generator loss: 0.8786927461624146, discriminator loss: 1.1229338645935059 
Batch 1684, generator loss: 0.8796150088310242, discriminator loss: 1.162299394607544 
Batch 1685, generator loss: 0.8807529211044312, discriminator loss: 1.1671721935272217 
Batch 1686, generator loss: 0.8998501300811768, discriminator loss: 1.1375596523284912 
Batch 1687, generator loss: 0.900684118270874, discriminator loss: 1.2125647068023682 
Batch 1688, generator loss: 0.9004988670349121, discriminator loss: 1.1944031715393066 
Batch 1689, generator loss: 0.8813449740409851, discriminator loss: 1.1989836692810059 
Batch 1690, generator loss: 0.8889973163604736, discriminator loss: 1.2607052326202393 
Batch 1691, generator loss: 0.8817919492721558, discriminator loss: 1.222154140472412 
Batch 1692, generator loss: 0.8365888595581055, discriminator loss: 1.271440863609314 
Batch 1693, generator loss: 0.8746974468231201, discriminator loss: 1.214570164680481 
Batch 1694, generator loss: 0.8830075263977051, discriminator loss: 1.3378217220306396 
Batch 1695, generator loss: 0.8793147802352905, discriminator loss: 1.371666431427002 
Batch 1696, generator loss: 0.8478585481643677, discriminator loss: 1.3427228927612305 
Batch 1697, generator loss: 0.8375383615493774, discriminator loss: 1.331390142440796 
Batch 1698, generator loss: 0.8296430110931396, discriminator loss: 1.3534181118011475 
Batch 1699, generator loss: 0.7893573641777039, discriminator loss: 1.4352378845214844 
Batch 1700, generator loss: 0.779816746711731, discriminator loss: 1.4845209121704102 
Batch 1701, generator loss: 0.7696949243545532, discriminator loss: 1.499143362045288 
Batch 1702, generator loss: 0.7494707107543945, discriminator loss: 1.4951328039169312 
Batch 1703, generator loss: 0.764961838722229, discriminator loss: 1.5085153579711914 
Batch 1704, generator loss: 0.7542529106140137, discriminator loss: 1.6198325157165527 
Batch 1705, generator loss: 0.7590130567550659, discriminator loss: 1.5796384811401367 
Batch 1706, generator loss: 0.7393295764923096, discriminator loss: 1.6179344654083252 
Batch 1707, generator loss: 0.7174339294433594, discriminator loss: 1.6393651962280273 
Batch 1708, generator loss: 0.6955090761184692, discriminator loss: 1.6011210680007935 
Batch 1709, generator loss: 0.7019476294517517, discriminator loss: 1.731102466583252 
Batch 1710, generator loss: 0.7396393418312073, discriminator loss: 1.6950099468231201 
Batch 1711, generator loss: 0.7055844068527222, discriminator loss: 1.8147242069244385 
Batch 1712, generator loss: 0.6681944131851196, discriminator loss: 1.6831321716308594 
Batch 1713, generator loss: 0.6594057083129883, discriminator loss: 1.7002873420715332 
Batch 1714, generator loss: 0.651740550994873, discriminator loss: 1.7907443046569824 
Batch 1715, generator loss: 0.7086310386657715, discriminator loss: 1.567983627319336 
Batch 1716, generator loss: 0.6985042095184326, discriminator loss: 1.7398502826690674 
Batch 1717, generator loss: 0.7263780236244202, discriminator loss: 1.7781709432601929 
Batch 1718, generator loss: 0.6999891996383667, discriminator loss: 1.7540849447250366 
Batch 1719, generator loss: 0.6704225540161133, discriminator loss: 1.7239633798599243 
Batch 1720, generator loss: 0.6473894119262695, discriminator loss: 1.7546628713607788 
Batch 1721, generator loss: 0.6279903054237366, discriminator loss: 1.8336195945739746 
Batch 1722, generator loss: 0.6470807194709778, discriminator loss: 1.7102291584014893 
Batch 1723, generator loss: 0.6822443604469299, discriminator loss: 1.7466837167739868 
Batch 1724, generator loss: 0.7228876948356628, discriminator loss: 1.7284703254699707 
Batch 1725, generator loss: 0.7167234420776367, discriminator loss: 1.7176165580749512 
Batch 1726, generator loss: 0.6998472213745117, discriminator loss: 1.7266032695770264 
Batch 1727, generator loss: 0.6865755319595337, discriminator loss: 1.5980963706970215 
Batch 1728, generator loss: 0.6920320987701416, discriminator loss: 1.655470848083496 
Batch 1729, generator loss: 0.6768349409103394, discriminator loss: 1.6109334230422974 
Batch 1730, generator loss: 0.7220321893692017, discriminator loss: 1.5968445539474487 
Batch 1731, generator loss: 0.7195641398429871, discriminator loss: 1.6285088062286377 
Batch 1732, generator loss: 0.7344415187835693, discriminator loss: 1.5634088516235352 
Batch 1733, generator loss: 0.7061945199966431, discriminator loss: 1.5788837671279907 
Batch 1734, generator loss: 0.7228530049324036, discriminator loss: 1.5762298107147217 
Batch 1735, generator loss: 0.7186756134033203, discriminator loss: 1.587556004524231 
Batch 1736, generator loss: 0.7105269432067871, discriminator loss: 1.5035768747329712 
Batch 1737, generator loss: 0.7327351570129395, discriminator loss: 1.5325474739074707 
Batch 1738, generator loss: 0.7368703484535217, discriminator loss: 1.4895132780075073 
Batch 1739, generator loss: 0.7308658957481384, discriminator loss: 1.489967703819275 
Batch 1740, generator loss: 0.738968014717102, discriminator loss: 1.480931282043457 
Batch 1741, generator loss: 0.7546777725219727, discriminator loss: 1.4266785383224487 
Batch 1742, generator loss: 0.7696080207824707, discriminator loss: 1.4248460531234741 
Batch 1743, generator loss: 0.7667780518531799, discriminator loss: 1.3917287588119507 
Batch 1744, generator loss: 0.7748241424560547, discriminator loss: 1.3591618537902832 
Batch 1745, generator loss: 0.7804962396621704, discriminator loss: 1.3637577295303345 
Batch 1746, generator loss: 0.7942781448364258, discriminator loss: 1.3951220512390137 
Batch 1747, generator loss: 0.7961214780807495, discriminator loss: 1.3593342304229736 
Batch 1748, generator loss: 0.7775254249572754, discriminator loss: 1.3691680431365967 
Batch 1749, generator loss: 0.7870484590530396, discriminator loss: 1.3500330448150635 
Batch 1750, generator loss: 0.7599220275878906, discriminator loss: 1.3774073123931885 
Batch 1751, generator loss: 0.7863305807113647, discriminator loss: 1.3154165744781494 
Batch 1752, generator loss: 0.7945315837860107, discriminator loss: 1.2972121238708496 
Batch 1753, generator loss: 0.8018636703491211, discriminator loss: 1.3026151657104492 
Batch 1754, generator loss: 0.828152060508728, discriminator loss: 1.3201698064804077 
Batch 1755, generator loss: 0.8196269869804382, discriminator loss: 1.2672367095947266 
Batch 1756, generator loss: 0.8246496915817261, discriminator loss: 1.2688510417938232 
Batch 1757, generator loss: 0.8405709862709045, discriminator loss: 1.2611067295074463 
Batch 1758, generator loss: 0.8139039874076843, discriminator loss: 1.275357961654663 
Batch 1759, generator loss: 0.8325477838516235, discriminator loss: 1.227115273475647 
Batch 1760, generator loss: 0.8241429328918457, discriminator loss: 1.2483720779418945 
Batch 1761, generator loss: 0.8310409188270569, discriminator loss: 1.248542070388794 
Batch 1762, generator loss: 0.8272272348403931, discriminator loss: 1.2281399965286255 
Batch 1763, generator loss: 0.8483710289001465, discriminator loss: 1.2156250476837158 
Batch 1764, generator loss: 0.8485906720161438, discriminator loss: 1.2145622968673706 
Batch 1765, generator loss: 0.8476464748382568, discriminator loss: 1.2224429845809937 
Batch 1766, generator loss: 0.8595719933509827, discriminator loss: 1.2167448997497559 
Batch 1767, generator loss: 0.8568510413169861, discriminator loss: 1.199378490447998 
Batch 1768, generator loss: 0.8486295938491821, discriminator loss: 1.2165541648864746 
Batch 1769, generator loss: 0.8646421432495117, discriminator loss: 1.1919398307800293 
Batch 1770, generator loss: 0.8628153204917908, discriminator loss: 1.219104290008545 
Batch 1771, generator loss: 0.8555281758308411, discriminator loss: 1.1879006624221802 
Batch 1772, generator loss: 0.8669224977493286, discriminator loss: 1.1966242790222168 
Generator loss: tf.Tensor(0.8669225, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.1966243, shape=(), dtype=float32)
EPOCH: 7
Batch 1794, generator loss: 0.8612897396087646, discriminator loss: 1.188715934753418 
Batch 1795, generator loss: 0.8486179709434509, discriminator loss: 1.2279455661773682 
Batch 1796, generator loss: 0.8871164917945862, discriminator loss: 1.2111283540725708 
Batch 1797, generator loss: 0.902895450592041, discriminator loss: 1.2116937637329102 
Batch 1798, generator loss: 0.8583857417106628, discriminator loss: 1.2368862628936768 
Batch 1799, generator loss: 0.8290281295776367, discriminator loss: 1.1948847770690918 
Batch 1800, generator loss: 0.8338101506233215, discriminator loss: 1.1954030990600586 
Batch 1801, generator loss: 0.8267303705215454, discriminator loss: 1.2153656482696533 
Batch 1802, generator loss: 0.8668230175971985, discriminator loss: 1.2341701984405518 
Batch 1803, generator loss: 0.8879706859588623, discriminator loss: 1.219052791595459 
Batch 1804, generator loss: 0.8907601237297058, discriminator loss: 1.2046468257904053 
Batch 1805, generator loss: 0.8928802013397217, discriminator loss: 1.1943345069885254 
Batch 1806, generator loss: 0.8786684274673462, discriminator loss: 1.2041466236114502 
Batch 1807, generator loss: 0.8784067034721375, discriminator loss: 1.2261446714401245 
Batch 1808, generator loss: 0.8498318195343018, discriminator loss: 1.2239720821380615 
Batch 1809, generator loss: 0.8221423625946045, discriminator loss: 1.225472092628479 
Batch 1810, generator loss: 0.8391093015670776, discriminator loss: 1.2367725372314453 
Batch 1811, generator loss: 0.8464527130126953, discriminator loss: 1.193504810333252 
Batch 1812, generator loss: 0.8805136680603027, discriminator loss: 1.25327467918396 
Batch 1813, generator loss: 0.8794752955436707, discriminator loss: 1.220110297203064 
Batch 1814, generator loss: 0.8839923739433289, discriminator loss: 1.2700719833374023 
Batch 1815, generator loss: 0.9074379801750183, discriminator loss: 1.2430813312530518 
Batch 1816, generator loss: 0.8606233596801758, discriminator loss: 1.318217158317566 
Batch 1817, generator loss: 0.8319281339645386, discriminator loss: 1.2173999547958374 
Batch 1818, generator loss: 0.804613471031189, discriminator loss: 1.2966573238372803 
Batch 1819, generator loss: 0.7917959690093994, discriminator loss: 1.2801167964935303 
Batch 1820, generator loss: 0.7855705618858337, discriminator loss: 1.310232400894165 
Batch 1821, generator loss: 0.8650016784667969, discriminator loss: 1.264704704284668 
Batch 1822, generator loss: 0.88653564453125, discriminator loss: 1.3437076807022095 
Batch 1823, generator loss: 0.8891700506210327, discriminator loss: 1.2933059930801392 
Batch 1824, generator loss: 0.8428112268447876, discriminator loss: 1.3029522895812988 
Batch 1825, generator loss: 0.8110625147819519, discriminator loss: 1.324830412864685 
Batch 1826, generator loss: 0.826820969581604, discriminator loss: 1.2726795673370361 
Batch 1827, generator loss: 0.7675860524177551, discriminator loss: 1.3004330396652222 
Batch 1828, generator loss: 0.7968593239784241, discriminator loss: 1.2698910236358643 
Batch 1829, generator loss: 0.812974214553833, discriminator loss: 1.3677786588668823 
Batch 1830, generator loss: 0.859822154045105, discriminator loss: 1.2505733966827393 
Batch 1831, generator loss: 0.8724141120910645, discriminator loss: 1.319563388824463 
Batch 1832, generator loss: 0.8725390434265137, discriminator loss: 1.270866870880127 
Batch 1833, generator loss: 0.8259309530258179, discriminator loss: 1.253922939300537 
Batch 1834, generator loss: 0.8118497133255005, discriminator loss: 1.2737395763397217 
Batch 1835, generator loss: 0.8095004558563232, discriminator loss: 1.2798070907592773 
Batch 1836, generator loss: 0.8123440742492676, discriminator loss: 1.2418532371520996 
Batch 1837, generator loss: 0.8228209614753723, discriminator loss: 1.2929381132125854 
Batch 1838, generator loss: 0.8429330587387085, discriminator loss: 1.2744550704956055 
Batch 1839, generator loss: 0.8633350133895874, discriminator loss: 1.2439892292022705 
Batch 1840, generator loss: 0.8645485043525696, discriminator loss: 1.201221227645874 
Batch 1841, generator loss: 0.8641935586929321, discriminator loss: 1.2496576309204102 
Batch 1842, generator loss: 0.8641302585601807, discriminator loss: 1.1695363521575928 
Batch 1843, generator loss: 0.8538162708282471, discriminator loss: 1.2033203840255737 
Batch 1844, generator loss: 0.8624624013900757, discriminator loss: 1.1614093780517578 
Batch 1845, generator loss: 0.861091673374176, discriminator loss: 1.164247751235962 
Batch 1846, generator loss: 0.8691713213920593, discriminator loss: 1.202681303024292 
Batch 1847, generator loss: 0.8942772150039673, discriminator loss: 1.1126290559768677 
Batch 1848, generator loss: 0.9094507098197937, discriminator loss: 1.170849323272705 
Batch 1849, generator loss: 0.878211498260498, discriminator loss: 1.14193856716156 
Batch 1850, generator loss: 0.8655989170074463, discriminator loss: 1.1819030046463013 
Batch 1851, generator loss: 0.8691763877868652, discriminator loss: 1.0628174543380737 
Batch 1852, generator loss: 0.897170901298523, discriminator loss: 1.137568712234497 
Batch 1853, generator loss: 0.9196534156799316, discriminator loss: 1.0723254680633545 
Batch 1854, generator loss: 0.9077603816986084, discriminator loss: 1.0912928581237793 
Batch 1855, generator loss: 0.9503766298294067, discriminator loss: 1.0566761493682861 
Batch 1856, generator loss: 0.9531258940696716, discriminator loss: 1.0961885452270508 
Batch 1857, generator loss: 0.9417439699172974, discriminator loss: 1.0782415866851807 
Batch 1858, generator loss: 0.932134747505188, discriminator loss: 1.0913710594177246 
Batch 1859, generator loss: 0.9529741406440735, discriminator loss: 1.0274088382720947 
Batch 1860, generator loss: 0.9311349987983704, discriminator loss: 1.0173730850219727 
Batch 1861, generator loss: 0.9152822494506836, discriminator loss: 1.0433998107910156 
Batch 1862, generator loss: 0.9082891941070557, discriminator loss: 1.0465198755264282 
Batch 1863, generator loss: 0.9497502446174622, discriminator loss: 1.0128865242004395 
Batch 1864, generator loss: 0.9701838493347168, discriminator loss: 1.0588948726654053 
Batch 1865, generator loss: 0.9897446036338806, discriminator loss: 1.0413031578063965 
Batch 1866, generator loss: 0.9489263296127319, discriminator loss: 1.0093921422958374 
Batch 1867, generator loss: 0.9504830241203308, discriminator loss: 1.0188586711883545 
Batch 1868, generator loss: 0.9553379416465759, discriminator loss: 1.0592617988586426 
Batch 1869, generator loss: 0.9529271125793457, discriminator loss: 1.0415347814559937 
Batch 1870, generator loss: 0.924015998840332, discriminator loss: 1.0707136392593384 
Batch 1871, generator loss: 0.9250365495681763, discriminator loss: 1.1111010313034058 
Batch 1872, generator loss: 0.9042254686355591, discriminator loss: 1.0942151546478271 
Batch 1873, generator loss: 0.9220181703567505, discriminator loss: 1.0860422849655151 
Batch 1874, generator loss: 0.9132461547851562, discriminator loss: 1.0976598262786865 
Batch 1875, generator loss: 0.8919960260391235, discriminator loss: 1.1252906322479248 
Batch 1876, generator loss: 0.8995869159698486, discriminator loss: 1.1140631437301636 
Batch 1877, generator loss: 0.8952629566192627, discriminator loss: 1.175761342048645 
Batch 1878, generator loss: 0.8837543725967407, discriminator loss: 1.1356194019317627 
Batch 1879, generator loss: 0.8410729169845581, discriminator loss: 1.1789630651474 
Batch 1880, generator loss: 0.8594616651535034, discriminator loss: 1.1957985162734985 
Batch 1881, generator loss: 0.8575069904327393, discriminator loss: 1.202086091041565 
Batch 1882, generator loss: 0.8216601610183716, discriminator loss: 1.2229020595550537 
Batch 1883, generator loss: 0.8428404331207275, discriminator loss: 1.2136058807373047 
Batch 1884, generator loss: 0.8256998062133789, discriminator loss: 1.274597406387329 
Batch 1885, generator loss: 0.8390690088272095, discriminator loss: 1.2623717784881592 
Batch 1886, generator loss: 0.8648965954780579, discriminator loss: 1.2539796829223633 
Batch 1887, generator loss: 0.8294281363487244, discriminator loss: 1.2408363819122314 
Batch 1888, generator loss: 0.8010462522506714, discriminator loss: 1.2751621007919312 
Batch 1889, generator loss: 0.8114195466041565, discriminator loss: 1.2762799263000488 
Batch 1890, generator loss: 0.7914832830429077, discriminator loss: 1.2969000339508057 
Batch 1891, generator loss: 0.7773433923721313, discriminator loss: 1.3294168710708618 
Batch 1892, generator loss: 0.776820957660675, discriminator loss: 1.299527645111084 
Batch 1893, generator loss: 0.7973400354385376, discriminator loss: 1.3169430494308472 
Batch 1894, generator loss: 0.8043431043624878, discriminator loss: 1.2995635271072388 
Batch 1895, generator loss: 0.7834736704826355, discriminator loss: 1.3459378480911255 
Batch 1896, generator loss: 0.7774272561073303, discriminator loss: 1.3176348209381104 
Batch 1897, generator loss: 0.7883619070053101, discriminator loss: 1.3227312564849854 
Batch 1898, generator loss: 0.7932033538818359, discriminator loss: 1.321946144104004 
Batch 1899, generator loss: 0.8012161254882812, discriminator loss: 1.3175398111343384 
Batch 1900, generator loss: 0.7634305953979492, discriminator loss: 1.328416109085083 
Batch 1901, generator loss: 0.7895288467407227, discriminator loss: 1.304957389831543 
Batch 1902, generator loss: 0.8077033758163452, discriminator loss: 1.303317666053772 
Batch 1903, generator loss: 0.8016958832740784, discriminator loss: 1.2626475095748901 
Batch 1904, generator loss: 0.7793546915054321, discriminator loss: 1.3088514804840088 
Batch 1905, generator loss: 0.7859485149383545, discriminator loss: 1.2821121215820312 
Batch 1906, generator loss: 0.8110940456390381, discriminator loss: 1.2731833457946777 
Batch 1907, generator loss: 0.8257043957710266, discriminator loss: 1.2919989824295044 
Batch 1908, generator loss: 0.806932806968689, discriminator loss: 1.2660053968429565 
Batch 1909, generator loss: 0.8281540870666504, discriminator loss: 1.2804661989212036 
Batch 1910, generator loss: 0.8319754600524902, discriminator loss: 1.2190666198730469 
Batch 1911, generator loss: 0.8173843026161194, discriminator loss: 1.1944884061813354 
Batch 1912, generator loss: 0.8682711124420166, discriminator loss: 1.2096459865570068 
Batch 1913, generator loss: 0.8732926249504089, discriminator loss: 1.1822330951690674 
Batch 1914, generator loss: 0.905791163444519, discriminator loss: 1.1760802268981934 
Batch 1915, generator loss: 0.9008491635322571, discriminator loss: 1.1674168109893799 
Batch 1916, generator loss: 0.9259637594223022, discriminator loss: 1.1566755771636963 
Batch 1917, generator loss: 0.8773962259292603, discriminator loss: 1.166002869606018 
Batch 1918, generator loss: 0.8643550872802734, discriminator loss: 1.1342036724090576 
Batch 1919, generator loss: 0.8878843188285828, discriminator loss: 1.1901912689208984 
Batch 1920, generator loss: 0.9084292650222778, discriminator loss: 1.1573290824890137 
Batch 1921, generator loss: 0.9585884809494019, discriminator loss: 1.1415455341339111 
Batch 1922, generator loss: 0.9369630217552185, discriminator loss: 1.185654878616333 
Batch 1923, generator loss: 0.9696869254112244, discriminator loss: 1.1303625106811523 
Batch 1924, generator loss: 0.9835113286972046, discriminator loss: 1.1297225952148438 
Batch 1925, generator loss: 0.940192461013794, discriminator loss: 1.1408870220184326 
Batch 1926, generator loss: 0.9470216035842896, discriminator loss: 1.159934401512146 
Batch 1927, generator loss: 0.9440054893493652, discriminator loss: 1.1341729164123535 
Batch 1928, generator loss: 0.9161828756332397, discriminator loss: 1.1791958808898926 
Batch 1929, generator loss: 0.9814440011978149, discriminator loss: 1.126686692237854 
Batch 1930, generator loss: 0.9629437923431396, discriminator loss: 1.1847307682037354 
Batch 1931, generator loss: 1.007894515991211, discriminator loss: 1.1483536958694458 
Batch 1932, generator loss: 1.0017712116241455, discriminator loss: 1.168428659439087 
Batch 1933, generator loss: 0.9835299253463745, discriminator loss: 1.1887048482894897 
Batch 1934, generator loss: 0.9950965642929077, discriminator loss: 1.1825858354568481 
Batch 1935, generator loss: 0.9750430583953857, discriminator loss: 1.2111830711364746 
Batch 1936, generator loss: 0.9553698301315308, discriminator loss: 1.2350634336471558 
Batch 1937, generator loss: 0.9628701210021973, discriminator loss: 1.1888841390609741 
Batch 1938, generator loss: 0.9249933958053589, discriminator loss: 1.2300715446472168 
Batch 1939, generator loss: 1.008865237236023, discriminator loss: 1.2400784492492676 
Batch 1940, generator loss: 0.9881428480148315, discriminator loss: 1.331788182258606 
Batch 1941, generator loss: 0.9548758268356323, discriminator loss: 1.3662874698638916 
Batch 1942, generator loss: 0.900871217250824, discriminator loss: 1.3115568161010742 
Batch 1943, generator loss: 0.8826050758361816, discriminator loss: 1.3363580703735352 
Batch 1944, generator loss: 0.8873038291931152, discriminator loss: 1.3576221466064453 
Batch 1945, generator loss: 0.8680508136749268, discriminator loss: 1.361382246017456 
Batch 1946, generator loss: 0.8748549222946167, discriminator loss: 1.3537603616714478 
Batch 1947, generator loss: 0.9452670812606812, discriminator loss: 1.4397313594818115 
Batch 1948, generator loss: 0.9259214401245117, discriminator loss: 1.4280369281768799 
Batch 1949, generator loss: 0.8858885765075684, discriminator loss: 1.4192004203796387 
Batch 1950, generator loss: 0.8596592545509338, discriminator loss: 1.3905203342437744 
Batch 1951, generator loss: 0.8487889766693115, discriminator loss: 1.471590280532837 
Batch 1952, generator loss: 0.8051052093505859, discriminator loss: 1.455049991607666 
Batch 1953, generator loss: 0.7963196039199829, discriminator loss: 1.5206217765808105 
Batch 1954, generator loss: 0.7952239513397217, discriminator loss: 1.4789506196975708 
Batch 1955, generator loss: 0.8409824371337891, discriminator loss: 1.4601751565933228 
Batch 1956, generator loss: 0.8339966535568237, discriminator loss: 1.5196754932403564 
Batch 1957, generator loss: 0.8241937756538391, discriminator loss: 1.4885342121124268 
Batch 1958, generator loss: 0.7823194861412048, discriminator loss: 1.5095009803771973 
Batch 1959, generator loss: 0.7590732574462891, discriminator loss: 1.4669687747955322 
Batch 1960, generator loss: 0.7581110000610352, discriminator loss: 1.446406364440918 
Batch 1961, generator loss: 0.7923375368118286, discriminator loss: 1.4547462463378906 
Batch 1962, generator loss: 0.8301835060119629, discriminator loss: 1.3998847007751465 
Batch 1963, generator loss: 0.8641411066055298, discriminator loss: 1.50034761428833 
Batch 1964, generator loss: 0.8594827651977539, discriminator loss: 1.3930913209915161 
Batch 1965, generator loss: 0.7787691354751587, discriminator loss: 1.38755202293396 
Batch 1966, generator loss: 0.783674955368042, discriminator loss: 1.3765507936477661 
Batch 1967, generator loss: 0.7851439714431763, discriminator loss: 1.3868993520736694 
Batch 1968, generator loss: 0.7833464741706848, discriminator loss: 1.3863458633422852 
Batch 1969, generator loss: 0.8267894983291626, discriminator loss: 1.3477524518966675 
Batch 1970, generator loss: 0.8681426644325256, discriminator loss: 1.2943130731582642 
Batch 1971, generator loss: 0.8731844425201416, discriminator loss: 1.3084638118743896 
Batch 1972, generator loss: 0.8908851146697998, discriminator loss: 1.2780160903930664 
Batch 1973, generator loss: 0.8847531676292419, discriminator loss: 1.2967901229858398 
Batch 1974, generator loss: 0.842862606048584, discriminator loss: 1.273998498916626 
Batch 1975, generator loss: 0.8234415054321289, discriminator loss: 1.239367961883545 
Batch 1976, generator loss: 0.8470838069915771, discriminator loss: 1.2222356796264648 
Batch 1977, generator loss: 0.8393110036849976, discriminator loss: 1.2219822406768799 
Batch 1978, generator loss: 0.8854656219482422, discriminator loss: 1.2119066715240479 
Batch 1979, generator loss: 0.907045841217041, discriminator loss: 1.220178484916687 
Batch 1980, generator loss: 0.9324917197227478, discriminator loss: 1.1906890869140625 
Batch 1981, generator loss: 0.9443442821502686, discriminator loss: 1.1267520189285278 
Batch 1982, generator loss: 0.9371711015701294, discriminator loss: 1.131439447402954 
Batch 1983, generator loss: 0.9275249242782593, discriminator loss: 1.1238911151885986 
Batch 1984, generator loss: 0.9205372333526611, discriminator loss: 1.1286853551864624 
Batch 1985, generator loss: 0.9243531227111816, discriminator loss: 1.116831660270691 
Batch 1986, generator loss: 0.9418649673461914, discriminator loss: 1.0755914449691772 
Batch 1987, generator loss: 0.9843654632568359, discriminator loss: 1.0672601461410522 
Batch 1988, generator loss: 0.9769240617752075, discriminator loss: 1.092374324798584 
Batch 1989, generator loss: 0.9726427793502808, discriminator loss: 1.0989383459091187 
Batch 1990, generator loss: 0.9915497303009033, discriminator loss: 1.0654089450836182 
Batch 1991, generator loss: 0.9700635671615601, discriminator loss: 1.1022236347198486 
Batch 1992, generator loss: 0.9715746641159058, discriminator loss: 1.084725260734558 
Batch 1993, generator loss: 0.9756214618682861, discriminator loss: 1.078230857849121 
Batch 1994, generator loss: 0.9649566411972046, discriminator loss: 1.0538182258605957 
Batch 1995, generator loss: 0.9980101585388184, discriminator loss: 1.081073522567749 
Batch 1996, generator loss: 0.9908254742622375, discriminator loss: 1.0710952281951904 
Batch 1997, generator loss: 1.021669864654541, discriminator loss: 1.0537855625152588 
Batch 1998, generator loss: 0.9937078952789307, discriminator loss: 1.072718858718872 
Batch 1999, generator loss: 0.9784250855445862, discriminator loss: 1.1110434532165527 
Batch 2000, generator loss: 0.9729760885238647, discriminator loss: 1.1290619373321533 
Batch 2001, generator loss: 0.9795708060264587, discriminator loss: 1.0683881044387817 
Batch 2002, generator loss: 0.9528266191482544, discriminator loss: 1.0994493961334229 
Batch 2003, generator loss: 0.9835866689682007, discriminator loss: 1.0935499668121338 
Batch 2004, generator loss: 0.9892374873161316, discriminator loss: 1.0772416591644287 
Batch 2005, generator loss: 1.0272440910339355, discriminator loss: 1.0352928638458252 
Batch 2006, generator loss: 1.0156209468841553, discriminator loss: 1.1227729320526123 
Batch 2007, generator loss: 1.0289008617401123, discriminator loss: 1.1031968593597412 
Batch 2008, generator loss: 1.0378825664520264, discriminator loss: 1.151947021484375 
Batch 2009, generator loss: 0.9785793423652649, discriminator loss: 1.1129553318023682 
Batch 2010, generator loss: 0.9389874935150146, discriminator loss: 1.1660380363464355 
Batch 2011, generator loss: 0.9233847856521606, discriminator loss: 1.1362509727478027 
Batch 2012, generator loss: 0.9110021591186523, discriminator loss: 1.1666624546051025 
Batch 2013, generator loss: 0.919337809085846, discriminator loss: 1.1748607158660889 
Batch 2014, generator loss: 0.9781157970428467, discriminator loss: 1.1909524202346802 
Batch 2015, generator loss: 0.9696022272109985, discriminator loss: 1.155212640762329 
Batch 2016, generator loss: 0.9489607810974121, discriminator loss: 1.2879271507263184 
Batch 2017, generator loss: 0.9119918346405029, discriminator loss: 1.231433391571045 
Batch 2018, generator loss: 0.9095361232757568, discriminator loss: 1.2576756477355957 
Batch 2019, generator loss: 0.8373793363571167, discriminator loss: 1.2522410154342651 
Batch 2020, generator loss: 0.8347141146659851, discriminator loss: 1.2953835725784302 
Batch 2021, generator loss: 0.8213520050048828, discriminator loss: 1.2930941581726074 
Batch 2022, generator loss: 0.8627581596374512, discriminator loss: 1.28294837474823 
Batch 2023, generator loss: 0.8901253938674927, discriminator loss: 1.2477184534072876 
Batch 2024, generator loss: 0.9460451602935791, discriminator loss: 1.3142518997192383 
Batch 2025, generator loss: 0.9023863673210144, discriminator loss: 1.3282580375671387 
Batch 2026, generator loss: 0.8838592767715454, discriminator loss: 1.2699363231658936 
Batch 2027, generator loss: 0.8327996730804443, discriminator loss: 1.417770504951477 
Batch 2028, generator loss: 0.7955536842346191, discriminator loss: 1.2889058589935303 
Generator loss: tf.Tensor(0.7955537, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.2889059, shape=(), dtype=float32)
EPOCH: 8
Batch 2050, generator loss: 0.7855096459388733, discriminator loss: 1.2447391748428345 
Batch 2051, generator loss: 0.7991544008255005, discriminator loss: 1.2639039754867554 
Batch 2052, generator loss: 0.8498175144195557, discriminator loss: 1.3017029762268066 
Batch 2053, generator loss: 0.8828653693199158, discriminator loss: 1.3314170837402344 
Batch 2054, generator loss: 0.9153294563293457, discriminator loss: 1.319286584854126 
Batch 2055, generator loss: 0.8899685144424438, discriminator loss: 1.3310761451721191 
Batch 2056, generator loss: 0.869174599647522, discriminator loss: 1.3690435886383057 
Batch 2057, generator loss: 0.8231943845748901, discriminator loss: 1.3238722085952759 
Batch 2058, generator loss: 0.7653673887252808, discriminator loss: 1.3314255475997925 
Batch 2059, generator loss: 0.765853226184845, discriminator loss: 1.2615885734558105 
Batch 2060, generator loss: 0.795826256275177, discriminator loss: 1.3411617279052734 
Batch 2061, generator loss: 0.8103968501091003, discriminator loss: 1.2758897542953491 
Batch 2062, generator loss: 0.8186962604522705, discriminator loss: 1.328562617301941 
Batch 2063, generator loss: 0.8879481554031372, discriminator loss: 1.2183356285095215 
Batch 2064, generator loss: 0.8860619068145752, discriminator loss: 1.2415904998779297 
Batch 2065, generator loss: 0.8774428963661194, discriminator loss: 1.3224222660064697 
Batch 2066, generator loss: 0.8791335821151733, discriminator loss: 1.277630090713501 
Batch 2067, generator loss: 0.8441770076751709, discriminator loss: 1.2200566530227661 
Batch 2068, generator loss: 0.8285043239593506, discriminator loss: 1.2611277103424072 
Batch 2069, generator loss: 0.8157563805580139, discriminator loss: 1.2411320209503174 
Batch 2070, generator loss: 0.7990673184394836, discriminator loss: 1.2616056203842163 
Batch 2071, generator loss: 0.801718533039093, discriminator loss: 1.2718623876571655 
Batch 2072, generator loss: 0.8357892632484436, discriminator loss: 1.2203445434570312 
Batch 2073, generator loss: 0.8553617596626282, discriminator loss: 1.156364917755127 
Batch 2074, generator loss: 0.8499253988265991, discriminator loss: 1.2200965881347656 
Batch 2075, generator loss: 0.9048130512237549, discriminator loss: 1.217444658279419 
Batch 2076, generator loss: 0.8812775015830994, discriminator loss: 1.2213988304138184 
Batch 2077, generator loss: 0.8808298110961914, discriminator loss: 1.1947015523910522 
Batch 2078, generator loss: 0.9158115386962891, discriminator loss: 1.1542645692825317 
Batch 2079, generator loss: 0.8776567578315735, discriminator loss: 1.1499521732330322 
Batch 2080, generator loss: 0.8360878229141235, discriminator loss: 1.1864755153656006 
Batch 2081, generator loss: 0.8452309370040894, discriminator loss: 1.1302156448364258 
Batch 2082, generator loss: 0.8739713430404663, discriminator loss: 1.1178638935089111 
Batch 2083, generator loss: 0.8800519704818726, discriminator loss: 1.1444034576416016 
Batch 2084, generator loss: 0.8967334032058716, discriminator loss: 1.16633141040802 
Batch 2085, generator loss: 0.9569775462150574, discriminator loss: 1.1315205097198486 
Batch 2086, generator loss: 0.9312485456466675, discriminator loss: 1.152678370475769 
Batch 2087, generator loss: 0.9109205007553101, discriminator loss: 1.1286842823028564 
Batch 2088, generator loss: 0.8759868741035461, discriminator loss: 1.1501206159591675 
Batch 2089, generator loss: 0.8547383546829224, discriminator loss: 1.099307894706726 
Batch 2090, generator loss: 0.8677613735198975, discriminator loss: 1.1093401908874512 
Batch 2091, generator loss: 0.9043805599212646, discriminator loss: 1.1231820583343506 
Batch 2092, generator loss: 0.9009998440742493, discriminator loss: 1.1108191013336182 
Batch 2093, generator loss: 0.9119549989700317, discriminator loss: 1.1326299905776978 
Batch 2094, generator loss: 0.9103552103042603, discriminator loss: 1.113422155380249 
Batch 2095, generator loss: 0.972902238368988, discriminator loss: 1.0859429836273193 
Batch 2096, generator loss: 0.9205840229988098, discriminator loss: 1.117883563041687 
Batch 2097, generator loss: 0.9506618976593018, discriminator loss: 1.1552441120147705 
Batch 2098, generator loss: 0.9015721082687378, discriminator loss: 1.1327908039093018 
Batch 2099, generator loss: 0.8721884489059448, discriminator loss: 1.1358036994934082 
Batch 2100, generator loss: 0.8493149876594543, discriminator loss: 1.1355364322662354 
Batch 2101, generator loss: 0.877252459526062, discriminator loss: 1.1296360492706299 
Batch 2102, generator loss: 0.882412850856781, discriminator loss: 1.174487829208374 
Batch 2103, generator loss: 0.8788473606109619, discriminator loss: 1.1570627689361572 
Batch 2104, generator loss: 0.9056491851806641, discriminator loss: 1.1553455591201782 
Batch 2105, generator loss: 0.8929810523986816, discriminator loss: 1.1650433540344238 
Batch 2106, generator loss: 0.8945039510726929, discriminator loss: 1.1711680889129639 
Batch 2107, generator loss: 0.8727176785469055, discriminator loss: 1.1934998035430908 
Batch 2108, generator loss: 0.862553596496582, discriminator loss: 1.1892766952514648 
Batch 2109, generator loss: 0.857796311378479, discriminator loss: 1.1612703800201416 
Batch 2110, generator loss: 0.8708950877189636, discriminator loss: 1.2302509546279907 
Batch 2111, generator loss: 0.8391140699386597, discriminator loss: 1.239455223083496 
Batch 2112, generator loss: 0.8025653958320618, discriminator loss: 1.2453290224075317 
Batch 2113, generator loss: 0.841679573059082, discriminator loss: 1.207395315170288 
Batch 2114, generator loss: 0.8230485320091248, discriminator loss: 1.2191901206970215 
Batch 2115, generator loss: 0.863334059715271, discriminator loss: 1.250253677368164 
Batch 2116, generator loss: 0.8504723310470581, discriminator loss: 1.2444086074829102 
Batch 2117, generator loss: 0.8602763414382935, discriminator loss: 1.2286145687103271 
Batch 2118, generator loss: 0.862366795539856, discriminator loss: 1.2252061367034912 
Batch 2119, generator loss: 0.8736605048179626, discriminator loss: 1.2250123023986816 
Batch 2120, generator loss: 0.8599729537963867, discriminator loss: 1.230722427368164 
Batch 2121, generator loss: 0.8578338027000427, discriminator loss: 1.2107657194137573 
Batch 2122, generator loss: 0.83600914478302, discriminator loss: 1.2699663639068604 
Batch 2123, generator loss: 0.857813835144043, discriminator loss: 1.1803241968154907 
Batch 2124, generator loss: 0.8326753377914429, discriminator loss: 1.2777034044265747 
Batch 2125, generator loss: 0.8463975191116333, discriminator loss: 1.2705459594726562 
Batch 2126, generator loss: 0.8745822906494141, discriminator loss: 1.2213603258132935 
Batch 2127, generator loss: 0.8509800434112549, discriminator loss: 1.2534418106079102 
Batch 2128, generator loss: 0.8825118541717529, discriminator loss: 1.2115850448608398 
Batch 2129, generator loss: 0.9014124274253845, discriminator loss: 1.2271926403045654 
Batch 2130, generator loss: 0.8861717581748962, discriminator loss: 1.1980209350585938 
Batch 2131, generator loss: 0.8814725279808044, discriminator loss: 1.2077341079711914 
Batch 2132, generator loss: 0.865312397480011, discriminator loss: 1.2188259363174438 
Batch 2133, generator loss: 0.9222511053085327, discriminator loss: 1.2074146270751953 
Batch 2134, generator loss: 0.8957873582839966, discriminator loss: 1.20255708694458 
Batch 2135, generator loss: 0.9108611941337585, discriminator loss: 1.2322611808776855 
Batch 2136, generator loss: 0.9237492680549622, discriminator loss: 1.2146943807601929 
Batch 2137, generator loss: 0.9539483189582825, discriminator loss: 1.2084052562713623 
Batch 2138, generator loss: 0.9266130328178406, discriminator loss: 1.208740472793579 
Batch 2139, generator loss: 0.8774603605270386, discriminator loss: 1.2061305046081543 
Batch 2140, generator loss: 0.8865167498588562, discriminator loss: 1.1702537536621094 
Batch 2141, generator loss: 0.8992859125137329, discriminator loss: 1.1750197410583496 
Batch 2142, generator loss: 0.9087464213371277, discriminator loss: 1.2026233673095703 
Batch 2143, generator loss: 0.9354490041732788, discriminator loss: 1.1811797618865967 
Batch 2144, generator loss: 0.9599286317825317, discriminator loss: 1.174401044845581 
Batch 2145, generator loss: 0.9904889464378357, discriminator loss: 1.1632328033447266 
Batch 2146, generator loss: 1.0005574226379395, discriminator loss: 1.115249514579773 
Batch 2147, generator loss: 0.9431263208389282, discriminator loss: 1.187513828277588 
Batch 2148, generator loss: 0.9305911064147949, discriminator loss: 1.169673204421997 
Batch 2149, generator loss: 0.9087000489234924, discriminator loss: 1.1510319709777832 
Batch 2150, generator loss: 0.9042683839797974, discriminator loss: 1.2040249109268188 
Batch 2151, generator loss: 0.9225887060165405, discriminator loss: 1.171736240386963 
Batch 2152, generator loss: 0.9398714900016785, discriminator loss: 1.181547999382019 
Batch 2153, generator loss: 0.9473379850387573, discriminator loss: 1.1462743282318115 
Batch 2154, generator loss: 0.9395043849945068, discriminator loss: 1.1561039686203003 
Batch 2155, generator loss: 0.9734518527984619, discriminator loss: 1.1599931716918945 
Batch 2156, generator loss: 0.953728199005127, discriminator loss: 1.1530451774597168 
Batch 2157, generator loss: 0.9374647736549377, discriminator loss: 1.1821858882904053 
Batch 2158, generator loss: 0.9019201993942261, discriminator loss: 1.1957472562789917 
Batch 2159, generator loss: 0.8431106209754944, discriminator loss: 1.231458067893982 
Batch 2160, generator loss: 0.8725484609603882, discriminator loss: 1.196916103363037 
Batch 2161, generator loss: 0.8966511487960815, discriminator loss: 1.1686897277832031 
Batch 2162, generator loss: 0.9236799478530884, discriminator loss: 1.1723649501800537 
Batch 2163, generator loss: 0.9726187586784363, discriminator loss: 1.1372132301330566 
Batch 2164, generator loss: 0.9268553256988525, discriminator loss: 1.1681437492370605 
Batch 2165, generator loss: 0.9401599168777466, discriminator loss: 1.1700025796890259 
Batch 2166, generator loss: 0.8886939883232117, discriminator loss: 1.1655638217926025 
Batch 2167, generator loss: 0.8557451367378235, discriminator loss: 1.1660614013671875 
Batch 2168, generator loss: 0.8400632739067078, discriminator loss: 1.1886934041976929 
Batch 2169, generator loss: 0.8812456130981445, discriminator loss: 1.1610442399978638 
Batch 2170, generator loss: 0.8726850748062134, discriminator loss: 1.2194337844848633 
Batch 2171, generator loss: 0.9041919708251953, discriminator loss: 1.1779624223709106 
Batch 2172, generator loss: 0.8982512950897217, discriminator loss: 1.2157127857208252 
Batch 2173, generator loss: 0.911590039730072, discriminator loss: 1.2167630195617676 
Batch 2174, generator loss: 0.8247170448303223, discriminator loss: 1.256563425064087 
Batch 2175, generator loss: 0.8199275732040405, discriminator loss: 1.2330735921859741 
Batch 2176, generator loss: 0.8082479238510132, discriminator loss: 1.2326271533966064 
Batch 2177, generator loss: 0.8002296686172485, discriminator loss: 1.2694616317749023 
Batch 2178, generator loss: 0.8687581419944763, discriminator loss: 1.2208917140960693 
Batch 2179, generator loss: 0.8971725702285767, discriminator loss: 1.2563141584396362 
Batch 2180, generator loss: 0.9054112434387207, discriminator loss: 1.2497177124023438 
Batch 2181, generator loss: 0.851987361907959, discriminator loss: 1.2932806015014648 
Batch 2182, generator loss: 0.8015055656433105, discriminator loss: 1.2431007623672485 
Batch 2183, generator loss: 0.7973045110702515, discriminator loss: 1.3149147033691406 
Batch 2184, generator loss: 0.7984765768051147, discriminator loss: 1.3118016719818115 
Batch 2185, generator loss: 0.808695912361145, discriminator loss: 1.3169269561767578 
Batch 2186, generator loss: 0.8389030694961548, discriminator loss: 1.2774015665054321 
Batch 2187, generator loss: 0.863874077796936, discriminator loss: 1.357614278793335 
Batch 2188, generator loss: 0.8314573168754578, discriminator loss: 1.39697265625 
Batch 2189, generator loss: 0.7916319370269775, discriminator loss: 1.3761765956878662 
Batch 2190, generator loss: 0.7526060342788696, discriminator loss: 1.370650053024292 
Batch 2191, generator loss: 0.7313340306282043, discriminator loss: 1.3829714059829712 
Batch 2192, generator loss: 0.7847986221313477, discriminator loss: 1.4256259202957153 
Batch 2193, generator loss: 0.8329746127128601, discriminator loss: 1.4120917320251465 
Batch 2194, generator loss: 0.857305109500885, discriminator loss: 1.392641305923462 
Batch 2195, generator loss: 0.7964047789573669, discriminator loss: 1.4489915370941162 
Batch 2196, generator loss: 0.7574161887168884, discriminator loss: 1.4220455884933472 
Batch 2197, generator loss: 0.7559268474578857, discriminator loss: 1.433496356010437 
Batch 2198, generator loss: 0.759895920753479, discriminator loss: 1.4424705505371094 
Batch 2199, generator loss: 0.7523049116134644, discriminator loss: 1.5053343772888184 
Batch 2200, generator loss: 0.8143864870071411, discriminator loss: 1.4977943897247314 
Batch 2201, generator loss: 0.7918285131454468, discriminator loss: 1.484049677848816 
Batch 2202, generator loss: 0.7886941432952881, discriminator loss: 1.5198132991790771 
Batch 2203, generator loss: 0.7215437889099121, discriminator loss: 1.5154017210006714 
Batch 2204, generator loss: 0.7136563062667847, discriminator loss: 1.512061357498169 
Batch 2205, generator loss: 0.7451599836349487, discriminator loss: 1.4816324710845947 
Batch 2206, generator loss: 0.7926983833312988, discriminator loss: 1.5034751892089844 
Batch 2207, generator loss: 0.8538308143615723, discriminator loss: 1.4808781147003174 
Batch 2208, generator loss: 0.819720983505249, discriminator loss: 1.5226935148239136 
Batch 2209, generator loss: 0.76784348487854, discriminator loss: 1.4697743654251099 
Batch 2210, generator loss: 0.748424768447876, discriminator loss: 1.4693670272827148 
Batch 2211, generator loss: 0.737674355506897, discriminator loss: 1.5247081518173218 
Batch 2212, generator loss: 0.7697035670280457, discriminator loss: 1.4543451070785522 
Batch 2213, generator loss: 0.7652061581611633, discriminator loss: 1.5329794883728027 
Batch 2214, generator loss: 0.7776170969009399, discriminator loss: 1.4780843257904053 
Batch 2215, generator loss: 0.7916700839996338, discriminator loss: 1.502573847770691 
Batch 2216, generator loss: 0.8107200264930725, discriminator loss: 1.4711861610412598 
Batch 2217, generator loss: 0.7693017721176147, discriminator loss: 1.4906827211380005 
Batch 2218, generator loss: 0.795540452003479, discriminator loss: 1.4289777278900146 
Batch 2219, generator loss: 0.7616349458694458, discriminator loss: 1.4733073711395264 
Batch 2220, generator loss: 0.7865171432495117, discriminator loss: 1.4142661094665527 
Batch 2221, generator loss: 0.7988893985748291, discriminator loss: 1.4358712434768677 
Batch 2222, generator loss: 0.7967522144317627, discriminator loss: 1.4320435523986816 
Batch 2223, generator loss: 0.8091195821762085, discriminator loss: 1.414825439453125 
Batch 2224, generator loss: 0.7851826548576355, discriminator loss: 1.4287500381469727 
Batch 2225, generator loss: 0.7800260186195374, discriminator loss: 1.4038945436477661 
Batch 2226, generator loss: 0.7986494898796082, discriminator loss: 1.4021984338760376 
Batch 2227, generator loss: 0.7733525037765503, discriminator loss: 1.4001208543777466 
Batch 2228, generator loss: 0.812507152557373, discriminator loss: 1.3290770053863525 
Batch 2229, generator loss: 0.7894165515899658, discriminator loss: 1.3516499996185303 
Batch 2230, generator loss: 0.8200783729553223, discriminator loss: 1.344452977180481 
Batch 2231, generator loss: 0.8585904836654663, discriminator loss: 1.312813639640808 
Batch 2232, generator loss: 0.8341922760009766, discriminator loss: 1.3163858652114868 
Batch 2233, generator loss: 0.8470939993858337, discriminator loss: 1.3196744918823242 
Batch 2234, generator loss: 0.8635269999504089, discriminator loss: 1.2880706787109375 
Batch 2235, generator loss: 0.7982460260391235, discriminator loss: 1.281817078590393 
Batch 2236, generator loss: 0.815651535987854, discriminator loss: 1.256751298904419 
Batch 2237, generator loss: 0.817779541015625, discriminator loss: 1.2398991584777832 
Batch 2238, generator loss: 0.8467704653739929, discriminator loss: 1.2876269817352295 
Batch 2239, generator loss: 0.8595460653305054, discriminator loss: 1.238088607788086 
Batch 2240, generator loss: 0.9114759564399719, discriminator loss: 1.1599311828613281 
Batch 2241, generator loss: 0.8848753571510315, discriminator loss: 1.220624566078186 
Batch 2242, generator loss: 0.8665559887886047, discriminator loss: 1.228668212890625 
Batch 2243, generator loss: 0.8600638508796692, discriminator loss: 1.168033242225647 
Batch 2244, generator loss: 0.8491876721382141, discriminator loss: 1.1866546869277954 
Batch 2245, generator loss: 0.847571611404419, discriminator loss: 1.1457126140594482 
Batch 2246, generator loss: 0.8915709257125854, discriminator loss: 1.1675899028778076 
Batch 2247, generator loss: 0.9369134902954102, discriminator loss: 1.1267383098602295 
Batch 2248, generator loss: 0.9495408535003662, discriminator loss: 1.1778919696807861 
Batch 2249, generator loss: 0.9040997624397278, discriminator loss: 1.1393805742263794 
Batch 2250, generator loss: 0.8709484338760376, discriminator loss: 1.1413321495056152 
Batch 2251, generator loss: 0.860255241394043, discriminator loss: 1.1584594249725342 
Batch 2252, generator loss: 0.8497748374938965, discriminator loss: 1.1541905403137207 
Batch 2253, generator loss: 0.885307788848877, discriminator loss: 1.177734375 
Batch 2254, generator loss: 0.8760439157485962, discriminator loss: 1.1678428649902344 
Batch 2255, generator loss: 0.8577876091003418, discriminator loss: 1.1524298191070557 
Batch 2256, generator loss: 0.8709918260574341, discriminator loss: 1.1532429456710815 
Batch 2257, generator loss: 0.8935607075691223, discriminator loss: 1.109339714050293 
Batch 2258, generator loss: 0.9142777919769287, discriminator loss: 1.1276917457580566 
Batch 2259, generator loss: 0.9303687810897827, discriminator loss: 1.1081620454788208 
Batch 2260, generator loss: 0.8976770043373108, discriminator loss: 1.133388638496399 
Batch 2261, generator loss: 0.8751910924911499, discriminator loss: 1.1353991031646729 
Batch 2262, generator loss: 0.8666195869445801, discriminator loss: 1.161780834197998 
Batch 2263, generator loss: 0.8822643160820007, discriminator loss: 1.1019415855407715 
Batch 2264, generator loss: 0.870115339756012, discriminator loss: 1.149404764175415 
Batch 2265, generator loss: 0.9140113592147827, discriminator loss: 1.1272928714752197 
Batch 2266, generator loss: 0.8871179223060608, discriminator loss: 1.1443603038787842 
Batch 2267, generator loss: 0.8738773465156555, discriminator loss: 1.1362981796264648 
Batch 2268, generator loss: 0.8903673887252808, discriminator loss: 1.1450940370559692 
Batch 2269, generator loss: 0.9151448011398315, discriminator loss: 1.1804218292236328 
Batch 2270, generator loss: 0.8965312242507935, discriminator loss: 1.120353102684021 
Batch 2271, generator loss: 0.8902871608734131, discriminator loss: 1.1453092098236084 
Batch 2272, generator loss: 0.8850499391555786, discriminator loss: 1.1291522979736328 
Batch 2273, generator loss: 0.8801459670066833, discriminator loss: 1.1131832599639893 
Batch 2274, generator loss: 0.8944694995880127, discriminator loss: 1.1802361011505127 
Batch 2275, generator loss: 0.9235870242118835, discriminator loss: 1.0747809410095215 
Batch 2276, generator loss: 0.8983858227729797, discriminator loss: 1.1718779802322388 
Batch 2277, generator loss: 0.9058996438980103, discriminator loss: 1.1113855838775635 
Batch 2278, generator loss: 0.8833156824111938, discriminator loss: 1.1086366176605225 
Batch 2279, generator loss: 0.9261872172355652, discriminator loss: 1.1235768795013428 
Batch 2280, generator loss: 0.940978467464447, discriminator loss: 1.089503288269043 
Batch 2281, generator loss: 0.9392205476760864, discriminator loss: 1.1025724411010742 
Batch 2282, generator loss: 0.9205490946769714, discriminator loss: 1.15848708152771 
Batch 2283, generator loss: 0.892959713935852, discriminator loss: 1.1157472133636475 
Batch 2284, generator loss: 0.8907232284545898, discriminator loss: 1.1126360893249512 
Generator loss: tf.Tensor(0.8907232, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.1126361, shape=(), dtype=float32)
EPOCH: 9
Batch 2306, generator loss: 0.9571571350097656, discriminator loss: 1.0707324743270874 
Batch 2307, generator loss: 0.944776713848114, discriminator loss: 1.1695449352264404 
Batch 2308, generator loss: 0.9664216041564941, discriminator loss: 1.0732033252716064 
Batch 2309, generator loss: 0.9263885021209717, discriminator loss: 1.1072126626968384 
Batch 2310, generator loss: 0.937507152557373, discriminator loss: 1.0971760749816895 
Batch 2311, generator loss: 0.9087575674057007, discriminator loss: 1.0696598291397095 
Batch 2312, generator loss: 0.9693433046340942, discriminator loss: 1.0809587240219116 
Batch 2313, generator loss: 0.9387376308441162, discriminator loss: 1.1081547737121582 
Batch 2314, generator loss: 0.9889466166496277, discriminator loss: 1.0731313228607178 
Batch 2315, generator loss: 0.9729809761047363, discriminator loss: 1.104490041732788 
Batch 2316, generator loss: 0.9717473387718201, discriminator loss: 1.1040071249008179 
Batch 2317, generator loss: 0.9645406007766724, discriminator loss: 1.0958994626998901 
Batch 2318, generator loss: 0.9209040999412537, discriminator loss: 1.0978219509124756 
Batch 2319, generator loss: 0.9041649699211121, discriminator loss: 1.1427092552185059 
Batch 2320, generator loss: 0.9001234173774719, discriminator loss: 1.144028902053833 
Batch 2321, generator loss: 0.913175642490387, discriminator loss: 1.1032030582427979 
Batch 2322, generator loss: 0.9806487560272217, discriminator loss: 1.084937572479248 
Batch 2323, generator loss: 1.006697416305542, discriminator loss: 1.0552181005477905 
Batch 2324, generator loss: 1.00214684009552, discriminator loss: 1.1501268148422241 
Batch 2325, generator loss: 0.9709988832473755, discriminator loss: 1.1789097785949707 
Batch 2326, generator loss: 0.9766345024108887, discriminator loss: 1.142406702041626 
Batch 2327, generator loss: 0.9282680749893188, discriminator loss: 1.1711218357086182 
Batch 2328, generator loss: 0.8664303421974182, discriminator loss: 1.176924467086792 
Batch 2329, generator loss: 0.8397986888885498, discriminator loss: 1.222438931465149 
Batch 2330, generator loss: 0.921252965927124, discriminator loss: 1.1884403228759766 
Batch 2331, generator loss: 0.93204665184021, discriminator loss: 1.2083351612091064 
Batch 2332, generator loss: 0.9443066120147705, discriminator loss: 1.226801872253418 
Batch 2333, generator loss: 0.9577363729476929, discriminator loss: 1.2212787866592407 
Batch 2334, generator loss: 0.9249310493469238, discriminator loss: 1.2307156324386597 
Batch 2335, generator loss: 0.8663814663887024, discriminator loss: 1.211395502090454 
Batch 2336, generator loss: 0.8441612720489502, discriminator loss: 1.2808561325073242 
Batch 2337, generator loss: 0.854911208152771, discriminator loss: 1.3384442329406738 
Batch 2338, generator loss: 0.8410128355026245, discriminator loss: 1.314361333847046 
Batch 2339, generator loss: 0.8780028820037842, discriminator loss: 1.2876288890838623 
Batch 2340, generator loss: 0.8946470618247986, discriminator loss: 1.3140714168548584 
Batch 2341, generator loss: 0.8859177827835083, discriminator loss: 1.307826042175293 
Batch 2342, generator loss: 0.8819708824157715, discriminator loss: 1.3159399032592773 
Batch 2343, generator loss: 0.819909393787384, discriminator loss: 1.3984532356262207 
Batch 2344, generator loss: 0.843833327293396, discriminator loss: 1.3814973831176758 
Batch 2345, generator loss: 0.8222300410270691, discriminator loss: 1.3574684858322144 
Batch 2346, generator loss: 0.8159373998641968, discriminator loss: 1.3167824745178223 
Batch 2347, generator loss: 0.8268281817436218, discriminator loss: 1.3551177978515625 
Batch 2348, generator loss: 0.8509881496429443, discriminator loss: 1.3775238990783691 
Batch 2349, generator loss: 0.8670507669448853, discriminator loss: 1.3914482593536377 
Batch 2350, generator loss: 0.8567596673965454, discriminator loss: 1.4405827522277832 
Batch 2351, generator loss: 0.8160479068756104, discriminator loss: 1.4124759435653687 
Batch 2352, generator loss: 0.7875968813896179, discriminator loss: 1.418088436126709 
Batch 2353, generator loss: 0.7648488283157349, discriminator loss: 1.3923296928405762 
Batch 2354, generator loss: 0.7720293998718262, discriminator loss: 1.4285364151000977 
Batch 2355, generator loss: 0.8218935132026672, discriminator loss: 1.4685221910476685 
Batch 2356, generator loss: 0.8729143738746643, discriminator loss: 1.3586373329162598 
Batch 2357, generator loss: 0.8663146495819092, discriminator loss: 1.46665358543396 
Batch 2358, generator loss: 0.7987246513366699, discriminator loss: 1.427873134613037 
Batch 2359, generator loss: 0.7908929586410522, discriminator loss: 1.446434497833252 
Batch 2360, generator loss: 0.7523181438446045, discriminator loss: 1.4591972827911377 
Batch 2361, generator loss: 0.7688573002815247, discriminator loss: 1.4899334907531738 
Batch 2362, generator loss: 0.7593587040901184, discriminator loss: 1.4308865070343018 
Batch 2363, generator loss: 0.7977760434150696, discriminator loss: 1.4307591915130615 
Batch 2364, generator loss: 0.8272527456283569, discriminator loss: 1.3914995193481445 
Batch 2365, generator loss: 0.8597195744514465, discriminator loss: 1.4217114448547363 
Batch 2366, generator loss: 0.8407031297683716, discriminator loss: 1.4207966327667236 
Batch 2367, generator loss: 0.8282681703567505, discriminator loss: 1.382206678390503 
Batch 2368, generator loss: 0.7439218759536743, discriminator loss: 1.410381555557251 
Batch 2369, generator loss: 0.7402992248535156, discriminator loss: 1.3940327167510986 
Batch 2370, generator loss: 0.8045379519462585, discriminator loss: 1.4242548942565918 
Batch 2371, generator loss: 0.8165100812911987, discriminator loss: 1.385880947113037 
Batch 2372, generator loss: 0.825334906578064, discriminator loss: 1.3976056575775146 
Batch 2373, generator loss: 0.8272795677185059, discriminator loss: 1.3849860429763794 
Batch 2374, generator loss: 0.8321430683135986, discriminator loss: 1.345611572265625 
Batch 2375, generator loss: 0.7849063873291016, discriminator loss: 1.3818408250808716 
Batch 2376, generator loss: 0.7940345406532288, discriminator loss: 1.372823715209961 
Batch 2377, generator loss: 0.8168606758117676, discriminator loss: 1.3487780094146729 
Batch 2378, generator loss: 0.8286113739013672, discriminator loss: 1.3457947969436646 
Batch 2379, generator loss: 0.8527830243110657, discriminator loss: 1.3389838933944702 
Batch 2380, generator loss: 0.8122969269752502, discriminator loss: 1.3280531167984009 
Batch 2381, generator loss: 0.841705322265625, discriminator loss: 1.306955337524414 
Batch 2382, generator loss: 0.8260877132415771, discriminator loss: 1.2959901094436646 
Batch 2383, generator loss: 0.8451476097106934, discriminator loss: 1.2685890197753906 
Batch 2384, generator loss: 0.80851149559021, discriminator loss: 1.2909702062606812 
Batch 2385, generator loss: 0.8431586027145386, discriminator loss: 1.2764232158660889 
Batch 2386, generator loss: 0.8203005790710449, discriminator loss: 1.2730435132980347 
Batch 2387, generator loss: 0.8940150141716003, discriminator loss: 1.2247638702392578 
Batch 2388, generator loss: 0.872706413269043, discriminator loss: 1.2328908443450928 
Batch 2389, generator loss: 0.8736595511436462, discriminator loss: 1.2083818912506104 
Batch 2390, generator loss: 0.8695228695869446, discriminator loss: 1.2116156816482544 
Batch 2391, generator loss: 0.8719722032546997, discriminator loss: 1.2222462892532349 
Batch 2392, generator loss: 0.8950655460357666, discriminator loss: 1.1374849081039429 
Batch 2393, generator loss: 0.8855082988739014, discriminator loss: 1.1717878580093384 
Batch 2394, generator loss: 0.9061989784240723, discriminator loss: 1.1482036113739014 
Batch 2395, generator loss: 0.928455114364624, discriminator loss: 1.0906933546066284 
Batch 2396, generator loss: 0.9113563895225525, discriminator loss: 1.145388126373291 
Batch 2397, generator loss: 0.9332842826843262, discriminator loss: 1.1296683549880981 
Batch 2398, generator loss: 0.9645692706108093, discriminator loss: 1.0804808139801025 
Batch 2399, generator loss: 0.9761710166931152, discriminator loss: 1.0823376178741455 
Batch 2400, generator loss: 0.9540044069290161, discriminator loss: 1.0955419540405273 
Batch 2401, generator loss: 0.9052039384841919, discriminator loss: 1.1055796146392822 
Batch 2402, generator loss: 0.9014911651611328, discriminator loss: 1.0741149187088013 
Batch 2403, generator loss: 0.9541683197021484, discriminator loss: 1.0540515184402466 
Batch 2404, generator loss: 0.9479735493659973, discriminator loss: 1.073152780532837 
Batch 2405, generator loss: 1.0079690217971802, discriminator loss: 1.0437694787979126 
Batch 2406, generator loss: 0.9921039938926697, discriminator loss: 1.10409677028656 
Batch 2407, generator loss: 0.9614291191101074, discriminator loss: 1.085662603378296 
Batch 2408, generator loss: 0.9579763412475586, discriminator loss: 1.0310429334640503 
Batch 2409, generator loss: 0.9389678239822388, discriminator loss: 1.0638961791992188 
Batch 2410, generator loss: 0.9530981779098511, discriminator loss: 1.0341876745224 
Batch 2411, generator loss: 0.9514285326004028, discriminator loss: 1.06563138961792 
Batch 2412, generator loss: 0.9597476720809937, discriminator loss: 1.0310781002044678 
Batch 2413, generator loss: 0.9972052574157715, discriminator loss: 0.9857410192489624 
Batch 2414, generator loss: 0.9897528290748596, discriminator loss: 1.0323686599731445 
Batch 2415, generator loss: 1.0715148448944092, discriminator loss: 1.0272839069366455 
Batch 2416, generator loss: 1.0568881034851074, discriminator loss: 1.0451298952102661 
Batch 2417, generator loss: 1.0176012516021729, discriminator loss: 1.0371615886688232 
Batch 2418, generator loss: 0.9774876832962036, discriminator loss: 0.9979338645935059 
Batch 2419, generator loss: 0.9367554187774658, discriminator loss: 0.9885666370391846 
Batch 2420, generator loss: 0.9654159545898438, discriminator loss: 1.0048185586929321 
Batch 2421, generator loss: 1.0159759521484375, discriminator loss: 0.9883475303649902 
Batch 2422, generator loss: 1.058514952659607, discriminator loss: 0.992713212966919 
Batch 2423, generator loss: 1.0682706832885742, discriminator loss: 0.9891084432601929 
Batch 2424, generator loss: 1.066448450088501, discriminator loss: 0.9478462338447571 
Batch 2425, generator loss: 1.059328317642212, discriminator loss: 0.9955084323883057 
Batch 2426, generator loss: 1.0621337890625, discriminator loss: 0.9506725668907166 
Batch 2427, generator loss: 1.0728644132614136, discriminator loss: 0.9627366662025452 
Batch 2428, generator loss: 1.027037501335144, discriminator loss: 0.9358742237091064 
Batch 2429, generator loss: 1.0327181816101074, discriminator loss: 0.9533395767211914 
Batch 2430, generator loss: 1.0377882719039917, discriminator loss: 0.9576130509376526 
Batch 2431, generator loss: 1.0739290714263916, discriminator loss: 0.953027606010437 
Batch 2432, generator loss: 1.083027720451355, discriminator loss: 0.94925457239151 
Batch 2433, generator loss: 1.0890023708343506, discriminator loss: 0.9531649351119995 
Batch 2434, generator loss: 1.1182818412780762, discriminator loss: 0.9280743598937988 
Batch 2435, generator loss: 1.1119184494018555, discriminator loss: 0.9451829195022583 
Batch 2436, generator loss: 1.0348505973815918, discriminator loss: 0.9786617755889893 
Batch 2437, generator loss: 1.076087236404419, discriminator loss: 0.9400681853294373 
Batch 2438, generator loss: 1.0603071451187134, discriminator loss: 0.9348766207695007 
Batch 2439, generator loss: 1.1005258560180664, discriminator loss: 0.92701256275177 
Batch 2440, generator loss: 1.0638914108276367, discriminator loss: 0.9524236917495728 
Batch 2441, generator loss: 1.1154084205627441, discriminator loss: 0.9419856071472168 
Batch 2442, generator loss: 1.1120022535324097, discriminator loss: 0.9412268400192261 
Batch 2443, generator loss: 1.0997889041900635, discriminator loss: 0.9438600540161133 
Batch 2444, generator loss: 1.1231002807617188, discriminator loss: 0.9388917088508606 
Batch 2445, generator loss: 1.0788848400115967, discriminator loss: 0.9069991707801819 
Batch 2446, generator loss: 1.075586199760437, discriminator loss: 0.9756080508232117 
Batch 2447, generator loss: 1.0863620042800903, discriminator loss: 0.9321455955505371 
Batch 2448, generator loss: 1.069051742553711, discriminator loss: 0.9895426034927368 
Batch 2449, generator loss: 1.0718977451324463, discriminator loss: 0.9921253323554993 
Batch 2450, generator loss: 1.0637861490249634, discriminator loss: 0.9656055569648743 
Batch 2451, generator loss: 1.1279432773590088, discriminator loss: 0.9375912547111511 
Batch 2452, generator loss: 1.068620204925537, discriminator loss: 1.0016465187072754 
Batch 2453, generator loss: 1.0346693992614746, discriminator loss: 0.9905281662940979 
Batch 2454, generator loss: 1.0708181858062744, discriminator loss: 1.0024785995483398 
Batch 2455, generator loss: 0.9972344636917114, discriminator loss: 1.0296814441680908 
Batch 2456, generator loss: 1.0823575258255005, discriminator loss: 1.0330071449279785 
Batch 2457, generator loss: 1.0771819353103638, discriminator loss: 1.0129892826080322 
Batch 2458, generator loss: 1.0242393016815186, discriminator loss: 1.0426853895187378 
Batch 2459, generator loss: 1.0294004678726196, discriminator loss: 1.0518028736114502 
Batch 2460, generator loss: 0.9519896507263184, discriminator loss: 1.145202875137329 
Batch 2461, generator loss: 0.9717330932617188, discriminator loss: 1.080674409866333 
Batch 2462, generator loss: 0.9468311071395874, discriminator loss: 1.110349416732788 
Batch 2463, generator loss: 0.9800044894218445, discriminator loss: 1.144702672958374 
Batch 2464, generator loss: 0.9653581976890564, discriminator loss: 1.145178198814392 
Batch 2465, generator loss: 1.0052073001861572, discriminator loss: 1.1615833044052124 
Batch 2466, generator loss: 0.9315701723098755, discriminator loss: 1.231750726699829 
Batch 2467, generator loss: 0.8668105602264404, discriminator loss: 1.2157180309295654 
Batch 2468, generator loss: 0.8405013680458069, discriminator loss: 1.2521038055419922 
Batch 2469, generator loss: 0.8516019582748413, discriminator loss: 1.3058720827102661 
Batch 2470, generator loss: 0.8889917135238647, discriminator loss: 1.2717769145965576 
Batch 2471, generator loss: 0.9086977243423462, discriminator loss: 1.3259875774383545 
Batch 2472, generator loss: 0.910696268081665, discriminator loss: 1.3482043743133545 
Batch 2473, generator loss: 0.8322768211364746, discriminator loss: 1.3930968046188354 
Batch 2474, generator loss: 0.824609637260437, discriminator loss: 1.3383570909500122 
Batch 2475, generator loss: 0.7656798362731934, discriminator loss: 1.4322161674499512 
Batch 2476, generator loss: 0.8288751840591431, discriminator loss: 1.4219204187393188 
Batch 2477, generator loss: 0.7467864751815796, discriminator loss: 1.4918489456176758 
Batch 2478, generator loss: 0.8049320578575134, discriminator loss: 1.4158449172973633 
Batch 2479, generator loss: 0.7963434457778931, discriminator loss: 1.515934944152832 
Batch 2480, generator loss: 0.8192378282546997, discriminator loss: 1.4588384628295898 
Batch 2481, generator loss: 0.7629332542419434, discriminator loss: 1.5202102661132812 
Batch 2482, generator loss: 0.7275439500808716, discriminator loss: 1.5373421907424927 
Batch 2483, generator loss: 0.7314591407775879, discriminator loss: 1.5570012331008911 
Batch 2484, generator loss: 0.6955285668373108, discriminator loss: 1.6191692352294922 
Batch 2485, generator loss: 0.7141726613044739, discriminator loss: 1.5718612670898438 
Batch 2486, generator loss: 0.7284188270568848, discriminator loss: 1.5171945095062256 
Batch 2487, generator loss: 0.6982806921005249, discriminator loss: 1.6032748222351074 
Batch 2488, generator loss: 0.7210947871208191, discriminator loss: 1.5826566219329834 
Batch 2489, generator loss: 0.7264540195465088, discriminator loss: 1.574033498764038 
Batch 2490, generator loss: 0.7262734174728394, discriminator loss: 1.5789631605148315 
Batch 2491, generator loss: 0.6970552802085876, discriminator loss: 1.6050703525543213 
Batch 2492, generator loss: 0.7239129543304443, discriminator loss: 1.5164084434509277 
Batch 2493, generator loss: 0.7059742212295532, discriminator loss: 1.577184796333313 
Batch 2494, generator loss: 0.7048712968826294, discriminator loss: 1.5088261365890503 
Batch 2495, generator loss: 0.7185684442520142, discriminator loss: 1.5808544158935547 
Batch 2496, generator loss: 0.7227344512939453, discriminator loss: 1.5915164947509766 
Batch 2497, generator loss: 0.7024548649787903, discriminator loss: 1.4945404529571533 
Batch 2498, generator loss: 0.7413130402565002, discriminator loss: 1.460040807723999 
Batch 2499, generator loss: 0.7578414678573608, discriminator loss: 1.4740629196166992 
Batch 2500, generator loss: 0.7159805297851562, discriminator loss: 1.4906349182128906 
Batch 2501, generator loss: 0.7620754837989807, discriminator loss: 1.4443665742874146 
Batch 2502, generator loss: 0.7570846080780029, discriminator loss: 1.4211742877960205 
Batch 2503, generator loss: 0.754723310470581, discriminator loss: 1.413482427597046 
Batch 2504, generator loss: 0.7642349600791931, discriminator loss: 1.395037293434143 
Batch 2505, generator loss: 0.7604755759239197, discriminator loss: 1.3931176662445068 
Batch 2506, generator loss: 0.7880144119262695, discriminator loss: 1.3999483585357666 
Batch 2507, generator loss: 0.7854353189468384, discriminator loss: 1.3117306232452393 
Batch 2508, generator loss: 0.7648184299468994, discriminator loss: 1.2943180799484253 
Batch 2509, generator loss: 0.7738221287727356, discriminator loss: 1.31753408908844 
Batch 2510, generator loss: 0.8466869592666626, discriminator loss: 1.3237314224243164 
Batch 2511, generator loss: 0.8474316596984863, discriminator loss: 1.315309762954712 
Batch 2512, generator loss: 0.8474524617195129, discriminator loss: 1.2738091945648193 
Batch 2513, generator loss: 0.8220781087875366, discriminator loss: 1.250641107559204 
Batch 2514, generator loss: 0.7941656112670898, discriminator loss: 1.2335660457611084 
Batch 2515, generator loss: 0.7956887483596802, discriminator loss: 1.2150874137878418 
Batch 2516, generator loss: 0.8273707628250122, discriminator loss: 1.2143406867980957 
Batch 2517, generator loss: 0.8820362091064453, discriminator loss: 1.2188105583190918 
Batch 2518, generator loss: 0.9150184392929077, discriminator loss: 1.1979897022247314 
Batch 2519, generator loss: 0.9021992087364197, discriminator loss: 1.1922930479049683 
Batch 2520, generator loss: 0.8847575187683105, discriminator loss: 1.1489462852478027 
Batch 2521, generator loss: 0.8797821402549744, discriminator loss: 1.1298201084136963 
Batch 2522, generator loss: 0.8875518441200256, discriminator loss: 1.1297450065612793 
Batch 2523, generator loss: 0.8511068820953369, discriminator loss: 1.1546590328216553 
Batch 2524, generator loss: 0.909223735332489, discriminator loss: 1.1080846786499023 
Batch 2525, generator loss: 0.9433311223983765, discriminator loss: 1.1383123397827148 
Batch 2526, generator loss: 0.9456800222396851, discriminator loss: 1.1002836227416992 
Batch 2527, generator loss: 0.9086953401565552, discriminator loss: 1.1037195920944214 
Batch 2528, generator loss: 0.8890378475189209, discriminator loss: 1.1387916803359985 
Batch 2529, generator loss: 0.8887339234352112, discriminator loss: 1.091681718826294 
Batch 2530, generator loss: 0.9160811305046082, discriminator loss: 1.1075382232666016 
Batch 2531, generator loss: 0.9112727046012878, discriminator loss: 1.0886781215667725 
Batch 2532, generator loss: 0.9205180406570435, discriminator loss: 1.0952672958374023 
Batch 2533, generator loss: 0.928894579410553, discriminator loss: 1.08469820022583 
Batch 2534, generator loss: 0.9611232876777649, discriminator loss: 1.0819621086120605 
Batch 2535, generator loss: 0.9274448752403259, discriminator loss: 1.082783818244934 
Batch 2536, generator loss: 0.9338541030883789, discriminator loss: 1.092099905014038 
Batch 2537, generator loss: 0.9097096920013428, discriminator loss: 1.0784003734588623 
Batch 2538, generator loss: 0.8810601234436035, discriminator loss: 1.1229465007781982 
Batch 2539, generator loss: 0.9118096828460693, discriminator loss: 1.0827889442443848 
Batch 2540, generator loss: 0.9109870195388794, discriminator loss: 1.1020745038986206 
Generator loss: tf.Tensor(0.910987, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.1020745, shape=(), dtype=float32)
EPOCH: 10
Batch 2562, generator loss: 0.9419658184051514, discriminator loss: 1.1000497341156006 
Batch 2563, generator loss: 0.8976585268974304, discriminator loss: 1.1170992851257324 
Batch 2564, generator loss: 0.9082098007202148, discriminator loss: 1.1075860261917114 
Batch 2565, generator loss: 0.9347402453422546, discriminator loss: 1.1210873126983643 
Batch 2566, generator loss: 0.9118194580078125, discriminator loss: 1.0954363346099854 
Batch 2567, generator loss: 0.9045482873916626, discriminator loss: 1.1001923084259033 
Batch 2568, generator loss: 0.8886793255805969, discriminator loss: 1.0962616205215454 
Batch 2569, generator loss: 0.9534375667572021, discriminator loss: 1.0656369924545288 
Batch 2570, generator loss: 0.9630798101425171, discriminator loss: 1.118089199066162 
Batch 2571, generator loss: 0.9373776912689209, discriminator loss: 1.0945165157318115 
Batch 2572, generator loss: 0.9682916402816772, discriminator loss: 1.051039218902588 
Batch 2573, generator loss: 0.9306501150131226, discriminator loss: 1.0861339569091797 
Batch 2574, generator loss: 0.9327355623245239, discriminator loss: 1.0631945133209229 
Batch 2575, generator loss: 0.9990003108978271, discriminator loss: 1.0361084938049316 
Batch 2576, generator loss: 0.994295060634613, discriminator loss: 1.0377545356750488 
Batch 2577, generator loss: 1.0448744297027588, discriminator loss: 1.0440928936004639 
Batch 2578, generator loss: 1.0102287530899048, discriminator loss: 1.0132309198379517 
Batch 2579, generator loss: 1.0172200202941895, discriminator loss: 0.9919557571411133 
Batch 2580, generator loss: 1.016289234161377, discriminator loss: 1.010704517364502 
Batch 2581, generator loss: 1.0569052696228027, discriminator loss: 1.0096001625061035 
Batch 2582, generator loss: 1.0573928356170654, discriminator loss: 1.012779951095581 
Batch 2583, generator loss: 1.0425238609313965, discriminator loss: 1.0398750305175781 
Batch 2584, generator loss: 1.0590522289276123, discriminator loss: 0.9619754552841187 
Batch 2585, generator loss: 1.0439252853393555, discriminator loss: 0.9723978638648987 
Batch 2586, generator loss: 1.013422966003418, discriminator loss: 1.043444037437439 
Batch 2587, generator loss: 1.064607858657837, discriminator loss: 0.9714113473892212 
Batch 2588, generator loss: 1.0614559650421143, discriminator loss: 1.0226057767868042 
Batch 2589, generator loss: 1.1005847454071045, discriminator loss: 1.0092893838882446 
Batch 2590, generator loss: 1.0641930103302002, discriminator loss: 1.0225872993469238 
Batch 2591, generator loss: 1.0953097343444824, discriminator loss: 1.0324783325195312 
Batch 2592, generator loss: 1.0341168642044067, discriminator loss: 1.0600974559783936 
Batch 2593, generator loss: 1.0440127849578857, discriminator loss: 1.0240259170532227 
Batch 2594, generator loss: 1.0243558883666992, discriminator loss: 1.0598969459533691 
Batch 2595, generator loss: 1.1297376155853271, discriminator loss: 1.0755195617675781 
Batch 2596, generator loss: 1.0915672779083252, discriminator loss: 1.1039934158325195 
Batch 2597, generator loss: 1.0441681146621704, discriminator loss: 1.0441091060638428 
Batch 2598, generator loss: 1.0495706796646118, discriminator loss: 1.0810887813568115 
Batch 2599, generator loss: 1.0718474388122559, discriminator loss: 1.1339168548583984 
Batch 2600, generator loss: 1.0355294942855835, discriminator loss: 1.114079475402832 
Batch 2601, generator loss: 1.0649926662445068, discriminator loss: 1.1372933387756348 
Batch 2602, generator loss: 1.0001698732376099, discriminator loss: 1.1590039730072021 
Batch 2603, generator loss: 0.9658885598182678, discriminator loss: 1.2286639213562012 
Batch 2604, generator loss: 0.9700857400894165, discriminator loss: 1.1935924291610718 
Batch 2605, generator loss: 0.9768083691596985, discriminator loss: 1.205102562904358 
Batch 2606, generator loss: 0.9663048982620239, discriminator loss: 1.291675090789795 
Batch 2607, generator loss: 1.0068690776824951, discriminator loss: 1.2660105228424072 
Batch 2608, generator loss: 0.9499114751815796, discriminator loss: 1.3727726936340332 
Batch 2609, generator loss: 0.9060770273208618, discriminator loss: 1.2907012701034546 
Batch 2610, generator loss: 0.900344967842102, discriminator loss: 1.3325207233428955 
Batch 2611, generator loss: 0.891042947769165, discriminator loss: 1.360724687576294 
Batch 2612, generator loss: 0.9236479997634888, discriminator loss: 1.3955929279327393 
Batch 2613, generator loss: 0.9491689205169678, discriminator loss: 1.3305861949920654 
Batch 2614, generator loss: 0.8557921648025513, discriminator loss: 1.4113292694091797 
Batch 2615, generator loss: 0.821617603302002, discriminator loss: 1.4119890928268433 
Batch 2616, generator loss: 0.810958981513977, discriminator loss: 1.4746477603912354 
Batch 2617, generator loss: 0.8157662153244019, discriminator loss: 1.4907584190368652 
Batch 2618, generator loss: 0.8093113303184509, discriminator loss: 1.4918056726455688 
Batch 2619, generator loss: 0.8223069310188293, discriminator loss: 1.573630690574646 
Batch 2620, generator loss: 0.7967051267623901, discriminator loss: 1.5781400203704834 
Batch 2621, generator loss: 0.7815936803817749, discriminator loss: 1.5614598989486694 
Batch 2622, generator loss: 0.7142884135246277, discriminator loss: 1.5589699745178223 
Batch 2623, generator loss: 0.7731163501739502, discriminator loss: 1.5177183151245117 
Batch 2624, generator loss: 0.7980517148971558, discriminator loss: 1.533151388168335 
Batch 2625, generator loss: 0.7236833572387695, discriminator loss: 1.5464367866516113 
Batch 2626, generator loss: 0.7475587129592896, discriminator loss: 1.6094375848770142 
Batch 2627, generator loss: 0.7432196140289307, discriminator loss: 1.6163814067840576 
Batch 2628, generator loss: 0.7215327024459839, discriminator loss: 1.6347806453704834 
Batch 2629, generator loss: 0.6890177130699158, discriminator loss: 1.5550801753997803 
Batch 2630, generator loss: 0.6652376651763916, discriminator loss: 1.6217877864837646 
Batch 2631, generator loss: 0.6722556352615356, discriminator loss: 1.6038498878479004 
Batch 2632, generator loss: 0.7169129848480225, discriminator loss: 1.6657004356384277 
Batch 2633, generator loss: 0.7443645000457764, discriminator loss: 1.6004048585891724 
Batch 2634, generator loss: 0.7236294746398926, discriminator loss: 1.583152174949646 
Batch 2635, generator loss: 0.7082874774932861, discriminator loss: 1.6186816692352295 
Batch 2636, generator loss: 0.6680269241333008, discriminator loss: 1.601252555847168 
Batch 2637, generator loss: 0.6390787363052368, discriminator loss: 1.6170324087142944 
Batch 2638, generator loss: 0.6777113676071167, discriminator loss: 1.5419988632202148 
Batch 2639, generator loss: 0.7029844522476196, discriminator loss: 1.586356282234192 
Batch 2640, generator loss: 0.7519235610961914, discriminator loss: 1.5048537254333496 
Batch 2641, generator loss: 0.7357960939407349, discriminator loss: 1.489345908164978 
Batch 2642, generator loss: 0.7431821823120117, discriminator loss: 1.4987736940383911 
Batch 2643, generator loss: 0.6985493898391724, discriminator loss: 1.4734458923339844 
Batch 2644, generator loss: 0.698339581489563, discriminator loss: 1.43241286277771 
Batch 2645, generator loss: 0.7309086918830872, discriminator loss: 1.48643159866333 
Batch 2646, generator loss: 0.768680989742279, discriminator loss: 1.4137985706329346 
Batch 2647, generator loss: 0.7659541368484497, discriminator loss: 1.446033239364624 
Batch 2648, generator loss: 0.7515762448310852, discriminator loss: 1.3730864524841309 
Batch 2649, generator loss: 0.7409335374832153, discriminator loss: 1.3577905893325806 
Batch 2650, generator loss: 0.7809920310974121, discriminator loss: 1.3344550132751465 
Batch 2651, generator loss: 0.7771998643875122, discriminator loss: 1.3083546161651611 
Batch 2652, generator loss: 0.7967533469200134, discriminator loss: 1.3226217031478882 
Batch 2653, generator loss: 0.7947120070457458, discriminator loss: 1.3272511959075928 
Batch 2654, generator loss: 0.7970274686813354, discriminator loss: 1.2830957174301147 
Batch 2655, generator loss: 0.8206460475921631, discriminator loss: 1.2622319459915161 
Batch 2656, generator loss: 0.8094353675842285, discriminator loss: 1.290590763092041 
Batch 2657, generator loss: 0.8085013628005981, discriminator loss: 1.2687478065490723 
Batch 2658, generator loss: 0.8362232446670532, discriminator loss: 1.2474088668823242 
Batch 2659, generator loss: 0.8510370254516602, discriminator loss: 1.2151875495910645 
Batch 2660, generator loss: 0.8357163667678833, discriminator loss: 1.22071373462677 
Batch 2661, generator loss: 0.8682811260223389, discriminator loss: 1.1756491661071777 
Batch 2662, generator loss: 0.8938173055648804, discriminator loss: 1.1751430034637451 
Batch 2663, generator loss: 0.9052878618240356, discriminator loss: 1.1720612049102783 
Batch 2664, generator loss: 0.8548725843429565, discriminator loss: 1.2156946659088135 
Batch 2665, generator loss: 0.8630626797676086, discriminator loss: 1.1344481706619263 
Batch 2666, generator loss: 0.8387760519981384, discriminator loss: 1.1597496271133423 
Batch 2667, generator loss: 0.8734818696975708, discriminator loss: 1.1928246021270752 
Batch 2668, generator loss: 0.943254828453064, discriminator loss: 1.125123381614685 
Batch 2669, generator loss: 0.9378104209899902, discriminator loss: 1.1486942768096924 
Batch 2670, generator loss: 0.9142292141914368, discriminator loss: 1.118520736694336 
Batch 2671, generator loss: 0.9257906675338745, discriminator loss: 1.1604681015014648 
Batch 2672, generator loss: 0.8948981761932373, discriminator loss: 1.1517335176467896 
Batch 2673, generator loss: 0.9138017892837524, discriminator loss: 1.102905035018921 
Batch 2674, generator loss: 0.9002863168716431, discriminator loss: 1.127955675125122 
Batch 2675, generator loss: 0.9005337953567505, discriminator loss: 1.0923736095428467 
Batch 2676, generator loss: 0.9212484359741211, discriminator loss: 1.1343803405761719 
Batch 2677, generator loss: 0.9571608304977417, discriminator loss: 1.1043341159820557 
Batch 2678, generator loss: 1.0001245737075806, discriminator loss: 1.1214895248413086 
Batch 2679, generator loss: 0.9703829288482666, discriminator loss: 1.0909072160720825 
Batch 2680, generator loss: 0.9337786436080933, discriminator loss: 1.1050350666046143 
Batch 2681, generator loss: 0.8719123005867004, discriminator loss: 1.1274232864379883 
Batch 2682, generator loss: 0.8941174149513245, discriminator loss: 1.130750060081482 
Batch 2683, generator loss: 0.8807907104492188, discriminator loss: 1.1574327945709229 
Batch 2684, generator loss: 0.9554506540298462, discriminator loss: 1.1273562908172607 
Batch 2685, generator loss: 1.0128202438354492, discriminator loss: 1.0803396701812744 
Batch 2686, generator loss: 0.9903314709663391, discriminator loss: 1.1102724075317383 
Batch 2687, generator loss: 0.9297686815261841, discriminator loss: 1.1641552448272705 
Batch 2688, generator loss: 0.8874516487121582, discriminator loss: 1.1572376489639282 
Batch 2689, generator loss: 0.9236646890640259, discriminator loss: 1.1074001789093018 
Batch 2690, generator loss: 0.894889235496521, discriminator loss: 1.1246534585952759 
Batch 2691, generator loss: 0.9256653189659119, discriminator loss: 1.1440924406051636 
Batch 2692, generator loss: 0.9701015949249268, discriminator loss: 1.144176959991455 
Batch 2693, generator loss: 0.9589093923568726, discriminator loss: 1.1724908351898193 
Batch 2694, generator loss: 0.9604681134223938, discriminator loss: 1.1139805316925049 
Batch 2695, generator loss: 0.9217153787612915, discriminator loss: 1.1816446781158447 
Batch 2696, generator loss: 0.87010258436203, discriminator loss: 1.1960724592208862 
Batch 2697, generator loss: 0.9120201468467712, discriminator loss: 1.1376549005508423 
Batch 2698, generator loss: 0.9575282335281372, discriminator loss: 1.1437392234802246 
Batch 2699, generator loss: 0.9488729238510132, discriminator loss: 1.1705777645111084 
Batch 2700, generator loss: 0.9435296058654785, discriminator loss: 1.155242681503296 
Batch 2701, generator loss: 0.9577361345291138, discriminator loss: 1.1459296941757202 
Batch 2702, generator loss: 0.9284509420394897, discriminator loss: 1.1563743352890015 
Batch 2703, generator loss: 0.9293725490570068, discriminator loss: 1.1967248916625977 
Batch 2704, generator loss: 0.905852198600769, discriminator loss: 1.2110339403152466 
Batch 2705, generator loss: 0.9119716882705688, discriminator loss: 1.2135246992111206 
Batch 2706, generator loss: 0.889409601688385, discriminator loss: 1.2120542526245117 
Batch 2707, generator loss: 0.872657299041748, discriminator loss: 1.2119040489196777 
Batch 2708, generator loss: 0.8912053108215332, discriminator loss: 1.280624508857727 
Batch 2709, generator loss: 0.8672075271606445, discriminator loss: 1.2727323770523071 
Batch 2710, generator loss: 0.8818231821060181, discriminator loss: 1.242497205734253 
Batch 2711, generator loss: 0.8780776858329773, discriminator loss: 1.2315783500671387 
Batch 2712, generator loss: 0.909939706325531, discriminator loss: 1.2689961194992065 
Batch 2713, generator loss: 0.8757976293563843, discriminator loss: 1.2607804536819458 
Batch 2714, generator loss: 0.84637850522995, discriminator loss: 1.2833540439605713 
Batch 2715, generator loss: 0.8284977674484253, discriminator loss: 1.2553824186325073 
Batch 2716, generator loss: 0.8444229364395142, discriminator loss: 1.3033450841903687 
Batch 2717, generator loss: 0.8523181676864624, discriminator loss: 1.30313241481781 
Batch 2718, generator loss: 0.8804636597633362, discriminator loss: 1.318871021270752 
Batch 2719, generator loss: 0.8569085597991943, discriminator loss: 1.4102720022201538 
Batch 2720, generator loss: 0.8694339990615845, discriminator loss: 1.324484944343567 
Batch 2721, generator loss: 0.8168946504592896, discriminator loss: 1.3388166427612305 
Batch 2722, generator loss: 0.8003103137016296, discriminator loss: 1.382735013961792 
Batch 2723, generator loss: 0.8460693359375, discriminator loss: 1.3669564723968506 
Batch 2724, generator loss: 0.8406102061271667, discriminator loss: 1.3204952478408813 
Batch 2725, generator loss: 0.8350651860237122, discriminator loss: 1.4009525775909424 
Batch 2726, generator loss: 0.7907928228378296, discriminator loss: 1.4194492101669312 
Batch 2727, generator loss: 0.7918764352798462, discriminator loss: 1.4570651054382324 
Batch 2728, generator loss: 0.8044323921203613, discriminator loss: 1.4381763935089111 
Batch 2729, generator loss: 0.7300577163696289, discriminator loss: 1.5208485126495361 
Batch 2730, generator loss: 0.8123003244400024, discriminator loss: 1.4437041282653809 
Batch 2731, generator loss: 0.7769047021865845, discriminator loss: 1.426642894744873 
Batch 2732, generator loss: 0.7488580346107483, discriminator loss: 1.475476622581482 
Batch 2733, generator loss: 0.7600284218788147, discriminator loss: 1.51235830783844 
Batch 2734, generator loss: 0.7499849796295166, discriminator loss: 1.5158967971801758 
Batch 2735, generator loss: 0.7798031568527222, discriminator loss: 1.4612027406692505 
Batch 2736, generator loss: 0.797217845916748, discriminator loss: 1.4993581771850586 
Batch 2737, generator loss: 0.7842428088188171, discriminator loss: 1.5343097448349 
Batch 2738, generator loss: 0.7210564613342285, discriminator loss: 1.5157711505889893 
Batch 2739, generator loss: 0.73591548204422, discriminator loss: 1.5588951110839844 
Batch 2740, generator loss: 0.730137050151825, discriminator loss: 1.5548431873321533 
Batch 2741, generator loss: 0.7382149696350098, discriminator loss: 1.5239042043685913 
Batch 2742, generator loss: 0.7324310541152954, discriminator loss: 1.5527126789093018 
Batch 2743, generator loss: 0.7490710020065308, discriminator loss: 1.5919336080551147 
Batch 2744, generator loss: 0.7256675958633423, discriminator loss: 1.5586868524551392 
Batch 2745, generator loss: 0.7192649841308594, discriminator loss: 1.584195613861084 
Batch 2746, generator loss: 0.7028865814208984, discriminator loss: 1.563449501991272 
Batch 2747, generator loss: 0.701148509979248, discriminator loss: 1.5950758457183838 
Batch 2748, generator loss: 0.7126602530479431, discriminator loss: 1.5445888042449951 
Batch 2749, generator loss: 0.697021484375, discriminator loss: 1.560021162033081 
Batch 2750, generator loss: 0.7834766507148743, discriminator loss: 1.4891504049301147 
Batch 2751, generator loss: 0.7715127468109131, discriminator loss: 1.5646066665649414 
Batch 2752, generator loss: 0.7459349036216736, discriminator loss: 1.5749908685684204 
Batch 2753, generator loss: 0.7446447610855103, discriminator loss: 1.5613301992416382 
Batch 2754, generator loss: 0.6821773052215576, discriminator loss: 1.544569730758667 
Batch 2755, generator loss: 0.659876823425293, discriminator loss: 1.5193859338760376 
Batch 2756, generator loss: 0.7002739906311035, discriminator loss: 1.502145528793335 
Batch 2757, generator loss: 0.761005699634552, discriminator loss: 1.4740417003631592 
Batch 2758, generator loss: 0.8219603300094604, discriminator loss: 1.4439609050750732 
Batch 2759, generator loss: 0.7830313444137573, discriminator loss: 1.507165551185608 
Batch 2760, generator loss: 0.7781788110733032, discriminator loss: 1.5074338912963867 
Batch 2761, generator loss: 0.7304160594940186, discriminator loss: 1.4667317867279053 
Batch 2762, generator loss: 0.701755166053772, discriminator loss: 1.4797663688659668 
Batch 2763, generator loss: 0.6913031339645386, discriminator loss: 1.4696515798568726 
Batch 2764, generator loss: 0.7138473391532898, discriminator loss: 1.4414851665496826 
Batch 2765, generator loss: 0.7514418959617615, discriminator loss: 1.4591777324676514 
Batch 2766, generator loss: 0.7649142742156982, discriminator loss: 1.4636213779449463 
Batch 2767, generator loss: 0.8232189416885376, discriminator loss: 1.408417820930481 
Batch 2768, generator loss: 0.8121144771575928, discriminator loss: 1.3952335119247437 
Batch 2769, generator loss: 0.7681804895401001, discriminator loss: 1.3986027240753174 
Batch 2770, generator loss: 0.7785449028015137, discriminator loss: 1.390928864479065 
Batch 2771, generator loss: 0.7544561624526978, discriminator loss: 1.3579421043395996 
Batch 2772, generator loss: 0.7481728792190552, discriminator loss: 1.3323324918746948 
Batch 2773, generator loss: 0.7460007667541504, discriminator loss: 1.3824400901794434 
Batch 2774, generator loss: 0.7866851091384888, discriminator loss: 1.3286614418029785 
Batch 2775, generator loss: 0.8106732368469238, discriminator loss: 1.350350022315979 
Batch 2776, generator loss: 0.8231639862060547, discriminator loss: 1.3066738843917847 
Batch 2777, generator loss: 0.8709648847579956, discriminator loss: 1.3193458318710327 
Batch 2778, generator loss: 0.8664657473564148, discriminator loss: 1.2934510707855225 
Batch 2779, generator loss: 0.8217552900314331, discriminator loss: 1.274524450302124 
Batch 2780, generator loss: 0.7903463244438171, discriminator loss: 1.2809104919433594 
Batch 2781, generator loss: 0.7893327474594116, discriminator loss: 1.224898099899292 
Batch 2782, generator loss: 0.803650975227356, discriminator loss: 1.2638053894042969 
Batch 2783, generator loss: 0.8653671145439148, discriminator loss: 1.2511177062988281 
Batch 2784, generator loss: 0.9020649194717407, discriminator loss: 1.2044589519500732 
Batch 2785, generator loss: 0.8738186359405518, discriminator loss: 1.2259011268615723 
Batch 2786, generator loss: 0.8943107724189758, discriminator loss: 1.182660698890686 
Batch 2787, generator loss: 0.9088343977928162, discriminator loss: 1.190488576889038 
Batch 2788, generator loss: 0.8775481581687927, discriminator loss: 1.184969425201416 
Batch 2789, generator loss: 0.8760238885879517, discriminator loss: 1.1955058574676514 
Batch 2790, generator loss: 0.8568569421768188, discriminator loss: 1.1802096366882324 
Batch 2791, generator loss: 0.873274028301239, discriminator loss: 1.1399381160736084 
Batch 2792, generator loss: 0.8824279308319092, discriminator loss: 1.1587023735046387 
Batch 2793, generator loss: 0.9298608303070068, discriminator loss: 1.1158411502838135 
Batch 2794, generator loss: 0.952354907989502, discriminator loss: 1.1471245288848877 
Batch 2795, generator loss: 0.9421060085296631, discriminator loss: 1.1444363594055176 
Batch 2796, generator loss: 0.9530653357505798, discriminator loss: 1.0783240795135498 
Generator loss: tf.Tensor(0.95306534, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.0783241, shape=(), dtype=float32)
EPOCH: 11
Batch 2818, generator loss: 0.9391738176345825, discriminator loss: 1.10994291305542 
Batch 2819, generator loss: 0.956000030040741, discriminator loss: 1.0564510822296143 
Batch 2820, generator loss: 0.9351110458374023, discriminator loss: 1.1251256465911865 
Batch 2821, generator loss: 0.9361617565155029, discriminator loss: 1.0834696292877197 
Batch 2822, generator loss: 0.9400119781494141, discriminator loss: 1.1009858846664429 
Batch 2823, generator loss: 0.9120447635650635, discriminator loss: 1.1005810499191284 
Batch 2824, generator loss: 0.9411033391952515, discriminator loss: 1.0587477684020996 
Batch 2825, generator loss: 0.9629886746406555, discriminator loss: 1.1070598363876343 
Batch 2826, generator loss: 0.9481810927391052, discriminator loss: 1.1150662899017334 
Batch 2827, generator loss: 0.9857819080352783, discriminator loss: 1.0418221950531006 
Batch 2828, generator loss: 0.9314215183258057, discriminator loss: 1.079328179359436 
Batch 2829, generator loss: 0.9431277513504028, discriminator loss: 1.0969282388687134 
Batch 2830, generator loss: 0.9434994459152222, discriminator loss: 1.0397794246673584 
Batch 2831, generator loss: 0.9339190721511841, discriminator loss: 1.0659019947052002 
Batch 2832, generator loss: 0.997418999671936, discriminator loss: 1.0625157356262207 
Batch 2833, generator loss: 0.9736508131027222, discriminator loss: 1.0620852708816528 
Batch 2834, generator loss: 0.980642557144165, discriminator loss: 1.0742805004119873 
Batch 2835, generator loss: 0.9576853513717651, discriminator loss: 1.045928716659546 
Batch 2836, generator loss: 0.9587734937667847, discriminator loss: 1.0807924270629883 
Batch 2837, generator loss: 0.9870225191116333, discriminator loss: 1.0688737630844116 
Batch 2838, generator loss: 0.937824010848999, discriminator loss: 1.0479736328125 
Batch 2839, generator loss: 0.9218346476554871, discriminator loss: 1.0824816226959229 
Batch 2840, generator loss: 0.9391739964485168, discriminator loss: 1.0398306846618652 
Batch 2841, generator loss: 0.9755067825317383, discriminator loss: 1.0669409036636353 
Batch 2842, generator loss: 1.0151395797729492, discriminator loss: 1.0640218257904053 
Batch 2843, generator loss: 0.9913917779922485, discriminator loss: 1.0875704288482666 
Batch 2844, generator loss: 0.9772592782974243, discriminator loss: 1.0854448080062866 
Batch 2845, generator loss: 0.9332228899002075, discriminator loss: 1.0562329292297363 
Batch 2846, generator loss: 0.9440255761146545, discriminator loss: 1.0202372074127197 
Batch 2847, generator loss: 0.8994358777999878, discriminator loss: 1.1032602787017822 
Batch 2848, generator loss: 0.9459392428398132, discriminator loss: 1.0515222549438477 
Batch 2849, generator loss: 0.9900981187820435, discriminator loss: 1.0935148000717163 
Batch 2850, generator loss: 1.0202677249908447, discriminator loss: 1.12229323387146 
Batch 2851, generator loss: 1.0439631938934326, discriminator loss: 1.0671679973602295 
Batch 2852, generator loss: 0.9664669036865234, discriminator loss: 1.0867385864257812 
Batch 2853, generator loss: 0.9450057744979858, discriminator loss: 1.083116054534912 
Batch 2854, generator loss: 0.9385511875152588, discriminator loss: 1.0168942213058472 
Batch 2855, generator loss: 0.9581568837165833, discriminator loss: 1.0431358814239502 
Batch 2856, generator loss: 0.9812900424003601, discriminator loss: 1.0217456817626953 
Batch 2857, generator loss: 1.0447654724121094, discriminator loss: 1.0371909141540527 
Batch 2858, generator loss: 1.0136390924453735, discriminator loss: 1.0611159801483154 
Batch 2859, generator loss: 1.1020376682281494, discriminator loss: 1.003827452659607 
Batch 2860, generator loss: 1.0810455083847046, discriminator loss: 1.0367650985717773 
Batch 2861, generator loss: 1.0431342124938965, discriminator loss: 1.076355218887329 
Batch 2862, generator loss: 0.9873241782188416, discriminator loss: 1.0382344722747803 
Batch 2863, generator loss: 1.0042929649353027, discriminator loss: 0.9845027923583984 
Batch 2864, generator loss: 1.0417871475219727, discriminator loss: 1.0505506992340088 
Batch 2865, generator loss: 1.0064899921417236, discriminator loss: 1.0416358709335327 
Batch 2866, generator loss: 1.0087836980819702, discriminator loss: 1.0648393630981445 
Batch 2867, generator loss: 1.0363969802856445, discriminator loss: 1.0287683010101318 
Batch 2868, generator loss: 1.1221767663955688, discriminator loss: 1.0605816841125488 
Batch 2869, generator loss: 1.1049017906188965, discriminator loss: 1.0315295457839966 
Batch 2870, generator loss: 1.0726193189620972, discriminator loss: 1.117192268371582 
Batch 2871, generator loss: 1.0341333150863647, discriminator loss: 1.0109492540359497 
Batch 2872, generator loss: 0.9873540997505188, discriminator loss: 1.1071020364761353 
Batch 2873, generator loss: 0.936831533908844, discriminator loss: 1.0967433452606201 
Batch 2874, generator loss: 0.940916121006012, discriminator loss: 1.1570658683776855 
Batch 2875, generator loss: 1.0153728723526, discriminator loss: 1.1779741048812866 
Batch 2876, generator loss: 1.0055220127105713, discriminator loss: 1.2848291397094727 
Batch 2877, generator loss: 1.0079095363616943, discriminator loss: 1.302254319190979 
Batch 2878, generator loss: 0.9038285613059998, discriminator loss: 1.3137032985687256 
Batch 2879, generator loss: 0.8635044693946838, discriminator loss: 1.3139448165893555 
Batch 2880, generator loss: 0.8140019178390503, discriminator loss: 1.256048321723938 
Batch 2881, generator loss: 0.8813439607620239, discriminator loss: 1.3375904560089111 
Batch 2882, generator loss: 0.9483006000518799, discriminator loss: 1.3033316135406494 
Batch 2883, generator loss: 0.8819767236709595, discriminator loss: 1.4568994045257568 
Batch 2884, generator loss: 0.8876539468765259, discriminator loss: 1.4770528078079224 
Batch 2885, generator loss: 0.8617857694625854, discriminator loss: 1.4881744384765625 
Batch 2886, generator loss: 0.8067418932914734, discriminator loss: 1.5380133390426636 
Batch 2887, generator loss: 0.7129290103912354, discriminator loss: 1.5632163286209106 
Batch 2888, generator loss: 0.6798220872879028, discriminator loss: 1.6987090110778809 
Batch 2889, generator loss: 0.7451915144920349, discriminator loss: 1.5936294794082642 
Batch 2890, generator loss: 0.755969226360321, discriminator loss: 1.6588060855865479 
Batch 2891, generator loss: 0.7851301431655884, discriminator loss: 1.681365966796875 
Batch 2892, generator loss: 0.8292992115020752, discriminator loss: 1.7202168703079224 
Batch 2893, generator loss: 0.7885593175888062, discriminator loss: 1.7155828475952148 
Batch 2894, generator loss: 0.7353838682174683, discriminator loss: 1.7044919729232788 
Batch 2895, generator loss: 0.679232120513916, discriminator loss: 1.644078254699707 
Batch 2896, generator loss: 0.6484358310699463, discriminator loss: 1.6781188249588013 
Batch 2897, generator loss: 0.7105885148048401, discriminator loss: 1.603703498840332 
Batch 2898, generator loss: 0.7560828328132629, discriminator loss: 1.6102391481399536 
Batch 2899, generator loss: 0.7915071845054626, discriminator loss: 1.6314228773117065 
Batch 2900, generator loss: 0.775743305683136, discriminator loss: 1.6186556816101074 
Batch 2901, generator loss: 0.788748025894165, discriminator loss: 1.627852201461792 
Batch 2902, generator loss: 0.7333120703697205, discriminator loss: 1.593672752380371 
Batch 2903, generator loss: 0.7184981107711792, discriminator loss: 1.5477087497711182 
Batch 2904, generator loss: 0.7269973754882812, discriminator loss: 1.5260603427886963 
Batch 2905, generator loss: 0.7523046731948853, discriminator loss: 1.468360185623169 
Batch 2906, generator loss: 0.7999559044837952, discriminator loss: 1.447339415550232 
Batch 2907, generator loss: 0.8609545826911926, discriminator loss: 1.3946819305419922 
Batch 2908, generator loss: 0.8826757669448853, discriminator loss: 1.3860485553741455 
Batch 2909, generator loss: 0.8547868728637695, discriminator loss: 1.3573873043060303 
Batch 2910, generator loss: 0.8282040357589722, discriminator loss: 1.3400758504867554 
Batch 2911, generator loss: 0.8358080387115479, discriminator loss: 1.2466415166854858 
Batch 2912, generator loss: 0.8394587635993958, discriminator loss: 1.2220128774642944 
Batch 2913, generator loss: 0.8970980644226074, discriminator loss: 1.1692907810211182 
Batch 2914, generator loss: 0.912195086479187, discriminator loss: 1.2013418674468994 
Batch 2915, generator loss: 1.03856360912323, discriminator loss: 1.1265535354614258 
Batch 2916, generator loss: 1.04523503780365, discriminator loss: 1.0903830528259277 
Batch 2917, generator loss: 1.0160707235336304, discriminator loss: 1.0700820684432983 
Batch 2918, generator loss: 1.0264899730682373, discriminator loss: 1.0558587312698364 
Batch 2919, generator loss: 0.9808340072631836, discriminator loss: 1.0339558124542236 
Batch 2920, generator loss: 1.0076568126678467, discriminator loss: 0.995216965675354 
Batch 2921, generator loss: 1.0502099990844727, discriminator loss: 0.9833846092224121 
Batch 2922, generator loss: 1.0820622444152832, discriminator loss: 0.9572428464889526 
Batch 2923, generator loss: 1.0908094644546509, discriminator loss: 0.9760336875915527 
Batch 2924, generator loss: 1.1186778545379639, discriminator loss: 0.9477604031562805 
Batch 2925, generator loss: 1.1279511451721191, discriminator loss: 0.9008206129074097 
Batch 2926, generator loss: 1.1433899402618408, discriminator loss: 0.9120173454284668 
Batch 2927, generator loss: 1.1418578624725342, discriminator loss: 0.9142206907272339 
Batch 2928, generator loss: 1.1201858520507812, discriminator loss: 0.8946451544761658 
Batch 2929, generator loss: 1.1734638214111328, discriminator loss: 0.9161211252212524 
Batch 2930, generator loss: 1.1471507549285889, discriminator loss: 0.9328678250312805 
Batch 2931, generator loss: 1.1656138896942139, discriminator loss: 0.8709858059883118 
Batch 2932, generator loss: 1.1689541339874268, discriminator loss: 0.8963159322738647 
Batch 2933, generator loss: 1.183199405670166, discriminator loss: 0.8998943567276001 
Batch 2934, generator loss: 1.17909574508667, discriminator loss: 0.9359526634216309 
Batch 2935, generator loss: 1.1455236673355103, discriminator loss: 0.9816843867301941 
Batch 2936, generator loss: 1.0867705345153809, discriminator loss: 0.9371456503868103 
Batch 2937, generator loss: 1.0970261096954346, discriminator loss: 0.9357074499130249 
Batch 2938, generator loss: 1.1216638088226318, discriminator loss: 0.9954878687858582 
Batch 2939, generator loss: 1.1062500476837158, discriminator loss: 0.9932729005813599 
Batch 2940, generator loss: 1.145398736000061, discriminator loss: 1.0238479375839233 
Batch 2941, generator loss: 1.1187926530838013, discriminator loss: 0.9958469867706299 
Batch 2942, generator loss: 1.0738613605499268, discriminator loss: 1.067249059677124 
Batch 2943, generator loss: 1.079695463180542, discriminator loss: 1.0512118339538574 
Batch 2944, generator loss: 1.0812640190124512, discriminator loss: 1.0833165645599365 
Batch 2945, generator loss: 1.043247938156128, discriminator loss: 1.1292951107025146 
Batch 2946, generator loss: 1.042287826538086, discriminator loss: 1.1367441415786743 
Batch 2947, generator loss: 1.0095670223236084, discriminator loss: 1.148578405380249 
Batch 2948, generator loss: 1.003953218460083, discriminator loss: 1.1887798309326172 
Batch 2949, generator loss: 0.9909889698028564, discriminator loss: 1.1989576816558838 
Batch 2950, generator loss: 1.002010464668274, discriminator loss: 1.206855058670044 
Batch 2951, generator loss: 0.9425236582756042, discriminator loss: 1.3069837093353271 
Batch 2952, generator loss: 0.9363600015640259, discriminator loss: 1.300661563873291 
Batch 2953, generator loss: 0.8810046315193176, discriminator loss: 1.3100767135620117 
Batch 2954, generator loss: 0.8858127593994141, discriminator loss: 1.3370466232299805 
Batch 2955, generator loss: 0.8689600229263306, discriminator loss: 1.3094857931137085 
Batch 2956, generator loss: 0.8942832946777344, discriminator loss: 1.3651509284973145 
Batch 2957, generator loss: 0.9656351208686829, discriminator loss: 1.4768812656402588 
Batch 2958, generator loss: 0.9457369446754456, discriminator loss: 1.4310081005096436 
Batch 2959, generator loss: 0.865863561630249, discriminator loss: 1.4254875183105469 
Batch 2960, generator loss: 0.7672778367996216, discriminator loss: 1.454511284828186 
Batch 2961, generator loss: 0.7401401996612549, discriminator loss: 1.5199506282806396 
Batch 2962, generator loss: 0.7362129092216492, discriminator loss: 1.514297604560852 
Batch 2963, generator loss: 0.7721847295761108, discriminator loss: 1.4768095016479492 
Batch 2964, generator loss: 0.9008092284202576, discriminator loss: 1.4219133853912354 
Batch 2965, generator loss: 0.8309529423713684, discriminator loss: 1.4189295768737793 
Batch 2966, generator loss: 0.8153234720230103, discriminator loss: 1.5983824729919434 
Batch 2967, generator loss: 0.7896912097930908, discriminator loss: 1.5139113664627075 
Batch 2968, generator loss: 0.7668294906616211, discriminator loss: 1.4286613464355469 
Batch 2969, generator loss: 0.7136622667312622, discriminator loss: 1.4609357118606567 
Batch 2970, generator loss: 0.7168303728103638, discriminator loss: 1.4718636274337769 
Batch 2971, generator loss: 0.7519299983978271, discriminator loss: 1.4341678619384766 
Batch 2972, generator loss: 0.8336374759674072, discriminator loss: 1.3520210981369019 
Batch 2973, generator loss: 0.8177993893623352, discriminator loss: 1.3956962823867798 
Batch 2974, generator loss: 0.8449057936668396, discriminator loss: 1.3625962734222412 
Batch 2975, generator loss: 0.8580937385559082, discriminator loss: 1.2901833057403564 
Batch 2976, generator loss: 0.8771191835403442, discriminator loss: 1.2743184566497803 
Batch 2977, generator loss: 0.8378915786743164, discriminator loss: 1.315483570098877 
Batch 2978, generator loss: 0.8212714195251465, discriminator loss: 1.1743451356887817 
Batch 2979, generator loss: 0.8950971364974976, discriminator loss: 1.1869230270385742 
Batch 2980, generator loss: 0.8618272542953491, discriminator loss: 1.1938612461090088 
Batch 2981, generator loss: 0.8756093382835388, discriminator loss: 1.1503922939300537 
Batch 2982, generator loss: 0.9696410894393921, discriminator loss: 1.139183759689331 
Batch 2983, generator loss: 0.971310019493103, discriminator loss: 1.1385124921798706 
Batch 2984, generator loss: 0.9896286129951477, discriminator loss: 1.0601445436477661 
Batch 2985, generator loss: 1.0174250602722168, discriminator loss: 0.9924043416976929 
Batch 2986, generator loss: 1.0136971473693848, discriminator loss: 1.0200270414352417 
Batch 2987, generator loss: 1.0455925464630127, discriminator loss: 1.0127899646759033 
Batch 2988, generator loss: 1.065560221672058, discriminator loss: 0.9563751220703125 
Batch 2989, generator loss: 1.1021463871002197, discriminator loss: 0.9519830942153931 
Batch 2990, generator loss: 1.0634733438491821, discriminator loss: 0.921150267124176 
Batch 2991, generator loss: 1.1157697439193726, discriminator loss: 0.9266330003738403 
Batch 2992, generator loss: 1.1490000486373901, discriminator loss: 0.8942583799362183 
Batch 2993, generator loss: 1.1667981147766113, discriminator loss: 0.8763004541397095 
Batch 2994, generator loss: 1.2300999164581299, discriminator loss: 0.8639975190162659 
Batch 2995, generator loss: 1.2334215641021729, discriminator loss: 0.862858772277832 
Batch 2996, generator loss: 1.2595312595367432, discriminator loss: 0.842420756816864 
Batch 2997, generator loss: 1.2425620555877686, discriminator loss: 0.8207701444625854 
Batch 2998, generator loss: 1.2404308319091797, discriminator loss: 0.8753658533096313 
Batch 2999, generator loss: 1.1840031147003174, discriminator loss: 0.8737190961837769 
Batch 3000, generator loss: 1.2245759963989258, discriminator loss: 0.8455583453178406 
Batch 3001, generator loss: 1.2353984117507935, discriminator loss: 0.8662636280059814 
Batch 3002, generator loss: 1.2882804870605469, discriminator loss: 0.8370655179023743 
Batch 3003, generator loss: 1.307065486907959, discriminator loss: 0.8451895117759705 
Batch 3004, generator loss: 1.2383540868759155, discriminator loss: 0.8668723106384277 
Batch 3005, generator loss: 1.2913093566894531, discriminator loss: 0.852876603603363 
Batch 3006, generator loss: 1.2333462238311768, discriminator loss: 0.8242799043655396 
Batch 3007, generator loss: 1.2110905647277832, discriminator loss: 0.8479897975921631 
Batch 3008, generator loss: 1.2916723489761353, discriminator loss: 0.8684261441230774 
Batch 3009, generator loss: 1.2409441471099854, discriminator loss: 0.9563106298446655 
Batch 3010, generator loss: 1.1840388774871826, discriminator loss: 0.9390037059783936 
Batch 3011, generator loss: 1.1799060106277466, discriminator loss: 0.9215865135192871 
Batch 3012, generator loss: 1.1774003505706787, discriminator loss: 0.9067736864089966 
Batch 3013, generator loss: 1.1929837465286255, discriminator loss: 0.9504597187042236 
Batch 3014, generator loss: 1.1656373739242554, discriminator loss: 0.9733309745788574 
Batch 3015, generator loss: 1.1273117065429688, discriminator loss: 0.9894589781761169 
Batch 3016, generator loss: 1.0572376251220703, discriminator loss: 1.015974998474121 
Batch 3017, generator loss: 1.0493288040161133, discriminator loss: 1.0952823162078857 
Batch 3018, generator loss: 1.076109766960144, discriminator loss: 1.1429448127746582 
Batch 3019, generator loss: 1.1129963397979736, discriminator loss: 1.1152393817901611 
Batch 3020, generator loss: 1.0556340217590332, discriminator loss: 1.13120698928833 
Batch 3021, generator loss: 0.9912302494049072, discriminator loss: 1.1706738471984863 
Batch 3022, generator loss: 0.9194475412368774, discriminator loss: 1.2708749771118164 
Batch 3023, generator loss: 0.9163303375244141, discriminator loss: 1.2448456287384033 
Batch 3024, generator loss: 0.9914318919181824, discriminator loss: 1.2728430032730103 
Batch 3025, generator loss: 1.0333216190338135, discriminator loss: 1.1906743049621582 
Batch 3026, generator loss: 1.0055460929870605, discriminator loss: 1.3436228036880493 
Batch 3027, generator loss: 0.9053219556808472, discriminator loss: 1.369804859161377 
Batch 3028, generator loss: 0.8402132391929626, discriminator loss: 1.400468349456787 
Batch 3029, generator loss: 0.8374269008636475, discriminator loss: 1.4210963249206543 
Batch 3030, generator loss: 0.8571045398712158, discriminator loss: 1.476499080657959 
Batch 3031, generator loss: 0.8880839347839355, discriminator loss: 1.6132800579071045 
Batch 3032, generator loss: 0.8402388095855713, discriminator loss: 1.5378303527832031 
Batch 3033, generator loss: 0.8228347301483154, discriminator loss: 1.5093588829040527 
Batch 3034, generator loss: 0.7599780559539795, discriminator loss: 1.5403547286987305 
Batch 3035, generator loss: 0.7117071747779846, discriminator loss: 1.585402250289917 
Batch 3036, generator loss: 0.8089896440505981, discriminator loss: 1.6289142370224 
Batch 3037, generator loss: 0.8534973859786987, discriminator loss: 1.5512969493865967 
Batch 3038, generator loss: 0.8324918746948242, discriminator loss: 1.5986335277557373 
Batch 3039, generator loss: 0.7674921154975891, discriminator loss: 1.648780345916748 
Batch 3040, generator loss: 0.6890636086463928, discriminator loss: 1.660409688949585 
Batch 3041, generator loss: 0.6637716293334961, discriminator loss: 1.6079297065734863 
Batch 3042, generator loss: 0.7645857930183411, discriminator loss: 1.660494327545166 
Batch 3043, generator loss: 0.8095481395721436, discriminator loss: 1.6413154602050781 
Batch 3044, generator loss: 0.8200520873069763, discriminator loss: 1.5429410934448242 
Batch 3045, generator loss: 0.7565006017684937, discriminator loss: 1.5698680877685547 
Batch 3046, generator loss: 0.7337454557418823, discriminator loss: 1.531589150428772 
Batch 3047, generator loss: 0.7619438171386719, discriminator loss: 1.5127607583999634 
Batch 3048, generator loss: 0.7112859487533569, discriminator loss: 1.525902271270752 
Batch 3049, generator loss: 0.7355653047561646, discriminator loss: 1.4964627027511597 
Batch 3050, generator loss: 0.8331621885299683, discriminator loss: 1.4571948051452637 
Batch 3051, generator loss: 0.8538285493850708, discriminator loss: 1.4673686027526855 
Batch 3052, generator loss: 0.8404053449630737, discriminator loss: 1.433375358581543 
Generator loss: tf.Tensor(0.84040534, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.4333754, shape=(), dtype=float32)
EPOCH: 12
Batch 3074, generator loss: 0.7999730706214905, discriminator loss: 1.353832721710205 
Batch 3075, generator loss: 0.765365719795227, discriminator loss: 1.4321399927139282 
Batch 3076, generator loss: 0.8073248267173767, discriminator loss: 1.3707306385040283 
Batch 3077, generator loss: 0.7995055317878723, discriminator loss: 1.3863325119018555 
Batch 3078, generator loss: 0.8494516611099243, discriminator loss: 1.324937105178833 
Batch 3079, generator loss: 0.8782532215118408, discriminator loss: 1.3121776580810547 
Batch 3080, generator loss: 0.8856924772262573, discriminator loss: 1.305738925933838 
Batch 3081, generator loss: 0.9030208587646484, discriminator loss: 1.2301366329193115 
Batch 3082, generator loss: 0.8951895236968994, discriminator loss: 1.2551465034484863 
Batch 3083, generator loss: 0.8613216876983643, discriminator loss: 1.2113356590270996 
Batch 3084, generator loss: 0.8457000255584717, discriminator loss: 1.2316837310791016 
Batch 3085, generator loss: 0.9070577621459961, discriminator loss: 1.1842613220214844 
Batch 3086, generator loss: 1.0112664699554443, discriminator loss: 1.130592703819275 
Batch 3087, generator loss: 0.9921937584877014, discriminator loss: 1.120133876800537 
Batch 3088, generator loss: 0.9850745797157288, discriminator loss: 1.1572468280792236 
Batch 3089, generator loss: 0.9474762678146362, discriminator loss: 1.1100695133209229 
Batch 3090, generator loss: 0.8362802863121033, discriminator loss: 1.1718374490737915 
Batch 3091, generator loss: 0.8806806802749634, discriminator loss: 1.1100397109985352 
Batch 3092, generator loss: 0.9533947706222534, discriminator loss: 1.0718631744384766 
Batch 3093, generator loss: 1.0570168495178223, discriminator loss: 1.0799849033355713 
Batch 3094, generator loss: 1.0768715143203735, discriminator loss: 1.0806024074554443 
Batch 3095, generator loss: 1.019151210784912, discriminator loss: 1.0256383419036865 
Batch 3096, generator loss: 0.9853246808052063, discriminator loss: 1.0442728996276855 
Batch 3097, generator loss: 0.9233571290969849, discriminator loss: 1.0271285772323608 
Batch 3098, generator loss: 0.8976620435714722, discriminator loss: 1.0627268552780151 
Batch 3099, generator loss: 1.017556071281433, discriminator loss: 1.0234261751174927 
Batch 3100, generator loss: 1.0734992027282715, discriminator loss: 1.0290513038635254 
Batch 3101, generator loss: 1.10471510887146, discriminator loss: 1.0222662687301636 
Batch 3102, generator loss: 1.033982515335083, discriminator loss: 0.9641655087471008 
Batch 3103, generator loss: 1.031334638595581, discriminator loss: 1.0311836004257202 
Batch 3104, generator loss: 1.0167372226715088, discriminator loss: 0.9999259114265442 
Batch 3105, generator loss: 1.0011277198791504, discriminator loss: 0.9971015453338623 
Batch 3106, generator loss: 0.9808328151702881, discriminator loss: 0.998178243637085 
Batch 3107, generator loss: 1.0267692804336548, discriminator loss: 1.021873116493225 
Batch 3108, generator loss: 1.0496811866760254, discriminator loss: 0.9975773692131042 
Batch 3109, generator loss: 1.0804240703582764, discriminator loss: 0.9893323183059692 
Batch 3110, generator loss: 1.0222773551940918, discriminator loss: 0.9886475801467896 
Batch 3111, generator loss: 1.0701128244400024, discriminator loss: 0.9729804992675781 
Batch 3112, generator loss: 0.9934198260307312, discriminator loss: 0.9899002313613892 
Batch 3113, generator loss: 1.0825964212417603, discriminator loss: 1.0170738697052002 
Batch 3114, generator loss: 1.0356557369232178, discriminator loss: 0.9613888263702393 
Batch 3115, generator loss: 1.0195302963256836, discriminator loss: 0.9464801549911499 
Batch 3116, generator loss: 1.0103328227996826, discriminator loss: 0.9527338147163391 
Batch 3117, generator loss: 1.136082410812378, discriminator loss: 0.9802078008651733 
Batch 3118, generator loss: 1.0801982879638672, discriminator loss: 0.948764443397522 
Batch 3119, generator loss: 1.1232469081878662, discriminator loss: 0.939532995223999 
Batch 3120, generator loss: 1.1278481483459473, discriminator loss: 0.9154195785522461 
Batch 3121, generator loss: 1.0927402973175049, discriminator loss: 0.9184306859970093 
Batch 3122, generator loss: 1.0652711391448975, discriminator loss: 0.8871757984161377 
Batch 3123, generator loss: 1.1008481979370117, discriminator loss: 0.908765435218811 
Batch 3124, generator loss: 1.14628267288208, discriminator loss: 0.8675297498703003 
Batch 3125, generator loss: 1.204824686050415, discriminator loss: 0.8642576932907104 
Batch 3126, generator loss: 1.1353238821029663, discriminator loss: 0.9003035426139832 
Batch 3127, generator loss: 1.1524171829223633, discriminator loss: 0.8618991374969482 
Batch 3128, generator loss: 1.1298835277557373, discriminator loss: 0.8578481078147888 
Batch 3129, generator loss: 1.1708077192306519, discriminator loss: 0.8670617341995239 
Batch 3130, generator loss: 1.216630458831787, discriminator loss: 0.8285782933235168 
Batch 3131, generator loss: 1.1878005266189575, discriminator loss: 0.8380166292190552 
Batch 3132, generator loss: 1.1621206998825073, discriminator loss: 0.8449938893318176 
Batch 3133, generator loss: 1.2255158424377441, discriminator loss: 0.8419268131256104 
Batch 3134, generator loss: 1.195011854171753, discriminator loss: 0.8094884157180786 
Batch 3135, generator loss: 1.2008967399597168, discriminator loss: 0.7966233491897583 
Batch 3136, generator loss: 1.2148678302764893, discriminator loss: 0.8026078939437866 
Batch 3137, generator loss: 1.2582342624664307, discriminator loss: 0.8177969455718994 
Batch 3138, generator loss: 1.2065826654434204, discriminator loss: 0.8274837732315063 
Batch 3139, generator loss: 1.2782084941864014, discriminator loss: 0.7827343344688416 
Batch 3140, generator loss: 1.1773605346679688, discriminator loss: 0.845112681388855 
Batch 3141, generator loss: 1.185835838317871, discriminator loss: 0.8392236828804016 
Batch 3142, generator loss: 1.19154691696167, discriminator loss: 0.8390237092971802 
Batch 3143, generator loss: 1.20442795753479, discriminator loss: 0.8548288941383362 
Batch 3144, generator loss: 1.1283551454544067, discriminator loss: 0.8868686556816101 
Batch 3145, generator loss: 1.1589475870132446, discriminator loss: 0.8715757131576538 
Batch 3146, generator loss: 1.0735067129135132, discriminator loss: 0.9173769354820251 
Batch 3147, generator loss: 1.0850160121917725, discriminator loss: 0.989794135093689 
Batch 3148, generator loss: 1.1270215511322021, discriminator loss: 0.9886276721954346 
Batch 3149, generator loss: 1.1656330823898315, discriminator loss: 0.9529640078544617 
Batch 3150, generator loss: 1.1076476573944092, discriminator loss: 0.9599395394325256 
Batch 3151, generator loss: 1.0196672677993774, discriminator loss: 1.0345079898834229 
Batch 3152, generator loss: 1.0317931175231934, discriminator loss: 1.0434677600860596 
Batch 3153, generator loss: 0.9950355291366577, discriminator loss: 1.095268726348877 
Batch 3154, generator loss: 1.0145716667175293, discriminator loss: 1.0475656986236572 
Batch 3155, generator loss: 1.0096336603164673, discriminator loss: 1.084409236907959 
Batch 3156, generator loss: 1.0354701280593872, discriminator loss: 1.0909242630004883 
Batch 3157, generator loss: 1.0240967273712158, discriminator loss: 1.225033164024353 
Batch 3158, generator loss: 0.9861716628074646, discriminator loss: 1.1752793788909912 
Batch 3159, generator loss: 0.9668276309967041, discriminator loss: 1.1646416187286377 
Batch 3160, generator loss: 0.9017679691314697, discriminator loss: 1.203972578048706 
Batch 3161, generator loss: 0.9134671688079834, discriminator loss: 1.2680790424346924 
Batch 3162, generator loss: 0.9345240592956543, discriminator loss: 1.2204619646072388 
Batch 3163, generator loss: 0.9105437994003296, discriminator loss: 1.237226963043213 
Batch 3164, generator loss: 0.8901706337928772, discriminator loss: 1.2259654998779297 
Batch 3165, generator loss: 0.9467257857322693, discriminator loss: 1.2766772508621216 
Batch 3166, generator loss: 0.9009295105934143, discriminator loss: 1.3713657855987549 
Batch 3167, generator loss: 0.8922962546348572, discriminator loss: 1.368912696838379 
Batch 3168, generator loss: 0.8516824841499329, discriminator loss: 1.3743752241134644 
Batch 3169, generator loss: 0.8131990432739258, discriminator loss: 1.393609642982483 
Batch 3170, generator loss: 0.8192735910415649, discriminator loss: 1.3745479583740234 
Batch 3171, generator loss: 0.7984597682952881, discriminator loss: 1.4142112731933594 
Batch 3172, generator loss: 0.8755627870559692, discriminator loss: 1.3299179077148438 
Batch 3173, generator loss: 0.8266459703445435, discriminator loss: 1.4080307483673096 
Batch 3174, generator loss: 0.819015622138977, discriminator loss: 1.4357877969741821 
Batch 3175, generator loss: 0.8204504251480103, discriminator loss: 1.5283074378967285 
Batch 3176, generator loss: 0.7706204652786255, discriminator loss: 1.4987984895706177 
Batch 3177, generator loss: 0.7759060263633728, discriminator loss: 1.4725441932678223 
Batch 3178, generator loss: 0.7755125761032104, discriminator loss: 1.4852240085601807 
Batch 3179, generator loss: 0.7887274026870728, discriminator loss: 1.5203649997711182 
Batch 3180, generator loss: 0.7837000489234924, discriminator loss: 1.5168800354003906 
Batch 3181, generator loss: 0.7686979174613953, discriminator loss: 1.5751087665557861 
Batch 3182, generator loss: 0.7333949208259583, discriminator loss: 1.557063341140747 
Batch 3183, generator loss: 0.7292460203170776, discriminator loss: 1.619969367980957 
Batch 3184, generator loss: 0.7266135215759277, discriminator loss: 1.574172019958496 
Batch 3185, generator loss: 0.7815999388694763, discriminator loss: 1.5392892360687256 
Batch 3186, generator loss: 0.7449883818626404, discriminator loss: 1.616527795791626 
Batch 3187, generator loss: 0.7055125832557678, discriminator loss: 1.5896368026733398 
Batch 3188, generator loss: 0.7413505911827087, discriminator loss: 1.5815403461456299 
Batch 3189, generator loss: 0.7651842832565308, discriminator loss: 1.6222463846206665 
Batch 3190, generator loss: 0.7504333257675171, discriminator loss: 1.6072008609771729 
Batch 3191, generator loss: 0.7140538692474365, discriminator loss: 1.6186068058013916 
Batch 3192, generator loss: 0.7119507789611816, discriminator loss: 1.644159197807312 
Batch 3193, generator loss: 0.7051814794540405, discriminator loss: 1.6206024885177612 
Batch 3194, generator loss: 0.7550880312919617, discriminator loss: 1.6420214176177979 
Batch 3195, generator loss: 0.7478203177452087, discriminator loss: 1.5592775344848633 
Batch 3196, generator loss: 0.7455284595489502, discriminator loss: 1.6005933284759521 
Batch 3197, generator loss: 0.7737547755241394, discriminator loss: 1.5235353708267212 
Batch 3198, generator loss: 0.7403203248977661, discriminator loss: 1.5526028871536255 
Batch 3199, generator loss: 0.7723202705383301, discriminator loss: 1.5272105932235718 
Batch 3200, generator loss: 0.7211142182350159, discriminator loss: 1.5439032316207886 
Batch 3201, generator loss: 0.7296737432479858, discriminator loss: 1.5549731254577637 
Batch 3202, generator loss: 0.7506341934204102, discriminator loss: 1.5348117351531982 
Batch 3203, generator loss: 0.794080376625061, discriminator loss: 1.4862942695617676 
Batch 3204, generator loss: 0.8031452298164368, discriminator loss: 1.450788974761963 
Batch 3205, generator loss: 0.7740538716316223, discriminator loss: 1.4623392820358276 
Batch 3206, generator loss: 0.7873361110687256, discriminator loss: 1.446220874786377 
Batch 3207, generator loss: 0.7362970113754272, discriminator loss: 1.3976726531982422 
Batch 3208, generator loss: 0.8350968956947327, discriminator loss: 1.3697929382324219 
Batch 3209, generator loss: 0.8709691762924194, discriminator loss: 1.375657320022583 
Batch 3210, generator loss: 0.8469899892807007, discriminator loss: 1.3403236865997314 
Batch 3211, generator loss: 0.8714600801467896, discriminator loss: 1.275697946548462 
Batch 3212, generator loss: 0.8273134231567383, discriminator loss: 1.302964687347412 
Batch 3213, generator loss: 0.8090430498123169, discriminator loss: 1.263089656829834 
Batch 3214, generator loss: 0.8408747911453247, discriminator loss: 1.2368742227554321 
Batch 3215, generator loss: 0.8779493570327759, discriminator loss: 1.265843152999878 
Batch 3216, generator loss: 0.9860458374023438, discriminator loss: 1.2149639129638672 
Batch 3217, generator loss: 0.9732701778411865, discriminator loss: 1.1955575942993164 
Batch 3218, generator loss: 0.8814913630485535, discriminator loss: 1.2089253664016724 
Batch 3219, generator loss: 0.8663720488548279, discriminator loss: 1.208490252494812 
Batch 3220, generator loss: 0.8804617524147034, discriminator loss: 1.1695892810821533 
Batch 3221, generator loss: 0.9226119518280029, discriminator loss: 1.1124866008758545 
Batch 3222, generator loss: 1.0118745565414429, discriminator loss: 1.1480505466461182 
Batch 3223, generator loss: 1.0255908966064453, discriminator loss: 1.1292102336883545 
Batch 3224, generator loss: 1.0170841217041016, discriminator loss: 1.1043896675109863 
Batch 3225, generator loss: 0.9217112064361572, discriminator loss: 1.1733388900756836 
Batch 3226, generator loss: 0.8746182322502136, discriminator loss: 1.1313704252243042 
Batch 3227, generator loss: 0.8898019790649414, discriminator loss: 1.112335443496704 
Batch 3228, generator loss: 0.9759289622306824, discriminator loss: 1.075662612915039 
Batch 3229, generator loss: 1.0524985790252686, discriminator loss: 1.0879545211791992 
Batch 3230, generator loss: 1.0706820487976074, discriminator loss: 1.1297913789749146 
Batch 3231, generator loss: 1.020024299621582, discriminator loss: 1.101243019104004 
Batch 3232, generator loss: 0.9487853050231934, discriminator loss: 1.082148790359497 
Batch 3233, generator loss: 0.9114004373550415, discriminator loss: 1.1208243370056152 
Batch 3234, generator loss: 0.9190448522567749, discriminator loss: 1.0616016387939453 
Batch 3235, generator loss: 1.041382908821106, discriminator loss: 1.0940709114074707 
Batch 3236, generator loss: 1.0742672681808472, discriminator loss: 1.1600028276443481 
Batch 3237, generator loss: 1.0203901529312134, discriminator loss: 1.109785795211792 
Batch 3238, generator loss: 0.9613598585128784, discriminator loss: 1.1226319074630737 
Batch 3239, generator loss: 0.9911289215087891, discriminator loss: 1.0533186197280884 
Batch 3240, generator loss: 0.9826093912124634, discriminator loss: 1.094133734703064 
Batch 3241, generator loss: 0.9633274078369141, discriminator loss: 1.0915184020996094 
Batch 3242, generator loss: 1.0145219564437866, discriminator loss: 1.1643716096878052 
Batch 3243, generator loss: 1.037473201751709, discriminator loss: 1.083918809890747 
Batch 3244, generator loss: 1.0021381378173828, discriminator loss: 1.0607017278671265 
Batch 3245, generator loss: 0.9872514009475708, discriminator loss: 1.0780043601989746 
Batch 3246, generator loss: 0.9248921871185303, discriminator loss: 1.1221376657485962 
Batch 3247, generator loss: 0.9711310863494873, discriminator loss: 1.1190073490142822 
Batch 3248, generator loss: 1.0295171737670898, discriminator loss: 1.0829824209213257 
Batch 3249, generator loss: 1.0190492868423462, discriminator loss: 1.1620492935180664 
Batch 3250, generator loss: 0.9800133109092712, discriminator loss: 1.137291669845581 
Batch 3251, generator loss: 0.9695166945457458, discriminator loss: 1.1388049125671387 
Batch 3252, generator loss: 0.9232451915740967, discriminator loss: 1.1252672672271729 
Batch 3253, generator loss: 0.9211633801460266, discriminator loss: 1.1829696893692017 
Batch 3254, generator loss: 0.9975903034210205, discriminator loss: 1.1087250709533691 
Batch 3255, generator loss: 0.994106650352478, discriminator loss: 1.1255247592926025 
Batch 3256, generator loss: 1.0390952825546265, discriminator loss: 1.1056983470916748 
Batch 3257, generator loss: 1.017761468887329, discriminator loss: 1.146392583847046 
Batch 3258, generator loss: 1.0061391592025757, discriminator loss: 1.1222680807113647 
Batch 3259, generator loss: 0.940561056137085, discriminator loss: 1.165278673171997 
Batch 3260, generator loss: 0.8539960384368896, discriminator loss: 1.1378779411315918 
Batch 3261, generator loss: 0.9866504669189453, discriminator loss: 1.1416598558425903 
Batch 3262, generator loss: 1.0480400323867798, discriminator loss: 1.149946689605713 
Batch 3263, generator loss: 1.0802831649780273, discriminator loss: 1.1224759817123413 
Batch 3264, generator loss: 0.9872746467590332, discriminator loss: 1.1320610046386719 
Batch 3265, generator loss: 0.9836970567703247, discriminator loss: 1.082885503768921 
Batch 3266, generator loss: 0.9596255421638489, discriminator loss: 1.0925683975219727 
Batch 3267, generator loss: 0.9262628555297852, discriminator loss: 1.097306251525879 
Batch 3268, generator loss: 1.1201165914535522, discriminator loss: 1.1186041831970215 
Batch 3269, generator loss: 1.0551471710205078, discriminator loss: 1.1177948713302612 
Batch 3270, generator loss: 1.0619609355926514, discriminator loss: 1.0654016733169556 
Batch 3271, generator loss: 0.8986392617225647, discriminator loss: 1.1535449028015137 
Batch 3272, generator loss: 0.9059221148490906, discriminator loss: 1.1047742366790771 
Batch 3273, generator loss: 0.9440481066703796, discriminator loss: 1.155259370803833 
Batch 3274, generator loss: 1.0343928337097168, discriminator loss: 1.0930383205413818 
Batch 3275, generator loss: 1.1579819917678833, discriminator loss: 1.0994246006011963 
Batch 3276, generator loss: 1.0996310710906982, discriminator loss: 1.1156275272369385 
Batch 3277, generator loss: 0.9715168476104736, discriminator loss: 1.092301845550537 
Batch 3278, generator loss: 0.9480970501899719, discriminator loss: 1.0630041360855103 
Batch 3279, generator loss: 0.9594662189483643, discriminator loss: 1.0792176723480225 
Batch 3280, generator loss: 1.0043387413024902, discriminator loss: 1.0951473712921143 
Batch 3281, generator loss: 1.0434346199035645, discriminator loss: 1.1105035543441772 
Batch 3282, generator loss: 1.029029130935669, discriminator loss: 1.1544904708862305 
Batch 3283, generator loss: 1.0313456058502197, discriminator loss: 1.1186341047286987 
Batch 3284, generator loss: 0.9930508732795715, discriminator loss: 1.115896463394165 
Batch 3285, generator loss: 0.9916972517967224, discriminator loss: 1.092207431793213 
Batch 3286, generator loss: 0.9032790660858154, discriminator loss: 1.1817660331726074 
Batch 3287, generator loss: 1.0432573556900024, discriminator loss: 1.0652186870574951 
Batch 3288, generator loss: 1.0597615242004395, discriminator loss: 1.1635571718215942 
Batch 3289, generator loss: 1.0536010265350342, discriminator loss: 1.1183664798736572 
Batch 3290, generator loss: 1.0189088582992554, discriminator loss: 1.1089341640472412 
Batch 3291, generator loss: 0.9101884365081787, discriminator loss: 1.1338107585906982 
Batch 3292, generator loss: 0.8918882012367249, discriminator loss: 1.2098958492279053 
Batch 3293, generator loss: 0.9664958715438843, discriminator loss: 1.147883653640747 
Batch 3294, generator loss: 1.0330170392990112, discriminator loss: 1.112928867340088 
Batch 3295, generator loss: 1.0081913471221924, discriminator loss: 1.1775259971618652 
Batch 3296, generator loss: 1.0278691053390503, discriminator loss: 1.1583994626998901 
Batch 3297, generator loss: 0.9344369173049927, discriminator loss: 1.1674222946166992 
Batch 3298, generator loss: 0.9238991737365723, discriminator loss: 1.160118818283081 
Batch 3299, generator loss: 0.9276349544525146, discriminator loss: 1.1802340745925903 
Batch 3300, generator loss: 0.9626523852348328, discriminator loss: 1.151380181312561 
Batch 3301, generator loss: 0.9706828594207764, discriminator loss: 1.1559187173843384 
Batch 3302, generator loss: 0.9352925419807434, discriminator loss: 1.229423999786377 
Batch 3303, generator loss: 0.9043292999267578, discriminator loss: 1.2323417663574219 
Batch 3304, generator loss: 0.9272410869598389, discriminator loss: 1.2062804698944092 
Batch 3305, generator loss: 0.9048595428466797, discriminator loss: 1.2189061641693115 
Batch 3306, generator loss: 0.9342356324195862, discriminator loss: 1.189319372177124 
Batch 3307, generator loss: 0.9053290486335754, discriminator loss: 1.207236409187317 
Batch 3308, generator loss: 0.9107272624969482, discriminator loss: 1.2130253314971924 
Generator loss: tf.Tensor(0.91072726, shape=(), dtype=float32)
Discriminator loss: tf.Tensor(1.2130253, shape=(), dtype=float32)
EPOCH: 13
Batch 3330, generator loss: 0.9711821675300598, discriminator loss: 1.1874885559082031 
Batch 3331, generator loss: 0.9397701025009155, discriminator loss: 1.206796407699585 
Batch 3332, generator loss: 0.905129075050354, discriminator loss: 1.240591287612915 
Batch 3333, generator loss: 0.8597990274429321, discriminator loss: 1.2642537355422974 
Batch 3334, generator loss: 0.9092896580696106, discriminator loss: 1.189512014389038 
Batch 3335, generator loss: 0.9412147998809814, discriminator loss: 1.235230565071106 
Batch 3336, generator loss: 0.909133791923523, discriminator loss: 1.2460564374923706 
Batch 3337, generator loss: 0.9137284755706787, discriminator loss: 1.2146968841552734 
Batch 3338, generator loss: 0.8813016414642334, discriminator loss: 1.2476956844329834 
Batch 3339, generator loss: 0.9247015714645386, discriminator loss: 1.3067328929901123 
Batch 3340, generator loss: 0.8990081548690796, discriminator loss: 1.2666088342666626 
Batch 3341, generator loss: 0.8953878879547119, discriminator loss: 1.2176631689071655 
Batch 3342, generator loss: 0.8492065668106079, discriminator loss: 1.2946717739105225 
Batch 3343, generator loss: 0.9232869148254395, discriminator loss: 1.207794189453125 
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-2-5651ded4e53c&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">    226</span> trainer<span class="ansi-blue-fg">.</span>load_data<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    227</span> 
<span class="ansi-green-fg">--&gt; 228</span><span class="ansi-red-fg"> </span>trainer<span class="ansi-blue-fg">.</span>train<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-2-5651ded4e53c&gt;</span> in <span class="ansi-cyan-fg">train</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">     90</span>       <span class="ansi-green-fg">for</span> images <span class="ansi-green-fg">in</span> self<span class="ansi-blue-fg">.</span>train_dataset<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     91</span>         counter <span class="ansi-blue-fg">=</span> counter <span class="ansi-blue-fg">+</span> <span class="ansi-cyan-fg">1</span>
<span class="ansi-green-fg">---&gt; 92</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>optimize<span class="ansi-blue-fg">(</span>images<span class="ansi-blue-fg">,</span> epoch<span class="ansi-blue-fg">*</span>self<span class="ansi-blue-fg">.</span>BATCH_SIZE <span class="ansi-blue-fg">+</span> counter<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     93</span>       counter <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">0</span>
<span class="ansi-green-intense-fg ansi-bold">     94</span>       print<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;Generator loss:&#34;</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>generator<span class="ansi-blue-fg">.</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-2-5651ded4e53c&gt;</span> in <span class="ansi-cyan-fg">optimize</span><span class="ansi-blue-fg">(self, images, batch)</span>
<span class="ansi-green-intense-fg ansi-bold">     49</span> 
<span class="ansi-green-intense-fg ansi-bold">     50</span>     <span class="ansi-green-fg">with</span> tf<span class="ansi-blue-fg">.</span>GradientTape<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">as</span> gen_tape<span class="ansi-blue-fg">,</span> tf<span class="ansi-blue-fg">.</span>GradientTape<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">as</span> disc_tape<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 51</span><span class="ansi-red-fg">       </span>generated_images <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>generator<span class="ansi-blue-fg">.</span>model<span class="ansi-blue-fg">(</span>noise<span class="ansi-blue-fg">,</span> training<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     52</span> 
<span class="ansi-green-intense-fg ansi-bold">     53</span>       real_output <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>discriminator<span class="ansi-blue-fg">.</span>model<span class="ansi-blue-fg">(</span>images<span class="ansi-blue-fg">,</span> training<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, inputs, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    889</span>           with base_layer_utils.autocast_context_manager(
<span class="ansi-green-intense-fg ansi-bold">    890</span>               self._compute_dtype):
<span class="ansi-green-fg">--&gt; 891</span><span class="ansi-red-fg">             </span>outputs <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>call<span class="ansi-blue-fg">(</span>cast_inputs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    892</span>           self<span class="ansi-blue-fg">.</span>_handle_activity_regularization<span class="ansi-blue-fg">(</span>inputs<span class="ansi-blue-fg">,</span> outputs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    893</span>           self<span class="ansi-blue-fg">.</span>_set_mask_metadata<span class="ansi-blue-fg">(</span>inputs<span class="ansi-blue-fg">,</span> outputs<span class="ansi-blue-fg">,</span> input_masks<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py</span> in <span class="ansi-cyan-fg">call</span><span class="ansi-blue-fg">(self, inputs, training, mask)</span>
<span class="ansi-green-intense-fg ansi-bold">    254</span>       <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> self<span class="ansi-blue-fg">.</span>built<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    255</span>         self<span class="ansi-blue-fg">.</span>_init_graph_network<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>inputs<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>outputs<span class="ansi-blue-fg">,</span> name<span class="ansi-blue-fg">=</span>self<span class="ansi-blue-fg">.</span>name<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 256</span><span class="ansi-red-fg">       </span><span class="ansi-green-fg">return</span> super<span class="ansi-blue-fg">(</span>Sequential<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>call<span class="ansi-blue-fg">(</span>inputs<span class="ansi-blue-fg">,</span> training<span class="ansi-blue-fg">=</span>training<span class="ansi-blue-fg">,</span> mask<span class="ansi-blue-fg">=</span>mask<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    257</span> 
<span class="ansi-green-intense-fg ansi-bold">    258</span>     outputs <span class="ansi-blue-fg">=</span> inputs  <span class="ansi-red-fg"># handle the corner case where self.layers is empty</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py</span> in <span class="ansi-cyan-fg">call</span><span class="ansi-blue-fg">(self, inputs, training, mask)</span>
<span class="ansi-green-intense-fg ansi-bold">    706</span>     return self._run_internal_graph(
<span class="ansi-green-intense-fg ansi-bold">    707</span>         inputs<span class="ansi-blue-fg">,</span> training<span class="ansi-blue-fg">=</span>training<span class="ansi-blue-fg">,</span> mask<span class="ansi-blue-fg">=</span>mask<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">--&gt; 708</span><span class="ansi-red-fg">         convert_kwargs_to_constants=base_layer_utils.call_context().saving)
</span><span class="ansi-green-intense-fg ansi-bold">    709</span> 
<span class="ansi-green-intense-fg ansi-bold">    710</span>   <span class="ansi-green-fg">def</span> compute_output_shape<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> input_shape<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py</span> in <span class="ansi-cyan-fg">_run_internal_graph</span><span class="ansi-blue-fg">(self, inputs, training, mask, convert_kwargs_to_constants)</span>
<span class="ansi-green-intense-fg ansi-bold">    858</span> 
<span class="ansi-green-intense-fg ansi-bold">    859</span>           <span class="ansi-red-fg"># Compute outputs.</span>
<span class="ansi-green-fg">--&gt; 860</span><span class="ansi-red-fg">           </span>output_tensors <span class="ansi-blue-fg">=</span> layer<span class="ansi-blue-fg">(</span>computed_tensors<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    861</span> 
<span class="ansi-green-intense-fg ansi-bold">    862</span>           <span class="ansi-red-fg"># Update tensor_dict.</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, inputs, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    889</span>           with base_layer_utils.autocast_context_manager(
<span class="ansi-green-intense-fg ansi-bold">    890</span>               self._compute_dtype):
<span class="ansi-green-fg">--&gt; 891</span><span class="ansi-red-fg">             </span>outputs <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>call<span class="ansi-blue-fg">(</span>cast_inputs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    892</span>           self<span class="ansi-blue-fg">.</span>_handle_activity_regularization<span class="ansi-blue-fg">(</span>inputs<span class="ansi-blue-fg">,</span> outputs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    893</span>           self<span class="ansi-blue-fg">.</span>_set_mask_metadata<span class="ansi-blue-fg">(</span>inputs<span class="ansi-blue-fg">,</span> outputs<span class="ansi-blue-fg">,</span> input_masks<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py</span> in <span class="ansi-cyan-fg">call</span><span class="ansi-blue-fg">(self, inputs)</span>
<span class="ansi-green-intense-fg ansi-bold">    833</span>         padding<span class="ansi-blue-fg">=</span>self<span class="ansi-blue-fg">.</span>padding<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    834</span>         data_format<span class="ansi-blue-fg">=</span>self<span class="ansi-blue-fg">.</span>data_format<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">--&gt; 835</span><span class="ansi-red-fg">         dilation_rate=self.dilation_rate)
</span><span class="ansi-green-intense-fg ansi-bold">    836</span> 
<span class="ansi-green-intense-fg ansi-bold">    837</span>     <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> context<span class="ansi-blue-fg">.</span>executing_eagerly<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py</span> in <span class="ansi-cyan-fg">conv2d_transpose</span><span class="ansi-blue-fg">(x, kernel, output_shape, strides, padding, data_format, dilation_rate)</span>
<span class="ansi-green-intense-fg ansi-bold">   4925</span>     x = nn.conv2d_transpose(x, kernel, output_shape, strides,
<span class="ansi-green-intense-fg ansi-bold">   4926</span>                             padding<span class="ansi-blue-fg">=</span>padding<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 4927</span><span class="ansi-red-fg">                             data_format=tf_data_format)
</span><span class="ansi-green-intense-fg ansi-bold">   4928</span>   <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">   4929</span>     <span class="ansi-green-fg">assert</span> dilation_rate<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">==</span> dilation_rate<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py</span> in <span class="ansi-cyan-fg">conv2d_transpose</span><span class="ansi-blue-fg">(value, filter, output_shape, strides, padding, data_format, name, input, filters, dilations)</span>
<span class="ansi-green-intense-fg ansi-bold">   2202</span>         data_format<span class="ansi-blue-fg">=</span>data_format<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   2203</span>         dilations<span class="ansi-blue-fg">=</span>dilations<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 2204</span><span class="ansi-red-fg">         name=name)
</span><span class="ansi-green-intense-fg ansi-bold">   2205</span> 
<span class="ansi-green-intense-fg ansi-bold">   2206</span> 

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py</span> in <span class="ansi-cyan-fg">conv2d_transpose_v2</span><span class="ansi-blue-fg">(input, filters, output_shape, strides, padding, data_format, dilations, name)</span>
<span class="ansi-green-intense-fg ansi-bold">   2273</span>         data_format<span class="ansi-blue-fg">=</span>data_format<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   2274</span>         dilations<span class="ansi-blue-fg">=</span>dilations<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 2275</span><span class="ansi-red-fg">         name=name)
</span><span class="ansi-green-intense-fg ansi-bold">   2276</span> 
<span class="ansi-green-intense-fg ansi-bold">   2277</span> 

<span class="ansi-green-fg">/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py</span> in <span class="ansi-cyan-fg">conv2d_backprop_input</span><span class="ansi-blue-fg">(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)</span>
<span class="ansi-green-intense-fg ansi-bold">   1354</span>         <span class="ansi-blue-fg">&#34;use_cudnn_on_gpu&#34;</span><span class="ansi-blue-fg">,</span> use_cudnn_on_gpu<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#34;padding&#34;</span><span class="ansi-blue-fg">,</span> padding<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">   1355</span>         <span class="ansi-blue-fg">&#34;explicit_paddings&#34;</span><span class="ansi-blue-fg">,</span> explicit_paddings<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#34;data_format&#34;</span><span class="ansi-blue-fg">,</span> data_format<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">-&gt; 1356</span><span class="ansi-red-fg">         &#34;dilations&#34;, dilations)
</span><span class="ansi-green-intense-fg ansi-bold">   1357</span>       <span class="ansi-green-fg">return</span> _result
<span class="ansi-green-intense-fg ansi-bold">   1358</span>     <span class="ansi-green-fg">except</span> _core<span class="ansi-blue-fg">.</span>_FallbackException<span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#%tensorboard --logdir {logs_base_dir}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
